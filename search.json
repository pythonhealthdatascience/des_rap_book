[
  {
    "objectID": "pages/reports/tables_figures.html",
    "href": "pages/reports/tables_figures.html",
    "title": "Producing tables and figures",
    "section": "",
    "text": "Producing tables and figures\n\nüîó Reproducibility guidelines:\n\nHeather et al.¬†2025 (‚≠ê): Include code to generate the tables, figures, and other reported results.\nHeather et al.¬†2025: Save outputs to a file.",
    "crumbs": [
      "Reports & manuscripts",
      "Producing tables and figures"
    ]
  },
  {
    "objectID": "pages/further_info/acknowledgements.html",
    "href": "pages/further_info/acknowledgements.html",
    "title": "Acknowledgements",
    "section": "",
    "text": "Acknowledgements",
    "crumbs": [
      "Further information",
      "Acknowledgements"
    ]
  },
  {
    "objectID": "pages/output_analysis/replications.html",
    "href": "pages/output_analysis/replications.html",
    "title": "Replications",
    "section": "",
    "text": "Replications",
    "crumbs": [
      "Output analysis",
      "Replications"
    ]
  },
  {
    "objectID": "pages/output_analysis/n_reps.html",
    "href": "pages/output_analysis/n_reps.html",
    "title": "Number of replications",
    "section": "",
    "text": "Number of replications",
    "crumbs": [
      "Output analysis",
      "Number of replications"
    ]
  },
  {
    "objectID": "pages/inputs/input_modelling.html",
    "href": "pages/inputs/input_modelling.html",
    "title": "Input modelling",
    "section": "",
    "text": "This page has step-by-step instructions for input modelling in Python or R, with inspiration from Robinson (2007) and Monks (2024). For advice on making your input modelling workflow reproducible and sharing data or scripts with sensitive content, see ?@sec-input_data.\n\n\n\nTo build a DES model, you first need data that reflects the system you want to model. In healthcare, this might mean you need to access healthcare records with patient arrival, service and departure times, for example. The quality of your simulation depends directly on the quality of your data. Key considerations include:\n\nAccuracy. Inaccurate data leads to inaccurate simulation results.\nSample size. Small samples can give misleading results if they capture unusual periods, lack variability, or are affected by outliers.\nLevel of detail. The data must be granular enough for your needs. For example, daily totals may be insufficient if you want to model hourly arrivals (although may still be possible if know distribution - see Section¬†1.3)\nRepresentative. The data should reflect the current system. For instance, data from the COVID-19 period may not represent typical operations.\n\n\n\n\n\nDiscrete-event simulation (DES) models are stochastic, which means they incorporate random variation, to reflect the inherent variability of real-world systems.\nInstead of using fixed times for events (like having a patient arrive exactly every five minutes), DES models pick the timing of events by randomly sampling values from a probability distribution.\nThe process of selecting the most appropriate statistical distributions to use in your model is called input modelling.\n\n\n\n\n\nWhen selecting appropriate distributions, if you only have summary statistics (like the mean), you may need to rely on expert opinion or the general properties of the process you‚Äôre modelling. For example:\n\nArrivals: Random independent arrivals are often modelled with the Poisson distribution, whilst their inter-arrival times are modelled using an exponential distribution (Pishro-Nik (2014)).\nLength of stay: Length of stay is commonly right skewed (Lee, Fung, and Fu (2003)), and so will often be modelled with distributions like exponential, gamma, log-normal (for log-transformed length of stay) or Weibull.\n\nHowever, these standard choices may not always be appropriate. If the actual process differs from your assumptions or has unique features, the chosen distribution might not fit well.\nTherefore, if you have enough data, it‚Äôs best to analyse it directly to select the most suitable distributions. This analysis generally involves two key steps:\n\nIdentify possible distributions. This is based on knowledge of the process being modelled, and by inspecting the data using time series plots and histograms.\nFit distributions to your data and compare goodness-of fit. You can do this using a:\n\nTargeted approach. Just test the distributions from step 1.\nComprehensive approach. Test a wide range of distributions.\n\n\nThough the comprehensive approach tests lots of different distributions, it‚Äôs still important to do step 1 as:\n\nIt‚Äôs important to be aware of temporal patterns in the data (e.g.¬†spikes in service length every Friday).\nYou may find distributions which mathematically fit but are contextually inappropriate (e.g.¬†normal distribution for service times, which can‚Äôt be negative).\nYou may find better fit for complex distributions, even when simpler are sufficient.\n\nWe‚Äôll demonstrate steps for input modelling below using synthetic arrival data from the nurse visit simulation (?@sec-examples). In this case, we already know which distributions to use (as we sampled from them to create our synthetic data!). However, the steps still illustrate how you might select distributions in practice with real data.\n\nWe‚Äôll create a Jupyter Notebook to perform this analysis in.\ntouch notebooks/input_modelling.ipynb\n\n\nWe‚Äôll create a R Markdown file to perform this analysis in.\ntouch rmarkdown/input_modelling.Rmd\n\n\n\n\nYou first need to select which distributions to fit to your data. You should both:\n\nConsider the known properties of the process being modelled (as above), and-\nInspect the data by plotting a histogram.\n\nRegarding the known properties, it would be good to consider the exponential distribution for our arrivals, as that is a common choice for random independent arrivals.\nTo inspect the data, we will create two plots:\n\n\n\n\n\n\n\n\nPlot type\nWhat does it show?\nWhy do we create this plot?\n\n\n\n\nTime series\nTrends, seasonality, and outliers (e.g., spikes or dips over time).\nTo check for stationarity (i.e.¬†no trends or sudden changes). Stationary is an assumption of many distributions, and if trends or anomalies do exist, we may need to exclude certain periods or model them separately. The time series can also be useful for spotting outliers and data gaps.\n\n\nHistogram\nThe shape of the data‚Äôs distribution.\nHelps identify which distributions might fit the data.\n\n\n\nWe repeat this for the arrivals and service (nurse consultation) time, so have created functions to avoid duplicate code between each.\n\nFirst, we import the data.\n\n\n\n# pylint: disable=missing-module-docstring\n# Import required packages\nfrom distfit import distfit\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.io as pio\nimport scipy.stats as stats\n\n\n\n# Import data\ndata = pd.read_csv(\"../../data/NHS_synthetic.csv\", dtype={\n    \"ARRIVAL_TIME\": str,\n    \"SERVICE_TIME\": str,\n    \"DEPARTURE_TIME\": str\n})\n\n# Preview data\ndata.head()\n\n  ARRIVAL_DATE ARRIVAL_TIME  ... DEPARTURE_DATE DEPARTURE_TIME\n0   2025-01-01         0001  ...     2025-01-01           0012\n1   2025-01-01         0002  ...     2025-01-01           0007\n2   2025-01-01         0003  ...     2025-01-01           0030\n3   2025-01-01         0007  ...     2025-01-01           0022\n4   2025-01-01         0010  ...     2025-01-01           0031\n\n[5 rows x 6 columns]\n\n\n\n\n\n\n# nolint start: undesirable_function_linter.\n# Import required packages\nlibrary(dplyr)\nlibrary(fitdistrplus) \nlibrary(ggplot2)\nlibrary(lubridate)\nlibrary(plotly)\nlibrary(readr)\nlibrary(tidyr)\n# nolint end\n\n\n\n# Import data\ndata &lt;- read_csv(\n    file.path(\"..\", \"..\", \"data\", \"NHS_synthetic.csv\"), show_col_types = FALSE\n)\n\n# Preview data\nhead(data)\n\n# A tibble: 6 √ó 6\n  ARRIVAL_DATE ARRIVAL_TIME SERVICE_DATE SERVICE_TIME DEPARTURE_DATE\n  &lt;date&gt;       &lt;chr&gt;        &lt;date&gt;       &lt;chr&gt;        &lt;date&gt;        \n1 2025-01-01   0001         2025-01-01   0007         2025-01-01    \n2 2025-01-01   0002         2025-01-01   0004         2025-01-01    \n3 2025-01-01   0003         2025-01-01   0010         2025-01-01    \n4 2025-01-01   0007         2025-01-01   0014         2025-01-01    \n5 2025-01-01   0010         2025-01-01   0012         2025-01-01    \n6 2025-01-01   0010         2025-01-01   0011         2025-01-01    \n# ‚Ñπ 1 more variable: DEPARTURE_TIME &lt;chr&gt;\n\n\n\n\nWe then calculate the inter-arrival times.\n\n\n\n# Combine date/time and convert to datetime\ndata[\"arrival_datetime\"] = pd.to_datetime(\n    data[\"ARRIVAL_DATE\"] + \" \" + data[\"ARRIVAL_TIME\"].str.zfill(4),\n    format=\"%Y-%m-%d %H%M\"\n)\n\n# Sort by arrival time and calculate inter-arrival times\ndata = data.sort_values(\"arrival_datetime\")\ndata[\"iat_mins\"] = (\n    data[\"arrival_datetime\"].diff().dt.total_seconds() / 60\n)\n\n# Preview\ndata[[\"ARRIVAL_DATE\", \"ARRIVAL_TIME\", \"arrival_datetime\", \"iat_mins\"]].head()\n\n  ARRIVAL_DATE ARRIVAL_TIME    arrival_datetime  iat_mins\n0   2025-01-01         0001 2025-01-01 00:01:00       NaN\n1   2025-01-01         0002 2025-01-01 00:02:00       1.0\n2   2025-01-01         0003 2025-01-01 00:03:00       1.0\n3   2025-01-01         0007 2025-01-01 00:07:00       4.0\n4   2025-01-01         0010 2025-01-01 00:10:00       3.0\n\n\n\n\n\n\ndata &lt;- data %&gt;%\n  # Combine date/time and convert to datetime\n  mutate(arrival_datetime = ymd_hm(paste(ARRIVAL_DATE, ARRIVAL_TIME))) %&gt;%\n  # Sort by arrival time\n  arrange(arrival_datetime) %&gt;%\n  # Calculate inter-arrival times\n  mutate(\n    iat_mins = as.numeric(\n      difftime(\n        arrival_datetime, lag(arrival_datetime), units = \"mins\"\n      )\n    )\n  )\n\n# Preview\ndata %&gt;%\n  select(ARRIVAL_DATE, ARRIVAL_TIME, arrival_datetime, iat_mins) %&gt;%\n  head()\n\n# A tibble: 6 √ó 4\n  ARRIVAL_DATE ARRIVAL_TIME arrival_datetime    iat_mins\n  &lt;date&gt;       &lt;chr&gt;        &lt;dttm&gt;                 &lt;dbl&gt;\n1 2025-01-01   0001         2025-01-01 00:01:00       NA\n2 2025-01-01   0002         2025-01-01 00:02:00        1\n3 2025-01-01   0003         2025-01-01 00:03:00        1\n4 2025-01-01   0007         2025-01-01 00:07:00        4\n5 2025-01-01   0010         2025-01-01 00:10:00        3\n6 2025-01-01   0010         2025-01-01 00:10:00        0\n\n\n\n\nWe also calculate the service times.\n\n\n\n# Combine dates with times\ndata[\"service_datetime\"] = pd.to_datetime(\n    data[\"SERVICE_DATE\"] + \" \" + data[\"SERVICE_TIME\"].str.zfill(4)\n)\ndata[\"departure_datetime\"] = pd.to_datetime(\n    data[\"DEPARTURE_DATE\"] + \" \" + data[\"DEPARTURE_TIME\"].str.zfill(4)\n)\n\n# Calculate time difference in minutes\ntime_delta = data[\"departure_datetime\"] - data[\"service_datetime\"]\ndata[\"service_mins\"] = time_delta / pd.Timedelta(minutes=1)\n\n# Preview\ndata[[\"service_datetime\", \"departure_datetime\", \"service_mins\"]].head()\n\n     service_datetime  departure_datetime  service_mins\n0 2025-01-01 00:07:00 2025-01-01 00:12:00           5.0\n1 2025-01-01 00:04:00 2025-01-01 00:07:00           3.0\n2 2025-01-01 00:10:00 2025-01-01 00:30:00          20.0\n3 2025-01-01 00:14:00 2025-01-01 00:22:00           8.0\n4 2025-01-01 00:12:00 2025-01-01 00:31:00          19.0\n\n\n\n\n\n\ndata &lt;- data %&gt;%\n  mutate(\n    service_datetime   = ymd_hm(paste(SERVICE_DATE, SERVICE_TIME)),\n    departure_datetime = ymd_hm(paste(DEPARTURE_DATE, DEPARTURE_TIME)),\n    service_mins = as.numeric(\n      difftime(departure_datetime, service_datetime, units = \"mins\")\n    )\n  )\n\n# Preview\ndata %&gt;% select(service_datetime, departure_datetime, service_mins) %&gt;% head()\n\n# A tibble: 6 √ó 3\n  service_datetime    departure_datetime  service_mins\n  &lt;dttm&gt;              &lt;dttm&gt;                     &lt;dbl&gt;\n1 2025-01-01 00:07:00 2025-01-01 00:12:00            5\n2 2025-01-01 00:04:00 2025-01-01 00:07:00            3\n3 2025-01-01 00:10:00 2025-01-01 00:30:00           20\n4 2025-01-01 00:14:00 2025-01-01 00:22:00            8\n5 2025-01-01 00:12:00 2025-01-01 00:31:00           19\n6 2025-01-01 00:11:00 2025-01-01 00:14:00            3\n\n\n\n\nTime series. For this data, we observe no trends, seasonality or outliers.\n\n\n\ndef inspect_time_series(time_series, y_lab):\n    \"\"\"\n    Plot time-series.\n\n    Parameters\n    ----------\n    time_series : pd.Series\n        Series containing the time series data (where index is the date).\n    y_lab : str\n        Y axis label.\n\n    Returns\n    -------\n    fig : plotly.graph_objects.Figure\n    \"\"\"\n    # Label as \"Date\" and provided y_lab, and convert to dataframe\n    df = time_series.rename_axis(\"Date\").reset_index(name=y_lab)\n\n    # Create plot\n    fig = px.line(df, x=\"Date\", y=y_lab)\n    fig.update_layout(showlegend=False, width=700, height=400)\n    return fig\n\n\n\n# Calculate mean arrivals per day and plot time series\np = inspect_time_series(\n    time_series=data.groupby(by=[\"ARRIVAL_DATE\"]).size(),\n    y_lab=\"Number of arrivals\")\n\np.show()\n\n                            \n                                            \n\n\n\n\n# Calculate mean service length per day, dropping last day (incomplete)\ndaily_service = data.groupby(\"SERVICE_DATE\")[\"service_mins\"].mean()\ndaily_service = daily_service.iloc[:-1]\n\n# Plot mean service length each day\np = inspect_time_series(time_series=daily_service,\n                        y_lab=\"Mean consultation length (min)\")\np.show()\n\n                            \n                                            \n\n\n\n\n\n\ninspect_time_series &lt;- function(\n  time_series, date_col, value_col, y_lab, interactive, save_path = NULL\n) {\n  #' Plot time-series\n  #'\n  #' @param time_series Dataframe with date column and numeric column to plot.\n  #' @param date_col String. Name of column with dates.\n  #' @param value_col String. Name of column with numeric values.\n  #' @param y_lab String. Y axis label.\n  #' @param interactive Boolean. Whether to render interactive or static plot.\n  #' @param save_path String. Path to save static file to (inc. name and\n  #' filetype). If NULL, then will not save.\n\n  # Create custom tooltip text\n  time_series$tooltip_text &lt;- paste0(\n    \"&lt;span style='color:white'&gt;\",\n    \"Date: \", time_series[[date_col]], \"&lt;br&gt;\",\n    y_lab, \": \", time_series[[value_col]], \"&lt;/span&gt;\"\n  )\n\n  # Create plot\n  p &lt;- ggplot(time_series, aes(x = .data[[date_col]],\n                               y = .data[[value_col]],\n                               text = tooltip_text)) +  # nolint: object_usage_linter\n    geom_line(group = 1L, color = \"#727af4\") +\n    labs(x = \"Date\", y = y_lab) +\n    theme_minimal()\n\n  # Save file if path provided\n  if (!is.null(save_path)) {\n    ggsave(save_path, p, width = 7L, height = 4L)\n  }\n\n  # Display as interactive or static figure\n  if (interactive) {\n    ggplotly(p, tooltip = \"text\", width = 700L, height = 400L)\n  } else {\n    p\n  }\n}\n\n\n\n# Plot daily arrivals\ndaily_arrivals &lt;- data %&gt;% group_by(ARRIVAL_DATE) %&gt;% count()\ninspect_time_series(\n  time_series = daily_arrivals, date_col = \"ARRIVAL_DATE\", value_col = \"n\",\n  y_lab = \"Number of arrivals\", interactive = TRUE\n)\n\n\n\n\n\n\n\n# Calculate mean service length per day, dropping last day (incomplete)\ndaily_service &lt;- data %&gt;%\n  group_by(SERVICE_DATE) %&gt;%\n  summarise(mean_service = mean(service_mins)) %&gt;%\n  filter(row_number() &lt;= n() - 1L)\n\n# Plot mean service length each day\ninspect_time_series(\n  time_series = daily_service, date_col = \"SERVICE_DATE\",\n  value_col = \"mean_service\", y_lab = \"Mean consultation length (min)\",\n  interactive = TRUE\n)\n\n\n\n\n\n\n\nHistogram. For both inter-arrival times and service times, we observe a right skewed distribution. Hence, it would be good to try exponential, gamma and Weibull distributions.\n\n\n\ndef inspect_histogram(series, x_lab):\n    \"\"\"\n    Plot histogram.\n\n    Parameters\n    ----------\n    series : pd.Series\n        Series containing the values to plot as a histogram.\n    x_lab : str\n        X axis label.\n\n    Returns\n    -------\n    fig : plotly.graph_objects.Figure\n    \"\"\"\n    fig = px.histogram(series)\n    fig.update_layout(\n        xaxis_title=x_lab, showlegend=False, width=700, height=400\n    )\n    fig.update_traces(\n        hovertemplate=x_lab + \": %{x}&lt;br&gt;Count: %{y}\", name=\"\"\n    )\n    return fig\n\n\n\n# Plot histogram of inter-arrival times\np = inspect_histogram(series=data[\"iat_mins\"],\n                      x_lab=\"Inter-arrival time (min)\")\np.show()\n\n                            \n                                            \n\n\n\n\n# Plot histogram of service times\np = inspect_histogram(series=data[\"service_mins\"],\n                      x_lab=\"Consultation length (min)\")\np.show()\n\n                            \n                                            \n\n\n\n\n\n\ninspect_histogram &lt;- function(\n  data, var, x_lab, interactive, save_path = NULL\n) {\n  #' Plot histogram\n  #'\n  #' @param data A dataframe or tibble containing the variable to plot.\n  #' @param var String. Name of the column to plot as a histogram.\n  #' @param x_lab String. X axis label.\n  #' @param interactive Boolean. Whether to render interactive or static plot.\n  #' @param save_path String. Path to save static file to (inc. name and\n  #' filetype). If NULL, then will not save.\n\n  # Remove non-finite values\n  data &lt;- data[is.finite(data[[var]]), ]\n\n  # Create plot\n  p &lt;- ggplot(data, aes(x = .data[[var]])) +\n    geom_histogram(aes(text = paste0(\"&lt;span style='color:white'&gt;\", x_lab, \": \",\n                                     round(after_stat(x), 2L), \"&lt;br&gt;Count: \",  # nolint: object_usage_linter\n                                     after_stat(count), \"&lt;/span&gt;\")),\n                   fill = \"#727af4\", bins = 30L) +\n    labs(x = x_lab, y = \"Count\") +\n    theme_minimal() +\n    theme(legend.position = \"none\")\n\n  # Save file if path provided\n  if (!is.null(save_path)) {\n    ggsave(save_path, p, width = 7L, height = 4L)\n  }\n\n  # Display as interactive or static figure\n  if (interactive) {\n    ggplotly(p, tooltip = \"text\", width = 700L, height = 400L)\n  } else {\n    p\n  }\n}\n\n\n\n# Plot histogram of inter-arrival times\ninspect_histogram(\n  data = data, var = \"iat_mins\", x_lab = \"Inter-arrival time (min)\",\n  interactive = TRUE\n)\n\n\n\n\n# Plot histogram of service times\ninspect_histogram(\n  data = data, var = \"service_mins\", x_lab = \"Consultation length (min)\",\n  interactive = TRUE\n)\n\n\n\n\n\n\nAlternative: You can use the fitdistrplus package to create these histograms - as well as the empirical cumulative distribution function (CDF), which can help you inspect the tails, central tendency, and spot jumps or plateaus in the data.\n\n\n# Get IAT and service time columns as numeric vectors (with NA dropped)\ndata_iat &lt;- data %&gt;% drop_na(iat_mins) %&gt;% select(iat_mins) %&gt;% pull()\ndata_service &lt;- data %&gt;% select(service_mins) %&gt;% pull()\n\n# Plot histograms and CDFs\nplotdist(data_iat, histo = TRUE, demp = TRUE)\n\n\n\n\n\n\n\nplotdist(data_service, histo = TRUE, demp = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe will fit distributions and assess goodness-of-fit using the Kolmogorov-Smirnov (KS) Test. This is a common test which is well-suited to continuous distributions. For categorical (or binned) data, consider using chi-squared tests.\nThe KS Test returns a statistic and p value.\n\nStatistic: Measures how well the distribution fits your data.\n\nHigher values indicate a better fit.\nRanges from 0 to 1.\n\nP-value: Tells you if the fit could have happened by chance.\n\nHigher p-values suggest the data follow the distribution.\nIn large datasets, even good fits often have small p-values.\nRanges from 0 to 1.\n\n\n\nscipy.stats is a popular library for fitting and testing statistical distributions. For more convenience, distfit, which is built on scipy, is another popular package which can test multiple distributions simultaneously (or evaluate specific distributions).\nWe will illustrate how to perform the targeted approach using scipy directly, and the comprehensive approach using distfit - but you could use either for each approach.\n\n\nfitdistrplus is a popular library for fitting and testing statistical distributions. This can evaluate specific distributions or test multiple distributions. We will use this library to illustrate how to perform the targeted or comprehensive approach.\n\n\n\n\n\nTo implement the targeted approach using scipy.stats‚Ä¶\n\n\ndef fit_distributions(input_series, dists):\n    \"\"\"\n    This function fits statistical distributions to the provided data and\n    performs Kolmogorov-Smirnov tests to assess the goodness of fit.\n\n    Parameters\n    ----------\n    input_series : pandas.Series\n        The observed data to fit the distributions to.\n    dists : list\n        List of the distributions in scipy.stats to fit, eg. [\"expon\", \"gamma\"]\n\n    Notes\n    -----\n    A lower test statistic and higher p-value indicates better fit to the data.\n    \"\"\"\n    for dist_name in dists:\n        # Fit distribution to the data\n        dist = getattr(stats, dist_name)\n        params = dist.fit(input_series)\n\n        # Return results from Kolmogorov-Smirnov test\n        ks_result = stats.kstest(input_series, dist_name, args=params)\n        print(f\"Kolmogorov-Smirnov statistic for {dist_name}: \" +\n            f\"{ks_result.statistic:.4f} (p={ks_result.pvalue:.2e})\")\n\n\n# Fit and run Kolmogorov-Smirnov test on the inter-arrival and service times\ndistributions = [\"expon\", \"gamma\", \"weibull_min\"]\n\nInter-arrival time:\n\n\nfit_distributions(input_series=data[\"iat_mins\"].dropna(), dists=distributions)\n\nKolmogorov-Smirnov statistic for expon: 0.1155 (p=0.00e+00)\nKolmogorov-Smirnov statistic for gamma: 0.2093 (p=0.00e+00)\nKolmogorov-Smirnov statistic for weibull_min: 0.1355 (p=0.00e+00)\n\n\nService time:\n\n\nfit_distributions(input_series=data[\"service_mins\"], dists=distributions)\n\nKolmogorov-Smirnov statistic for expon: 0.0480 (p=3.08e-264)\nKolmogorov-Smirnov statistic for gamma: 0.1226 (p=0.00e+00)\nKolmogorov-Smirnov statistic for weibull_min: 0.0696 (p=0.00e+00)\n\n\n\n\nWe have several of zeros (as times are rounded to nearest minute, and arrivals are frequent / service times can be short). Weibull is only defined for positive values, so we won‚Äôt try that. We have built in error-handling to fit_distributions to ensure that.\n\n\n# Percentage of inter-arrival times that are 0\npaste0(round(sum(data_iat == 0L) / length(data_iat) * 100L, 2L), \"%\")\n\n[1] \"11.55%\"\n\npaste0(round(sum(data_service == 0L) / length(data_service) * 100L, 2L), \"%\")\n\n[1] \"4.8%\"\n\n\n\n\nfit_distributions &lt;- function(data, dists) {\n  #' Compute Kolmogorov-Smirnov Statistics for Fitted Distributions\n  #'\n  #' @param data Numeric vector. The data to fit distributions to.\n  #' @param dists Character vector. Names of distributions to fit.\n  #'\n  #' @return Named numeric vector of Kolmogorov-Smirnov statistics, one per\n  #' distribution.\n\n  # Define distribution requirements\n  positive_only &lt;- c(\"lnorm\", \"weibull\")\n  non_negative &lt;- c(\"exp\", \"gamma\")\n  zero_to_one &lt;- \"beta\"\n\n  # Check data characteristics\n  has_negatives &lt;- any(data &lt; 0L)\n  has_zeros &lt;- any(data == 0L)\n  has_out_of_beta_range &lt;- any(data &lt; 0L | data &gt; 1L)\n\n  # Filter distributions based on data\n  valid_dists &lt;- dists\n  if (has_negatives || has_zeros) {\n    valid_dists &lt;- setdiff(valid_dists, positive_only)\n  }\n  if (has_negatives) {\n    valid_dists &lt;- setdiff(valid_dists, non_negative)\n  }\n  if (has_out_of_beta_range) {\n    valid_dists &lt;- setdiff(valid_dists, zero_to_one)\n  }\n\n  # Warn about skipped distributions\n  skipped &lt;- setdiff(dists, valid_dists)\n  if (length(skipped) &gt; 0L) {\n    warning(\"Skipped distributions due to data constraints: \",\n            toString(skipped), call. = FALSE)\n  }\n\n  # Exit early if no valid distributions remain\n  if (length(valid_dists) == 0L) {\n    warning(\"No valid distributions to test after filtering\", call. = FALSE)\n    return(numeric(0L))\n  }\n\n  # Fit remaining distributions\n  fits &lt;- lapply(\n    valid_dists, function(dist) suppressWarnings(fitdist(data, dist))\n  )\n  gof_results &lt;- gofstat(fits, fitnames = valid_dists)\n\n  # Return KS statistics\n  gof_results$ks\n}\n\n\ndistributions &lt;- c(\"exp\", \"gamma\", \"weibull\")\nfit_distributions(data_iat, distributions)\n\nWarning: Skipped distributions due to data constraints: weibull\n\n\n      exp     gamma \n0.1154607 0.2061950 \n\nfit_distributions(data_service, distributions)\n\nWarning: Skipped distributions due to data constraints: weibull\n\n\n       exp      gamma \n0.04796992 0.09755396 \n\n\n\nUnsurprisingly, the best fit for both is the exponential distribution (lowest test statistic).\nWe can create a version of our histograms from before but with the distributions overlaid, to visually support this.\n\n\n\ndef inspect_histogram_with_fits(series, x_lab, dist_name):\n    \"\"\"\n    Plot histogram with overlaid fitted distributions.\n\n    Parameters\n    ----------\n    series : pd.Series\n        Series containing the values to plot as a histogram.\n    x_lab : str\n        X axis label.\n    dist_name : str\n        Name of the distributions in scipy.stats to fit, eg. \"expon\"\n\n    Returns\n    -------\n    fig : plotly.graph_objects.Figure\n    \"\"\"\n    # Plot histogram with probability density normalisation\n    fig = px.histogram(series, nbins=30, histnorm=\"probability density\")\n    fig.update_layout(\n        xaxis_title=x_lab, showlegend=True, width=700, height=400\n    )\n\n    # Fit and plot each distribution\n    x = np.linspace(series.min(), series.max(), 1000)\n    dist = getattr(stats, dist_name)\n    params = dist.fit(series.dropna())\n    pdf_fitted = dist.pdf(x, *params[:-2], loc=params[-2], scale=params[-1])\n    fig.add_trace(go.Scatter(x=x, y=pdf_fitted, mode=\"lines\", name=dist_name))\n\n    return fig\n\n\n\n# Plot histograms with fitted distributions\np = inspect_histogram_with_fits(series=data[\"iat_mins\"].dropna(),\n                                x_lab=\"Inter-arrival time (min)\",\n                                dist_name=\"expon\")\np.show()\n\n                            \n                                            \n\np = inspect_histogram_with_fits(series=data[\"service_mins\"],\n                                x_lab=\"Service time (min)\",\n                                dist_name=\"expon\")\np.show()\n\n                            \n                                            \n\n\n\n\nThe simplest way to do this is to just use the plotting functions from fitdistrplus.\n\n\n# Fit and create plot for IAT\niat_exp &lt;- suppressWarnings(fitdist(data_iat, \"exp\"))\ndenscomp(iat_exp, legendtext = \"Exponential\")\n\n\n\n\n\n\n\n# Fit and create plot for service\nser_exp &lt;- suppressWarnings(fitdist(data_service, \"exp\"))\ndenscomp(ser_exp, legendtext = \"Exponential\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo implement the comprehensive approach using distfit‚Ä¶\n\n\n# Fit popular distributions to inter-arrivals times\ndfit_iat = distfit(distr=\"popular\", stats=\"RSS\", verbose=\"silent\")\n_ = dfit_iat.fit_transform(data[\"iat_mins\"].dropna())\n\n# Fit popular distributions to service times\ndfit_service = distfit(distr=\"popular\", stats=\"RSS\", verbose=\"silent\")\n_ = dfit_service.fit_transform(data[\"service_mins\"])\n\nWe can view a summary table from distfit.\nThe score column is the result from a goodness-of-fit test. This is set using stats in distfit (e.g.¬†distfit(stats=\"RSS\")). It provides several possible tests including:\n\nRSS - residual sum of squares\nwasserstein - Wasserstein distance\nks - Kolmogorov-Smirnov statistic\nenergy - energy distance\ngoodness_of_fit - general purpose test from scipy.stats.goodness_of_fit\n\nFor continuous data, ks is often a good choice - but for distfit, they use an unusual method for calculation of this statistic. In distfit, they resample from the fitted distribution and compare that to the original data. Meanwhile, our manual implementation just use the standard KS test from scipy.stats, which is the standard KS statistics that is commonly understood.\nAs such, we have left distfit with RSS. However, we can calculate a standard KS statistic ourselves using the function below - which, as we can see, matches up with our results above.\n\n\ndef calculate_ks(input_series, dfit_summary):\n    \"\"\"\n    Calculate standard Kolmogorov-Smirnov statistics for fitted distributions.\n\n    This function applies the standard scipy.stats.kstest to data using the\n    distribution parameters obtained from distfit, providing conventional\n    KS statistics rather than distfit's resampling-based approach.\n\n    Parameters\n    ----------\n    input_series : pandas.Series\n        The original data series used for distribution fitting.\n    dfit_summary : pandas.DataFrame\n        The summary DataFrame from a distfit object, containing fitted\n        distribution names and parameters.\n\n    Returns\n    -------\n    pandas.DataFrame\n        A DataFrame containing the distribution names, KS statistics,\n        and p-values from the standard KS test.\n\n    Notes\n    -----\n    Lower KS statistic values indicate better fits to the data.\n    \"\"\"\n    results = []\n    for _, row in dfit_summary.iterrows():\n        dist_name = row[\"name\"]\n        dist_params = row[\"params\"]\n\n        # Perform KS test using scipy.stats.kstest\n        ks_result = stats.kstest(input_series, dist_name, args=dist_params)\n\n        # Store the results\n        results.append({\n            \"name\": dist_name,\n            \"ks\": ks_result.statistic[0],\n            \"p_value\": ks_result.pvalue[0]\n        })\n\n    # Create a DataFrame with the results\n    return pd.DataFrame(results).sort_values(by=\"ks\")\n\n\n\ncalculate_ks(input_series=data[[\"iat_mins\"]].dropna(),\n             dfit_summary=dfit_iat.summary)\n\n          name        ks  p_value\n4   genextreme  0.115385      0.0\n3       pareto  0.115461      0.0\n2        expon  0.115461      0.0\n6     dweibull  0.158115      0.0\n5            t  0.160045      0.0\n8     loggamma  0.177966      0.0\n7         norm  0.179646      0.0\n0         beta  0.199921      0.0\n1        gamma  0.209314      0.0\n10     lognorm  0.523766      0.0\n9      uniform  0.703668      0.0\n\n\n\n\ncalculate_ks(input_series=data[[\"service_mins\"]],\n             dfit_summary=dfit_service.summary)\n\n          name        ks        p_value\n1       pareto  0.047970  3.085744e-264\n0        expon  0.047970  3.084811e-264\n3   genextreme  0.070976   0.000000e+00\n2         beta  0.092614   0.000000e+00\n4        gamma  0.122589   0.000000e+00\n7         norm  0.158105   0.000000e+00\n5            t  0.160056   0.000000e+00\n8     loggamma  0.161955   0.000000e+00\n6     dweibull  0.175325   0.000000e+00\n10     lognorm  0.536170   0.000000e+00\n9      uniform  0.713362   0.000000e+00\n\n\n\nThe distfit package has some nice visualisation functions. For example, using the inter-arrival times‚Ä¶\n\n\n# PDF with all the distributions overlaid\nfig, ax = dfit_iat.plot(n_top=11, figsize=(7, 4))\nfig.show()\n\n\n\n\n\n\n\n\n\n\n# CDF with all the distributions overlaid\nfig, ax = dfit_iat.plot(chart=\"cdf\", n_top=11, figsize=(7, 4))\nfig.show()\n\n\n\n\n\n\n\n\n\n\n# QQ plot with all distributions overlaid\nfig, ax = dfit_iat.qqplot(data[\"iat_mins\"].dropna(), n_top=11, figsize=(7, 4))\nfig.show()\n\n\n\n\n\n\n\n\n\n\n# Summary plot using the RSS\nfig, ax = dfit_iat.plot_summary(figsize=(7, 4))\nfig.show()\n\n\n\n\n\n\n\n\nWe can also use it to create a plot with a specific distribution overlaid, like in the targeted approach:\n\n\n# To create a plot with a specific distribution overlaid...\ndfit = distfit(distr=\"expon\")\n_ = dfit.fit_transform(data[\"iat_mins\"].dropna())\nfig, ax = dfit.plot(figsize=(7, 4))\nfig.show()\n\n\n\n\n\n\n\n\n\n\nThe fitdistrplus package does not have a built-in function to automatically fit a large set of distributions in a single command. Instead, we just need to specify a list of candidate distributions.\n\n\n# Continuous distributions supported natively by fitdist\n# (you could use other packages to get other distributions to test)\ndistributions &lt;- c(\"norm\", \"lnorm\", \"exp\", \"cauchy\", \"gamma\", \"logis\", \"beta\",\n                   \"weibull\", \"unif\")\nfit_distributions(data_iat, distributions)\n\nWarning: Skipped distributions due to data constraints: lnorm, beta, weibull\n\n\n     norm       exp    cauchy     gamma     logis      unif \n0.1796464 0.1154607 0.1968200 0.2061950 0.1564321 0.7036682 \n\nfit_distributions(data_service, distributions)\n\nWarning: Skipped distributions due to data constraints: lnorm, beta, weibull\n\n\n      norm        exp     cauchy      gamma      logis       unif \n0.15810466 0.04796992 0.19728636 0.09755396 0.15566197 0.71336153 \n\n\nAgain, exponential is returned as the best fit.\n\n\n\nThe fitdistrplus package also has some nice visualisation functions.\n\n\niat_exp &lt;- suppressWarnings(fitdist(data_iat, \"exp\"))\nplot(iat_exp)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing the targeted approach, we found exponential to be the best. Using the comprehensive approach, there were a few distributions that were all very low scores (pareto, expon, genextreme). Choosing between these‚Ä¶\n\nExponential - simple, wide application, good for context, fewer parameters\nGeneralised pareto - useful if data has heavier tail (not the case here)\nGeneralised extreme value - more complex, spefically designed for modeling maximum values or extreme events\n\nAs such, exponential seems a good choice for inter-arrival and service times.\n\n\nUsing the targeted and comprehensive approach, we found exponential to be best for inter-arrival and service times.\n\n\n\n\n\nThe exponential distribution is defined by a single parameter, but this parameter can be expressed in two ways - as the:\n\nMean (also called the scale) - this is just your sample mean.\nRate (also called lambda Œª) - this is calculated as 1 / mean.\n\n\nWe will use the Exponential class from sim-tools, which uses the numpy.random.exponential() function. That accepts the scale parameter, so we just need to calculate the sample mean.\nMean:\n\nInter-arrival time: 4 minutes.\nService time: 10 minutes.\n\n\n\nprint(data[\"iat_mins\"].dropna().mean())\n\n3.9843611416442406\n\nprint(data[\"service_mins\"].dropna().mean())\n\n9.991570393280572\n\n\n\n\nWe will use the rexp() function from the stats package which requires the rate parameter, not the mean.\nRate:\n\nInter-arrival time: 0.25\nService time: 0.1\n\n\n\n1 / mean(data_iat)\n\n[1] 0.2509813\n\n1 / mean(data_service)\n\n[1] 0.1000844\n\n\n\nFor guidance on storing and using these parameters, see the pages ?@sec-param_file and ?@sec-param_script.\n\n\n\n\n\n\n‚ÄúSimulation modelling for stochastic systems lab 3‚Äù from Tom Monks 2024.\n‚ÄúChapter 7: Data Collection and Analysis‚Äù from ‚ÄúSimulation: The Practice of Model Development and Use‚Äù by Stewart Robinson 2007.",
    "crumbs": [
      "Model inputs",
      "Input modelling"
    ]
  },
  {
    "objectID": "pages/inputs/input_modelling.html#data",
    "href": "pages/inputs/input_modelling.html#data",
    "title": "Input modelling",
    "section": "",
    "text": "To build a DES model, you first need data that reflects the system you want to model. In healthcare, this might mean you need to access healthcare records with patient arrival, service and departure times, for example. The quality of your simulation depends directly on the quality of your data. Key considerations include:\n\nAccuracy. Inaccurate data leads to inaccurate simulation results.\nSample size. Small samples can give misleading results if they capture unusual periods, lack variability, or are affected by outliers.\nLevel of detail. The data must be granular enough for your needs. For example, daily totals may be insufficient if you want to model hourly arrivals (although may still be possible if know distribution - see Section¬†1.3)\nRepresentative. The data should reflect the current system. For instance, data from the COVID-19 period may not represent typical operations.",
    "crumbs": [
      "Model inputs",
      "Input modelling"
    ]
  },
  {
    "objectID": "pages/inputs/input_modelling.html#how-is-this-data-used-in-the-model",
    "href": "pages/inputs/input_modelling.html#how-is-this-data-used-in-the-model",
    "title": "Input modelling",
    "section": "",
    "text": "Discrete-event simulation (DES) models are stochastic, which means they incorporate random variation, to reflect the inherent variability of real-world systems.\nInstead of using fixed times for events (like having a patient arrive exactly every five minutes), DES models pick the timing of events by randomly sampling values from a probability distribution.\nThe process of selecting the most appropriate statistical distributions to use in your model is called input modelling.",
    "crumbs": [
      "Model inputs",
      "Input modelling"
    ]
  },
  {
    "objectID": "pages/inputs/input_modelling.html#sec-input_modelling-options",
    "href": "pages/inputs/input_modelling.html#sec-input_modelling-options",
    "title": "Input modelling",
    "section": "",
    "text": "When selecting appropriate distributions, if you only have summary statistics (like the mean), you may need to rely on expert opinion or the general properties of the process you‚Äôre modelling. For example:\n\nArrivals: Random independent arrivals are often modelled with the Poisson distribution, whilst their inter-arrival times are modelled using an exponential distribution (Pishro-Nik (2014)).\nLength of stay: Length of stay is commonly right skewed (Lee, Fung, and Fu (2003)), and so will often be modelled with distributions like exponential, gamma, log-normal (for log-transformed length of stay) or Weibull.\n\nHowever, these standard choices may not always be appropriate. If the actual process differs from your assumptions or has unique features, the chosen distribution might not fit well.\nTherefore, if you have enough data, it‚Äôs best to analyse it directly to select the most suitable distributions. This analysis generally involves two key steps:\n\nIdentify possible distributions. This is based on knowledge of the process being modelled, and by inspecting the data using time series plots and histograms.\nFit distributions to your data and compare goodness-of fit. You can do this using a:\n\nTargeted approach. Just test the distributions from step 1.\nComprehensive approach. Test a wide range of distributions.\n\n\nThough the comprehensive approach tests lots of different distributions, it‚Äôs still important to do step 1 as:\n\nIt‚Äôs important to be aware of temporal patterns in the data (e.g.¬†spikes in service length every Friday).\nYou may find distributions which mathematically fit but are contextually inappropriate (e.g.¬†normal distribution for service times, which can‚Äôt be negative).\nYou may find better fit for complex distributions, even when simpler are sufficient.\n\nWe‚Äôll demonstrate steps for input modelling below using synthetic arrival data from the nurse visit simulation (?@sec-examples). In this case, we already know which distributions to use (as we sampled from them to create our synthetic data!). However, the steps still illustrate how you might select distributions in practice with real data.\n\nWe‚Äôll create a Jupyter Notebook to perform this analysis in.\ntouch notebooks/input_modelling.ipynb\n\n\nWe‚Äôll create a R Markdown file to perform this analysis in.\ntouch rmarkdown/input_modelling.Rmd\n\n\n\n\nYou first need to select which distributions to fit to your data. You should both:\n\nConsider the known properties of the process being modelled (as above), and-\nInspect the data by plotting a histogram.\n\nRegarding the known properties, it would be good to consider the exponential distribution for our arrivals, as that is a common choice for random independent arrivals.\nTo inspect the data, we will create two plots:\n\n\n\n\n\n\n\n\nPlot type\nWhat does it show?\nWhy do we create this plot?\n\n\n\n\nTime series\nTrends, seasonality, and outliers (e.g., spikes or dips over time).\nTo check for stationarity (i.e.¬†no trends or sudden changes). Stationary is an assumption of many distributions, and if trends or anomalies do exist, we may need to exclude certain periods or model them separately. The time series can also be useful for spotting outliers and data gaps.\n\n\nHistogram\nThe shape of the data‚Äôs distribution.\nHelps identify which distributions might fit the data.\n\n\n\nWe repeat this for the arrivals and service (nurse consultation) time, so have created functions to avoid duplicate code between each.\n\nFirst, we import the data.\n\n\n\n# pylint: disable=missing-module-docstring\n# Import required packages\nfrom distfit import distfit\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.io as pio\nimport scipy.stats as stats\n\n\n\n# Import data\ndata = pd.read_csv(\"../../data/NHS_synthetic.csv\", dtype={\n    \"ARRIVAL_TIME\": str,\n    \"SERVICE_TIME\": str,\n    \"DEPARTURE_TIME\": str\n})\n\n# Preview data\ndata.head()\n\n  ARRIVAL_DATE ARRIVAL_TIME  ... DEPARTURE_DATE DEPARTURE_TIME\n0   2025-01-01         0001  ...     2025-01-01           0012\n1   2025-01-01         0002  ...     2025-01-01           0007\n2   2025-01-01         0003  ...     2025-01-01           0030\n3   2025-01-01         0007  ...     2025-01-01           0022\n4   2025-01-01         0010  ...     2025-01-01           0031\n\n[5 rows x 6 columns]\n\n\n\n\n\n\n# nolint start: undesirable_function_linter.\n# Import required packages\nlibrary(dplyr)\nlibrary(fitdistrplus) \nlibrary(ggplot2)\nlibrary(lubridate)\nlibrary(plotly)\nlibrary(readr)\nlibrary(tidyr)\n# nolint end\n\n\n\n# Import data\ndata &lt;- read_csv(\n    file.path(\"..\", \"..\", \"data\", \"NHS_synthetic.csv\"), show_col_types = FALSE\n)\n\n# Preview data\nhead(data)\n\n# A tibble: 6 √ó 6\n  ARRIVAL_DATE ARRIVAL_TIME SERVICE_DATE SERVICE_TIME DEPARTURE_DATE\n  &lt;date&gt;       &lt;chr&gt;        &lt;date&gt;       &lt;chr&gt;        &lt;date&gt;        \n1 2025-01-01   0001         2025-01-01   0007         2025-01-01    \n2 2025-01-01   0002         2025-01-01   0004         2025-01-01    \n3 2025-01-01   0003         2025-01-01   0010         2025-01-01    \n4 2025-01-01   0007         2025-01-01   0014         2025-01-01    \n5 2025-01-01   0010         2025-01-01   0012         2025-01-01    \n6 2025-01-01   0010         2025-01-01   0011         2025-01-01    \n# ‚Ñπ 1 more variable: DEPARTURE_TIME &lt;chr&gt;\n\n\n\n\nWe then calculate the inter-arrival times.\n\n\n\n# Combine date/time and convert to datetime\ndata[\"arrival_datetime\"] = pd.to_datetime(\n    data[\"ARRIVAL_DATE\"] + \" \" + data[\"ARRIVAL_TIME\"].str.zfill(4),\n    format=\"%Y-%m-%d %H%M\"\n)\n\n# Sort by arrival time and calculate inter-arrival times\ndata = data.sort_values(\"arrival_datetime\")\ndata[\"iat_mins\"] = (\n    data[\"arrival_datetime\"].diff().dt.total_seconds() / 60\n)\n\n# Preview\ndata[[\"ARRIVAL_DATE\", \"ARRIVAL_TIME\", \"arrival_datetime\", \"iat_mins\"]].head()\n\n  ARRIVAL_DATE ARRIVAL_TIME    arrival_datetime  iat_mins\n0   2025-01-01         0001 2025-01-01 00:01:00       NaN\n1   2025-01-01         0002 2025-01-01 00:02:00       1.0\n2   2025-01-01         0003 2025-01-01 00:03:00       1.0\n3   2025-01-01         0007 2025-01-01 00:07:00       4.0\n4   2025-01-01         0010 2025-01-01 00:10:00       3.0\n\n\n\n\n\n\ndata &lt;- data %&gt;%\n  # Combine date/time and convert to datetime\n  mutate(arrival_datetime = ymd_hm(paste(ARRIVAL_DATE, ARRIVAL_TIME))) %&gt;%\n  # Sort by arrival time\n  arrange(arrival_datetime) %&gt;%\n  # Calculate inter-arrival times\n  mutate(\n    iat_mins = as.numeric(\n      difftime(\n        arrival_datetime, lag(arrival_datetime), units = \"mins\"\n      )\n    )\n  )\n\n# Preview\ndata %&gt;%\n  select(ARRIVAL_DATE, ARRIVAL_TIME, arrival_datetime, iat_mins) %&gt;%\n  head()\n\n# A tibble: 6 √ó 4\n  ARRIVAL_DATE ARRIVAL_TIME arrival_datetime    iat_mins\n  &lt;date&gt;       &lt;chr&gt;        &lt;dttm&gt;                 &lt;dbl&gt;\n1 2025-01-01   0001         2025-01-01 00:01:00       NA\n2 2025-01-01   0002         2025-01-01 00:02:00        1\n3 2025-01-01   0003         2025-01-01 00:03:00        1\n4 2025-01-01   0007         2025-01-01 00:07:00        4\n5 2025-01-01   0010         2025-01-01 00:10:00        3\n6 2025-01-01   0010         2025-01-01 00:10:00        0\n\n\n\n\nWe also calculate the service times.\n\n\n\n# Combine dates with times\ndata[\"service_datetime\"] = pd.to_datetime(\n    data[\"SERVICE_DATE\"] + \" \" + data[\"SERVICE_TIME\"].str.zfill(4)\n)\ndata[\"departure_datetime\"] = pd.to_datetime(\n    data[\"DEPARTURE_DATE\"] + \" \" + data[\"DEPARTURE_TIME\"].str.zfill(4)\n)\n\n# Calculate time difference in minutes\ntime_delta = data[\"departure_datetime\"] - data[\"service_datetime\"]\ndata[\"service_mins\"] = time_delta / pd.Timedelta(minutes=1)\n\n# Preview\ndata[[\"service_datetime\", \"departure_datetime\", \"service_mins\"]].head()\n\n     service_datetime  departure_datetime  service_mins\n0 2025-01-01 00:07:00 2025-01-01 00:12:00           5.0\n1 2025-01-01 00:04:00 2025-01-01 00:07:00           3.0\n2 2025-01-01 00:10:00 2025-01-01 00:30:00          20.0\n3 2025-01-01 00:14:00 2025-01-01 00:22:00           8.0\n4 2025-01-01 00:12:00 2025-01-01 00:31:00          19.0\n\n\n\n\n\n\ndata &lt;- data %&gt;%\n  mutate(\n    service_datetime   = ymd_hm(paste(SERVICE_DATE, SERVICE_TIME)),\n    departure_datetime = ymd_hm(paste(DEPARTURE_DATE, DEPARTURE_TIME)),\n    service_mins = as.numeric(\n      difftime(departure_datetime, service_datetime, units = \"mins\")\n    )\n  )\n\n# Preview\ndata %&gt;% select(service_datetime, departure_datetime, service_mins) %&gt;% head()\n\n# A tibble: 6 √ó 3\n  service_datetime    departure_datetime  service_mins\n  &lt;dttm&gt;              &lt;dttm&gt;                     &lt;dbl&gt;\n1 2025-01-01 00:07:00 2025-01-01 00:12:00            5\n2 2025-01-01 00:04:00 2025-01-01 00:07:00            3\n3 2025-01-01 00:10:00 2025-01-01 00:30:00           20\n4 2025-01-01 00:14:00 2025-01-01 00:22:00            8\n5 2025-01-01 00:12:00 2025-01-01 00:31:00           19\n6 2025-01-01 00:11:00 2025-01-01 00:14:00            3\n\n\n\n\nTime series. For this data, we observe no trends, seasonality or outliers.\n\n\n\ndef inspect_time_series(time_series, y_lab):\n    \"\"\"\n    Plot time-series.\n\n    Parameters\n    ----------\n    time_series : pd.Series\n        Series containing the time series data (where index is the date).\n    y_lab : str\n        Y axis label.\n\n    Returns\n    -------\n    fig : plotly.graph_objects.Figure\n    \"\"\"\n    # Label as \"Date\" and provided y_lab, and convert to dataframe\n    df = time_series.rename_axis(\"Date\").reset_index(name=y_lab)\n\n    # Create plot\n    fig = px.line(df, x=\"Date\", y=y_lab)\n    fig.update_layout(showlegend=False, width=700, height=400)\n    return fig\n\n\n\n# Calculate mean arrivals per day and plot time series\np = inspect_time_series(\n    time_series=data.groupby(by=[\"ARRIVAL_DATE\"]).size(),\n    y_lab=\"Number of arrivals\")\n\np.show()\n\n                            \n                                            \n\n\n\n\n# Calculate mean service length per day, dropping last day (incomplete)\ndaily_service = data.groupby(\"SERVICE_DATE\")[\"service_mins\"].mean()\ndaily_service = daily_service.iloc[:-1]\n\n# Plot mean service length each day\np = inspect_time_series(time_series=daily_service,\n                        y_lab=\"Mean consultation length (min)\")\np.show()\n\n                            \n                                            \n\n\n\n\n\n\ninspect_time_series &lt;- function(\n  time_series, date_col, value_col, y_lab, interactive, save_path = NULL\n) {\n  #' Plot time-series\n  #'\n  #' @param time_series Dataframe with date column and numeric column to plot.\n  #' @param date_col String. Name of column with dates.\n  #' @param value_col String. Name of column with numeric values.\n  #' @param y_lab String. Y axis label.\n  #' @param interactive Boolean. Whether to render interactive or static plot.\n  #' @param save_path String. Path to save static file to (inc. name and\n  #' filetype). If NULL, then will not save.\n\n  # Create custom tooltip text\n  time_series$tooltip_text &lt;- paste0(\n    \"&lt;span style='color:white'&gt;\",\n    \"Date: \", time_series[[date_col]], \"&lt;br&gt;\",\n    y_lab, \": \", time_series[[value_col]], \"&lt;/span&gt;\"\n  )\n\n  # Create plot\n  p &lt;- ggplot(time_series, aes(x = .data[[date_col]],\n                               y = .data[[value_col]],\n                               text = tooltip_text)) +  # nolint: object_usage_linter\n    geom_line(group = 1L, color = \"#727af4\") +\n    labs(x = \"Date\", y = y_lab) +\n    theme_minimal()\n\n  # Save file if path provided\n  if (!is.null(save_path)) {\n    ggsave(save_path, p, width = 7L, height = 4L)\n  }\n\n  # Display as interactive or static figure\n  if (interactive) {\n    ggplotly(p, tooltip = \"text\", width = 700L, height = 400L)\n  } else {\n    p\n  }\n}\n\n\n\n# Plot daily arrivals\ndaily_arrivals &lt;- data %&gt;% group_by(ARRIVAL_DATE) %&gt;% count()\ninspect_time_series(\n  time_series = daily_arrivals, date_col = \"ARRIVAL_DATE\", value_col = \"n\",\n  y_lab = \"Number of arrivals\", interactive = TRUE\n)\n\n\n\n\n\n\n\n# Calculate mean service length per day, dropping last day (incomplete)\ndaily_service &lt;- data %&gt;%\n  group_by(SERVICE_DATE) %&gt;%\n  summarise(mean_service = mean(service_mins)) %&gt;%\n  filter(row_number() &lt;= n() - 1L)\n\n# Plot mean service length each day\ninspect_time_series(\n  time_series = daily_service, date_col = \"SERVICE_DATE\",\n  value_col = \"mean_service\", y_lab = \"Mean consultation length (min)\",\n  interactive = TRUE\n)\n\n\n\n\n\n\n\nHistogram. For both inter-arrival times and service times, we observe a right skewed distribution. Hence, it would be good to try exponential, gamma and Weibull distributions.\n\n\n\ndef inspect_histogram(series, x_lab):\n    \"\"\"\n    Plot histogram.\n\n    Parameters\n    ----------\n    series : pd.Series\n        Series containing the values to plot as a histogram.\n    x_lab : str\n        X axis label.\n\n    Returns\n    -------\n    fig : plotly.graph_objects.Figure\n    \"\"\"\n    fig = px.histogram(series)\n    fig.update_layout(\n        xaxis_title=x_lab, showlegend=False, width=700, height=400\n    )\n    fig.update_traces(\n        hovertemplate=x_lab + \": %{x}&lt;br&gt;Count: %{y}\", name=\"\"\n    )\n    return fig\n\n\n\n# Plot histogram of inter-arrival times\np = inspect_histogram(series=data[\"iat_mins\"],\n                      x_lab=\"Inter-arrival time (min)\")\np.show()\n\n                            \n                                            \n\n\n\n\n# Plot histogram of service times\np = inspect_histogram(series=data[\"service_mins\"],\n                      x_lab=\"Consultation length (min)\")\np.show()\n\n                            \n                                            \n\n\n\n\n\n\ninspect_histogram &lt;- function(\n  data, var, x_lab, interactive, save_path = NULL\n) {\n  #' Plot histogram\n  #'\n  #' @param data A dataframe or tibble containing the variable to plot.\n  #' @param var String. Name of the column to plot as a histogram.\n  #' @param x_lab String. X axis label.\n  #' @param interactive Boolean. Whether to render interactive or static plot.\n  #' @param save_path String. Path to save static file to (inc. name and\n  #' filetype). If NULL, then will not save.\n\n  # Remove non-finite values\n  data &lt;- data[is.finite(data[[var]]), ]\n\n  # Create plot\n  p &lt;- ggplot(data, aes(x = .data[[var]])) +\n    geom_histogram(aes(text = paste0(\"&lt;span style='color:white'&gt;\", x_lab, \": \",\n                                     round(after_stat(x), 2L), \"&lt;br&gt;Count: \",  # nolint: object_usage_linter\n                                     after_stat(count), \"&lt;/span&gt;\")),\n                   fill = \"#727af4\", bins = 30L) +\n    labs(x = x_lab, y = \"Count\") +\n    theme_minimal() +\n    theme(legend.position = \"none\")\n\n  # Save file if path provided\n  if (!is.null(save_path)) {\n    ggsave(save_path, p, width = 7L, height = 4L)\n  }\n\n  # Display as interactive or static figure\n  if (interactive) {\n    ggplotly(p, tooltip = \"text\", width = 700L, height = 400L)\n  } else {\n    p\n  }\n}\n\n\n\n# Plot histogram of inter-arrival times\ninspect_histogram(\n  data = data, var = \"iat_mins\", x_lab = \"Inter-arrival time (min)\",\n  interactive = TRUE\n)\n\n\n\n\n# Plot histogram of service times\ninspect_histogram(\n  data = data, var = \"service_mins\", x_lab = \"Consultation length (min)\",\n  interactive = TRUE\n)\n\n\n\n\n\n\nAlternative: You can use the fitdistrplus package to create these histograms - as well as the empirical cumulative distribution function (CDF), which can help you inspect the tails, central tendency, and spot jumps or plateaus in the data.\n\n\n# Get IAT and service time columns as numeric vectors (with NA dropped)\ndata_iat &lt;- data %&gt;% drop_na(iat_mins) %&gt;% select(iat_mins) %&gt;% pull()\ndata_service &lt;- data %&gt;% select(service_mins) %&gt;% pull()\n\n# Plot histograms and CDFs\nplotdist(data_iat, histo = TRUE, demp = TRUE)\n\n\n\n\n\n\n\nplotdist(data_service, histo = TRUE, demp = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe will fit distributions and assess goodness-of-fit using the Kolmogorov-Smirnov (KS) Test. This is a common test which is well-suited to continuous distributions. For categorical (or binned) data, consider using chi-squared tests.\nThe KS Test returns a statistic and p value.\n\nStatistic: Measures how well the distribution fits your data.\n\nHigher values indicate a better fit.\nRanges from 0 to 1.\n\nP-value: Tells you if the fit could have happened by chance.\n\nHigher p-values suggest the data follow the distribution.\nIn large datasets, even good fits often have small p-values.\nRanges from 0 to 1.\n\n\n\nscipy.stats is a popular library for fitting and testing statistical distributions. For more convenience, distfit, which is built on scipy, is another popular package which can test multiple distributions simultaneously (or evaluate specific distributions).\nWe will illustrate how to perform the targeted approach using scipy directly, and the comprehensive approach using distfit - but you could use either for each approach.\n\n\nfitdistrplus is a popular library for fitting and testing statistical distributions. This can evaluate specific distributions or test multiple distributions. We will use this library to illustrate how to perform the targeted or comprehensive approach.\n\n\n\n\n\nTo implement the targeted approach using scipy.stats‚Ä¶\n\n\ndef fit_distributions(input_series, dists):\n    \"\"\"\n    This function fits statistical distributions to the provided data and\n    performs Kolmogorov-Smirnov tests to assess the goodness of fit.\n\n    Parameters\n    ----------\n    input_series : pandas.Series\n        The observed data to fit the distributions to.\n    dists : list\n        List of the distributions in scipy.stats to fit, eg. [\"expon\", \"gamma\"]\n\n    Notes\n    -----\n    A lower test statistic and higher p-value indicates better fit to the data.\n    \"\"\"\n    for dist_name in dists:\n        # Fit distribution to the data\n        dist = getattr(stats, dist_name)\n        params = dist.fit(input_series)\n\n        # Return results from Kolmogorov-Smirnov test\n        ks_result = stats.kstest(input_series, dist_name, args=params)\n        print(f\"Kolmogorov-Smirnov statistic for {dist_name}: \" +\n            f\"{ks_result.statistic:.4f} (p={ks_result.pvalue:.2e})\")\n\n\n# Fit and run Kolmogorov-Smirnov test on the inter-arrival and service times\ndistributions = [\"expon\", \"gamma\", \"weibull_min\"]\n\nInter-arrival time:\n\n\nfit_distributions(input_series=data[\"iat_mins\"].dropna(), dists=distributions)\n\nKolmogorov-Smirnov statistic for expon: 0.1155 (p=0.00e+00)\nKolmogorov-Smirnov statistic for gamma: 0.2093 (p=0.00e+00)\nKolmogorov-Smirnov statistic for weibull_min: 0.1355 (p=0.00e+00)\n\n\nService time:\n\n\nfit_distributions(input_series=data[\"service_mins\"], dists=distributions)\n\nKolmogorov-Smirnov statistic for expon: 0.0480 (p=3.08e-264)\nKolmogorov-Smirnov statistic for gamma: 0.1226 (p=0.00e+00)\nKolmogorov-Smirnov statistic for weibull_min: 0.0696 (p=0.00e+00)\n\n\n\n\nWe have several of zeros (as times are rounded to nearest minute, and arrivals are frequent / service times can be short). Weibull is only defined for positive values, so we won‚Äôt try that. We have built in error-handling to fit_distributions to ensure that.\n\n\n# Percentage of inter-arrival times that are 0\npaste0(round(sum(data_iat == 0L) / length(data_iat) * 100L, 2L), \"%\")\n\n[1] \"11.55%\"\n\npaste0(round(sum(data_service == 0L) / length(data_service) * 100L, 2L), \"%\")\n\n[1] \"4.8%\"\n\n\n\n\nfit_distributions &lt;- function(data, dists) {\n  #' Compute Kolmogorov-Smirnov Statistics for Fitted Distributions\n  #'\n  #' @param data Numeric vector. The data to fit distributions to.\n  #' @param dists Character vector. Names of distributions to fit.\n  #'\n  #' @return Named numeric vector of Kolmogorov-Smirnov statistics, one per\n  #' distribution.\n\n  # Define distribution requirements\n  positive_only &lt;- c(\"lnorm\", \"weibull\")\n  non_negative &lt;- c(\"exp\", \"gamma\")\n  zero_to_one &lt;- \"beta\"\n\n  # Check data characteristics\n  has_negatives &lt;- any(data &lt; 0L)\n  has_zeros &lt;- any(data == 0L)\n  has_out_of_beta_range &lt;- any(data &lt; 0L | data &gt; 1L)\n\n  # Filter distributions based on data\n  valid_dists &lt;- dists\n  if (has_negatives || has_zeros) {\n    valid_dists &lt;- setdiff(valid_dists, positive_only)\n  }\n  if (has_negatives) {\n    valid_dists &lt;- setdiff(valid_dists, non_negative)\n  }\n  if (has_out_of_beta_range) {\n    valid_dists &lt;- setdiff(valid_dists, zero_to_one)\n  }\n\n  # Warn about skipped distributions\n  skipped &lt;- setdiff(dists, valid_dists)\n  if (length(skipped) &gt; 0L) {\n    warning(\"Skipped distributions due to data constraints: \",\n            toString(skipped), call. = FALSE)\n  }\n\n  # Exit early if no valid distributions remain\n  if (length(valid_dists) == 0L) {\n    warning(\"No valid distributions to test after filtering\", call. = FALSE)\n    return(numeric(0L))\n  }\n\n  # Fit remaining distributions\n  fits &lt;- lapply(\n    valid_dists, function(dist) suppressWarnings(fitdist(data, dist))\n  )\n  gof_results &lt;- gofstat(fits, fitnames = valid_dists)\n\n  # Return KS statistics\n  gof_results$ks\n}\n\n\ndistributions &lt;- c(\"exp\", \"gamma\", \"weibull\")\nfit_distributions(data_iat, distributions)\n\nWarning: Skipped distributions due to data constraints: weibull\n\n\n      exp     gamma \n0.1154607 0.2061950 \n\nfit_distributions(data_service, distributions)\n\nWarning: Skipped distributions due to data constraints: weibull\n\n\n       exp      gamma \n0.04796992 0.09755396 \n\n\n\nUnsurprisingly, the best fit for both is the exponential distribution (lowest test statistic).\nWe can create a version of our histograms from before but with the distributions overlaid, to visually support this.\n\n\n\ndef inspect_histogram_with_fits(series, x_lab, dist_name):\n    \"\"\"\n    Plot histogram with overlaid fitted distributions.\n\n    Parameters\n    ----------\n    series : pd.Series\n        Series containing the values to plot as a histogram.\n    x_lab : str\n        X axis label.\n    dist_name : str\n        Name of the distributions in scipy.stats to fit, eg. \"expon\"\n\n    Returns\n    -------\n    fig : plotly.graph_objects.Figure\n    \"\"\"\n    # Plot histogram with probability density normalisation\n    fig = px.histogram(series, nbins=30, histnorm=\"probability density\")\n    fig.update_layout(\n        xaxis_title=x_lab, showlegend=True, width=700, height=400\n    )\n\n    # Fit and plot each distribution\n    x = np.linspace(series.min(), series.max(), 1000)\n    dist = getattr(stats, dist_name)\n    params = dist.fit(series.dropna())\n    pdf_fitted = dist.pdf(x, *params[:-2], loc=params[-2], scale=params[-1])\n    fig.add_trace(go.Scatter(x=x, y=pdf_fitted, mode=\"lines\", name=dist_name))\n\n    return fig\n\n\n\n# Plot histograms with fitted distributions\np = inspect_histogram_with_fits(series=data[\"iat_mins\"].dropna(),\n                                x_lab=\"Inter-arrival time (min)\",\n                                dist_name=\"expon\")\np.show()\n\n                            \n                                            \n\np = inspect_histogram_with_fits(series=data[\"service_mins\"],\n                                x_lab=\"Service time (min)\",\n                                dist_name=\"expon\")\np.show()\n\n                            \n                                            \n\n\n\n\nThe simplest way to do this is to just use the plotting functions from fitdistrplus.\n\n\n# Fit and create plot for IAT\niat_exp &lt;- suppressWarnings(fitdist(data_iat, \"exp\"))\ndenscomp(iat_exp, legendtext = \"Exponential\")\n\n\n\n\n\n\n\n# Fit and create plot for service\nser_exp &lt;- suppressWarnings(fitdist(data_service, \"exp\"))\ndenscomp(ser_exp, legendtext = \"Exponential\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo implement the comprehensive approach using distfit‚Ä¶\n\n\n# Fit popular distributions to inter-arrivals times\ndfit_iat = distfit(distr=\"popular\", stats=\"RSS\", verbose=\"silent\")\n_ = dfit_iat.fit_transform(data[\"iat_mins\"].dropna())\n\n# Fit popular distributions to service times\ndfit_service = distfit(distr=\"popular\", stats=\"RSS\", verbose=\"silent\")\n_ = dfit_service.fit_transform(data[\"service_mins\"])\n\nWe can view a summary table from distfit.\nThe score column is the result from a goodness-of-fit test. This is set using stats in distfit (e.g.¬†distfit(stats=\"RSS\")). It provides several possible tests including:\n\nRSS - residual sum of squares\nwasserstein - Wasserstein distance\nks - Kolmogorov-Smirnov statistic\nenergy - energy distance\ngoodness_of_fit - general purpose test from scipy.stats.goodness_of_fit\n\nFor continuous data, ks is often a good choice - but for distfit, they use an unusual method for calculation of this statistic. In distfit, they resample from the fitted distribution and compare that to the original data. Meanwhile, our manual implementation just use the standard KS test from scipy.stats, which is the standard KS statistics that is commonly understood.\nAs such, we have left distfit with RSS. However, we can calculate a standard KS statistic ourselves using the function below - which, as we can see, matches up with our results above.\n\n\ndef calculate_ks(input_series, dfit_summary):\n    \"\"\"\n    Calculate standard Kolmogorov-Smirnov statistics for fitted distributions.\n\n    This function applies the standard scipy.stats.kstest to data using the\n    distribution parameters obtained from distfit, providing conventional\n    KS statistics rather than distfit's resampling-based approach.\n\n    Parameters\n    ----------\n    input_series : pandas.Series\n        The original data series used for distribution fitting.\n    dfit_summary : pandas.DataFrame\n        The summary DataFrame from a distfit object, containing fitted\n        distribution names and parameters.\n\n    Returns\n    -------\n    pandas.DataFrame\n        A DataFrame containing the distribution names, KS statistics,\n        and p-values from the standard KS test.\n\n    Notes\n    -----\n    Lower KS statistic values indicate better fits to the data.\n    \"\"\"\n    results = []\n    for _, row in dfit_summary.iterrows():\n        dist_name = row[\"name\"]\n        dist_params = row[\"params\"]\n\n        # Perform KS test using scipy.stats.kstest\n        ks_result = stats.kstest(input_series, dist_name, args=dist_params)\n\n        # Store the results\n        results.append({\n            \"name\": dist_name,\n            \"ks\": ks_result.statistic[0],\n            \"p_value\": ks_result.pvalue[0]\n        })\n\n    # Create a DataFrame with the results\n    return pd.DataFrame(results).sort_values(by=\"ks\")\n\n\n\ncalculate_ks(input_series=data[[\"iat_mins\"]].dropna(),\n             dfit_summary=dfit_iat.summary)\n\n          name        ks  p_value\n4   genextreme  0.115385      0.0\n3       pareto  0.115461      0.0\n2        expon  0.115461      0.0\n6     dweibull  0.158115      0.0\n5            t  0.160045      0.0\n8     loggamma  0.177966      0.0\n7         norm  0.179646      0.0\n0         beta  0.199921      0.0\n1        gamma  0.209314      0.0\n10     lognorm  0.523766      0.0\n9      uniform  0.703668      0.0\n\n\n\n\ncalculate_ks(input_series=data[[\"service_mins\"]],\n             dfit_summary=dfit_service.summary)\n\n          name        ks        p_value\n1       pareto  0.047970  3.085744e-264\n0        expon  0.047970  3.084811e-264\n3   genextreme  0.070976   0.000000e+00\n2         beta  0.092614   0.000000e+00\n4        gamma  0.122589   0.000000e+00\n7         norm  0.158105   0.000000e+00\n5            t  0.160056   0.000000e+00\n8     loggamma  0.161955   0.000000e+00\n6     dweibull  0.175325   0.000000e+00\n10     lognorm  0.536170   0.000000e+00\n9      uniform  0.713362   0.000000e+00\n\n\n\nThe distfit package has some nice visualisation functions. For example, using the inter-arrival times‚Ä¶\n\n\n# PDF with all the distributions overlaid\nfig, ax = dfit_iat.plot(n_top=11, figsize=(7, 4))\nfig.show()\n\n\n\n\n\n\n\n\n\n\n# CDF with all the distributions overlaid\nfig, ax = dfit_iat.plot(chart=\"cdf\", n_top=11, figsize=(7, 4))\nfig.show()\n\n\n\n\n\n\n\n\n\n\n# QQ plot with all distributions overlaid\nfig, ax = dfit_iat.qqplot(data[\"iat_mins\"].dropna(), n_top=11, figsize=(7, 4))\nfig.show()\n\n\n\n\n\n\n\n\n\n\n# Summary plot using the RSS\nfig, ax = dfit_iat.plot_summary(figsize=(7, 4))\nfig.show()\n\n\n\n\n\n\n\n\nWe can also use it to create a plot with a specific distribution overlaid, like in the targeted approach:\n\n\n# To create a plot with a specific distribution overlaid...\ndfit = distfit(distr=\"expon\")\n_ = dfit.fit_transform(data[\"iat_mins\"].dropna())\nfig, ax = dfit.plot(figsize=(7, 4))\nfig.show()\n\n\n\n\n\n\n\n\n\n\nThe fitdistrplus package does not have a built-in function to automatically fit a large set of distributions in a single command. Instead, we just need to specify a list of candidate distributions.\n\n\n# Continuous distributions supported natively by fitdist\n# (you could use other packages to get other distributions to test)\ndistributions &lt;- c(\"norm\", \"lnorm\", \"exp\", \"cauchy\", \"gamma\", \"logis\", \"beta\",\n                   \"weibull\", \"unif\")\nfit_distributions(data_iat, distributions)\n\nWarning: Skipped distributions due to data constraints: lnorm, beta, weibull\n\n\n     norm       exp    cauchy     gamma     logis      unif \n0.1796464 0.1154607 0.1968200 0.2061950 0.1564321 0.7036682 \n\nfit_distributions(data_service, distributions)\n\nWarning: Skipped distributions due to data constraints: lnorm, beta, weibull\n\n\n      norm        exp     cauchy      gamma      logis       unif \n0.15810466 0.04796992 0.19728636 0.09755396 0.15566197 0.71336153 \n\n\nAgain, exponential is returned as the best fit.\n\n\n\nThe fitdistrplus package also has some nice visualisation functions.\n\n\niat_exp &lt;- suppressWarnings(fitdist(data_iat, \"exp\"))\nplot(iat_exp)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing the targeted approach, we found exponential to be the best. Using the comprehensive approach, there were a few distributions that were all very low scores (pareto, expon, genextreme). Choosing between these‚Ä¶\n\nExponential - simple, wide application, good for context, fewer parameters\nGeneralised pareto - useful if data has heavier tail (not the case here)\nGeneralised extreme value - more complex, spefically designed for modeling maximum values or extreme events\n\nAs such, exponential seems a good choice for inter-arrival and service times.\n\n\nUsing the targeted and comprehensive approach, we found exponential to be best for inter-arrival and service times.\n\n\n\n\n\nThe exponential distribution is defined by a single parameter, but this parameter can be expressed in two ways - as the:\n\nMean (also called the scale) - this is just your sample mean.\nRate (also called lambda Œª) - this is calculated as 1 / mean.\n\n\nWe will use the Exponential class from sim-tools, which uses the numpy.random.exponential() function. That accepts the scale parameter, so we just need to calculate the sample mean.\nMean:\n\nInter-arrival time: 4 minutes.\nService time: 10 minutes.\n\n\n\nprint(data[\"iat_mins\"].dropna().mean())\n\n3.9843611416442406\n\nprint(data[\"service_mins\"].dropna().mean())\n\n9.991570393280572\n\n\n\n\nWe will use the rexp() function from the stats package which requires the rate parameter, not the mean.\nRate:\n\nInter-arrival time: 0.25\nService time: 0.1\n\n\n\n1 / mean(data_iat)\n\n[1] 0.2509813\n\n1 / mean(data_service)\n\n[1] 0.1000844\n\n\n\nFor guidance on storing and using these parameters, see the pages ?@sec-param_file and ?@sec-param_script.",
    "crumbs": [
      "Model inputs",
      "Input modelling"
    ]
  },
  {
    "objectID": "pages/inputs/input_modelling.html#further-information",
    "href": "pages/inputs/input_modelling.html#further-information",
    "title": "Input modelling",
    "section": "",
    "text": "‚ÄúSimulation modelling for stochastic systems lab 3‚Äù from Tom Monks 2024.\n‚ÄúChapter 7: Data Collection and Analysis‚Äù from ‚ÄúSimulation: The Practice of Model Development and Use‚Äù by Stewart Robinson 2007.",
    "crumbs": [
      "Model inputs",
      "Input modelling"
    ]
  },
  {
    "objectID": "pages/inputs/parameters_file.html",
    "href": "pages/inputs/parameters_file.html",
    "title": "Parameters from file",
    "section": "",
    "text": "Parameters from file\n\nüîó Reproducibility guidelines:\n\nHeather et al.¬†2025: Avoid hard-coded parameters.",
    "crumbs": [
      "Model inputs",
      "Parameters from file"
    ]
  },
  {
    "objectID": "pages/inputs/parameters_validation.html",
    "href": "pages/inputs/parameters_validation.html",
    "title": "Parameter validation",
    "section": "",
    "text": "This is not necessary to reproducibility or a reproducible analytical pipeline, but is good practice when working, and may help avoid mistakes.\n\nAccidental parameter changes or typos can silently invalidate your results.\nFor example, mistyping transfer_prob as transfer_probs in a dictionary creates a new, unused parameter instead of raising an error.\nWhen you use function or class, they will prevent this error during set up (i.e.¬†when provide arguments to both), but you can still run into this issue, if you alter the dictionary from function, or alter the class attributes.\nin each of examples below, have just add a new param called transfer_probs, and transfer_prob remains 0.3. we dont get an error message, but unbenownst to us, simulation still uses 0.3.\n# Function example\nparams = create_params()\nparams[\"transfer_probs\"] = 0.4\n\n# Class example\nparams = Parameters()\nparams.transfer_probs = 0.4\nThere are two solutions to this, depending on whether you use functions or classes:\n\nFor functions, parameter validation within simulation functions\nFor classes, parameter validation within class\n\nValidation in class preferable as at point of defining param, all in one place. But both good options\n\n\n\nfor functions, this is done in model code, as if you did it in function, changes could be made to dict between making it with function and using it\n\n\n\n\nPrevent typos and unauthorised changes by locking down classes after initialisation.\nfor classes, can do it direct in class, anytime try to update an attribute, which is preferable approach you then have two options: - directly in class\nclass Param:\n    def __init__(self, ...):\n        # Disable restriction on attribute modification during initialisation\n        object.__setattr__(self, '_initialising', True)\n\n        ...\n\n        # Re-enable attribute checks after initialisation\n        object.__setattr__(self, '_initialising', False)\n\n    def __setattr__(self, name, value):\n        # Skip the check if the object is still initialising\n        # pylint: disable=maybe-no-member\n        if hasattr(self, '_initialising') and self._initialising:\n            super().__setattr__(name, value)\n        else:\n            # Check if attribute of that name is already present\n            if name in self.__dict__:\n                super().__setattr__(name, value)\n            else:\n                raise AttributeError(\n                    f'Cannot add new attribute \"{name}\" - only possible to ' +\n                    f'modify existing attributes: {self.__dict__.keys()}')\n\nsuper class\n\nclass RestrictAttributesMeta(type):\n    \"\"\"\n    Metaclass for attribute restriction.\n\n    A metaclass modifies class construction. It intercepts instance creation\n    via __call__, adding the _initialised flag after __init__ completes. This\n    is later used by RestrictAttributes to enforce attribute restrictions.\n    \"\"\"\n    def __call__(cls, *args, **kwargs):\n        # Create instance using the standard method\n        instance = super().__call__(*args, **kwargs)\n        # Set the \"_initialised\" flag to True, marking end of initialisation\n        instance.__dict__[\"_initialised\"] = True\n        return instance\n\n\nclass RestrictAttributes(metaclass=RestrictAttributesMeta):\n    \"\"\"\n    Base class that prevents the addition of new attributes after\n    initialisation.\n\n    This class uses RestrictAttributesMeta as its metaclass to implement\n    attribute restriction. It allows for safe initialisation of attributes\n    during the __init__ method, but prevents the addition of new attributes\n    afterwards.\n\n    The restriction is enforced through the custom __setattr__ method, which\n    checks if the attribute already exists before allowing assignment.\n    \"\"\"\n    def __setattr__(self, name, value):\n        \"\"\"\n        Prevent addition of new attributes.\n\n        Parameters\n        ----------\n        name: str\n            The name of the attribute to set.\n        value: any\n            The value to assign to the attribute.\n\n        Raises\n        ------\n        AttributeError\n            If `name` is not an existing attribute and an attempt is made\n            to add it to the class instance.\n        \"\"\"\n        # Check if the instance is initialised and the attribute doesn\"t exist\n        if hasattr(self, \"_initialised\") and not hasattr(self, name):\n            # Get a list of existing attributes for the error message\n            existing = \", \".join(self.__dict__.keys())\n            raise AttributeError(\n                f\"Cannot add new attribute '{name}' - only possible to \" +\n                f\"modify existing attributes: {existing}.\"\n            )\n        # If checks pass, set the attribute using the standard method\n        object.__setattr__(self, name, value)",
    "crumbs": [
      "Model inputs",
      "Parameter validation"
    ]
  },
  {
    "objectID": "pages/inputs/parameters_validation.html#parameter-validation-within-the-simulation-functions",
    "href": "pages/inputs/parameters_validation.html#parameter-validation-within-the-simulation-functions",
    "title": "Parameter validation",
    "section": "",
    "text": "for functions, this is done in model code, as if you did it in function, changes could be made to dict between making it with function and using it",
    "crumbs": [
      "Model inputs",
      "Parameter validation"
    ]
  },
  {
    "objectID": "pages/inputs/parameters_validation.html#parameter-validation-within-the-class",
    "href": "pages/inputs/parameters_validation.html#parameter-validation-within-the-class",
    "title": "Parameter validation",
    "section": "",
    "text": "Prevent typos and unauthorised changes by locking down classes after initialisation.\nfor classes, can do it direct in class, anytime try to update an attribute, which is preferable approach you then have two options: - directly in class\nclass Param:\n    def __init__(self, ...):\n        # Disable restriction on attribute modification during initialisation\n        object.__setattr__(self, '_initialising', True)\n\n        ...\n\n        # Re-enable attribute checks after initialisation\n        object.__setattr__(self, '_initialising', False)\n\n    def __setattr__(self, name, value):\n        # Skip the check if the object is still initialising\n        # pylint: disable=maybe-no-member\n        if hasattr(self, '_initialising') and self._initialising:\n            super().__setattr__(name, value)\n        else:\n            # Check if attribute of that name is already present\n            if name in self.__dict__:\n                super().__setattr__(name, value)\n            else:\n                raise AttributeError(\n                    f'Cannot add new attribute \"{name}\" - only possible to ' +\n                    f'modify existing attributes: {self.__dict__.keys()}')\n\nsuper class\n\nclass RestrictAttributesMeta(type):\n    \"\"\"\n    Metaclass for attribute restriction.\n\n    A metaclass modifies class construction. It intercepts instance creation\n    via __call__, adding the _initialised flag after __init__ completes. This\n    is later used by RestrictAttributes to enforce attribute restrictions.\n    \"\"\"\n    def __call__(cls, *args, **kwargs):\n        # Create instance using the standard method\n        instance = super().__call__(*args, **kwargs)\n        # Set the \"_initialised\" flag to True, marking end of initialisation\n        instance.__dict__[\"_initialised\"] = True\n        return instance\n\n\nclass RestrictAttributes(metaclass=RestrictAttributesMeta):\n    \"\"\"\n    Base class that prevents the addition of new attributes after\n    initialisation.\n\n    This class uses RestrictAttributesMeta as its metaclass to implement\n    attribute restriction. It allows for safe initialisation of attributes\n    during the __init__ method, but prevents the addition of new attributes\n    afterwards.\n\n    The restriction is enforced through the custom __setattr__ method, which\n    checks if the attribute already exists before allowing assignment.\n    \"\"\"\n    def __setattr__(self, name, value):\n        \"\"\"\n        Prevent addition of new attributes.\n\n        Parameters\n        ----------\n        name: str\n            The name of the attribute to set.\n        value: any\n            The value to assign to the attribute.\n\n        Raises\n        ------\n        AttributeError\n            If `name` is not an existing attribute and an attempt is made\n            to add it to the class instance.\n        \"\"\"\n        # Check if the instance is initialised and the attribute doesn\"t exist\n        if hasattr(self, \"_initialised\") and not hasattr(self, name):\n            # Get a list of existing attributes for the error message\n            existing = \", \".join(self.__dict__.keys())\n            raise AttributeError(\n                f\"Cannot add new attribute '{name}' - only possible to \" +\n                f\"modify existing attributes: {existing}.\"\n            )\n        # If checks pass, set the attribute using the standard method\n        object.__setattr__(self, name, value)",
    "crumbs": [
      "Model inputs",
      "Parameter validation"
    ]
  },
  {
    "objectID": "pages/model/patients.html",
    "href": "pages/model/patients.html",
    "title": "Entity generation",
    "section": "",
    "text": "Entity generation",
    "crumbs": [
      "Model building",
      "Entity generation"
    ]
  },
  {
    "objectID": "pages/verification_validation/tests.html",
    "href": "pages/verification_validation/tests.html",
    "title": "Tests",
    "section": "",
    "text": "Tests\n\nüîó Reproducibility guidelines:\n\nNHS Levels of RAP (ü•à): Pipeline includes a testing framework (unit tests, back tests).",
    "crumbs": [
      "Verification & validation",
      "Tests"
    ]
  },
  {
    "objectID": "pages/verification_validation/validation.html",
    "href": "pages/verification_validation/validation.html",
    "title": "Validation",
    "section": "",
    "text": "Validation",
    "crumbs": [
      "Verification & validation",
      "Validation"
    ]
  },
  {
    "objectID": "pages/sharing/archive.html",
    "href": "pages/sharing/archive.html",
    "title": "Sharing and archiving",
    "section": "",
    "text": "Sharing and archiving\n\nüîó Reproducibility guidelines:\n\nNHS Levels of RAP (ü•â): Code is published in the open and linked to & from accompanying publication (if relevant).",
    "crumbs": [
      "Collaboration & sharing",
      "Sharing and archiving"
    ]
  },
  {
    "objectID": "pages/sharing/changelog.html",
    "href": "pages/sharing/changelog.html",
    "title": "Changelog",
    "section": "",
    "text": "Changelog\n\nüîó Reproducibility guidelines:\n\nHeather et al.¬†2025: Link publication to a specific version of the code.\nNHS Levels of RAP (ü•á): Changes to the RAP are clearly signposted. E.g. a changelog in the package, releases etc.",
    "crumbs": [
      "Collaboration & sharing",
      "Changelog"
    ]
  },
  {
    "objectID": "pages/setup/environment.html",
    "href": "pages/setup/environment.html",
    "title": "Dependency management",
    "section": "",
    "text": "üîó Reproducibility guidelines:\n\nHeather et al.¬†2025: List dependencies and versions.\nNHS Levels of RAP (ü•à): Repository includes dependency information.\n\n\nDependency management is about keeping track of the environment used for your project. This includes the version of your programming languages, any packages used, and their versions.\nIt acts like a time capsule, allowing you to return to a project later and run it with the exact same packages and versions, reproducing the results generated previously.\n\nDependency management enables you to isolate environments for different projects. Each project can have it‚Äôs own set of dependencies, preventing conflicts and making it easy to switch between projects.\n\nIt is also important for collaboration, so that everyone working on the project is using the same environment.\n\n\n\n\n\n\n\n\n\nThere are lots of tools available for managing dependencies in isolated environments in python. A few of the most popular tools include venv, conda, and poetry.\nIn this book, we will use conda, as it allows us to specify a python version, so each environment we create can have a different specific version of python. This is not possible with venv and poetry, which just manage the package dependencies, and will just use the system python.\n\n\n\n\n\n\nMamba\n\n\n\nMamba is a drop-in replacement for conda that is often preferred as it is:\n\nFaster than conda.\nBetter at dealing with dependency conflicts, providing more helpful messages in cases where environments fail to builds due to clashing requirements of different packages.\n\nTo use mamba, simply replace conda in all the commands below with mamba.\n\n\n\n\n\n\nRefer to the conda or mamba documentation for the latest instructions on installing these for your operating system (windows, mac or linux).\n\nConda installation instructions.\nMamba installation instructions.\n\n\n\n\n\n1. Create an environment file. In the project root, we create environment.yaml.\ntouch environment.yaml\nWithin this file, we add three sections:\n\nName. The environment name.\nChannels. Where to find packages (e.g.¬†conda-forge). \nDependencies. The packages you need.\n\nWhen first creating our environment, we just list the dependencies we know we need at this point - we can always add more later! At the start of a project, you might only know one: python.\nAs an example, we will add simpy and python.\nname: des-example\nchannels:\n  - conda-forge\ndependencies:\n  - python\n  - simpy\n\n2. Build and activate the environment. In the command line, run the following to create your environment:\nconda env create --file environment.yaml\nYou can then activate it (replacing des-example with your environment name):\nconda activate des-example\nTo confirm your environment contains the expected packages, run:\nconda list\nThis will output a list of packages, versions, builds and channels. For example, it may look similar to:\n(des-example) amy@xps:~/Documents/hospital-des$ conda list\n# packages in environment at /home/amy/mambaforge/envs/des-example:\n#\n# Name                    Version                   Build  Channel\n_libgcc_mutex             0.1                 conda_forge    conda-forge\n_openmp_mutex             4.5                       2_gnu    conda-forge\nbzip2                     1.0.8                h4bc722e_7    conda-forge\nca-certificates           2025.1.31            hbd8a1cb_1    conda-forge\nld_impl_linux-64          2.43                 h712a8e2_4    conda-forge\nlibexpat                  2.7.0                h5888daf_0    conda-forge\nlibffi                    3.4.6                h2dba641_1    conda-forge\nlibgcc                    14.2.0               h767d61c_2    conda-forge\nlibgcc-ng                 14.2.0               h69a702a_2    conda-forge\nlibgomp                   14.2.0               h767d61c_2    conda-forge\nliblzma                   5.8.1                hb9d3cd8_0    conda-forge\nlibmpdec                  4.0.0                h4bc722e_0    conda-forge\nlibsqlite                 3.49.1               hee588c1_2    conda-forge\nlibuuid                   2.38.1               h0b41bf4_0    conda-forge\nlibzlib                   1.3.1                hb9d3cd8_2    conda-forge\nncurses                   6.5                  h2d0b736_3    conda-forge\nopenssl                   3.5.0                h7b32b05_0    conda-forge\npip                       25.0.1             pyh145f28c_0    conda-forge\npython                    3.13.3          hf636f53_101_cp313    conda-forge\npython_abi                3.13                    7_cp313    conda-forge\nreadline                  8.2                  h8c095d6_2    conda-forge\nsimpy                     4.1.1              pyhd8ed1ab_1    conda-forge\ntk                        8.6.13          noxft_h4845f30_101    conda-forge\ntzdata                    2025b                h78e105d_0    conda-forge\n\n3. Specify versions. For reproducibility, it‚Äôs best to specify the exact package versions in you environment.yaml. If you‚Äôre starting from scratch, you may not know which versions you need, so you can leave them out initially, as we did in step 1.\nHowever, now that we have built our environment (which used the latest versions as none were specified), it is important to then record your versions in the environment.yaml. These are the versions you saw when running conda list. For example:\nname: des-example\nchannels:\n  - conda-forge\ndependencies:\n  - python=3.13.3\n  - simpy=4.1.1\n\n4. Setting up the full environment for this book. When working on a project from scratch, you will often build up your environment organically and iteratively as you find more packages you want to use. However, to follow along with this book and ensure everything works as expected, you can use the full environment provided below. Copy this into your environment.yaml (feel free to alter the name!):\nname: des-example\nchannels:\n  - conda-forge\ndependencies:\n  - ipykernel=6.29.5\n  - jinja2=3.1.5\n  - joblib=1.4.2\n  - nbconvert=7.16.6\n  - nbformat=5.10.4\n  - nbqa=1.9.0\n  - numpy=2.2.2\n  - pandas=2.2.3\n  - pip=25.0\n  - plotly_express=0.4.1\n  - pylint=3.3.4\n  - pytest=8.3.4\n  - pytest-xdist=3.6.1\n  - python=3.13.1\n  - rich=13.9.4\n  - simpy=4.1.1\n  - pip:\n    - kaleido==0.2.1\n    - sim-tools==0.8.0\nThen update your environment to include these packages (after running conda activate des-example) with:\nconda env update --file environment.yaml --prune\n\n\n\n\n\n‚ÄúAn unbiased evaluation of environment management and packaging tools‚Äù from Anna-Lena Popkes 2024\n‚ÄúPython dependency management is a dumpster fire‚Äù from Niels Cautaerts 2024\n\n\n\n\n\n\n\nThe most popular tool for managing dependencies in R is renv. This replaced and improved upon the previous tool, Packrat.\nRenv will create isolated environments with the specific packages and their versions for a project. However, it won‚Äôt manage the version of R used - the version of R used is simply whatever is installed on your system.  \n\n\n\n\n1. Create an R project. It‚Äôs best to use renv within an R project. In RStudio, select File &gt; New Project‚Ä¶, and choose Existing Directory.\n\n\n\n\n\nNavigate to your project directory, then select Create Project.\n\n\n\n\n\nThis creates:\n\n.Rproj: project file (contains some settings for the project).\n.Rproj.user: hidden folder with temporary project files (e.g.¬†auto-saved source documents).\n\nNote: R projects are commonly created and managed by RStudio. If you are not using RStudio, they can be difficult to set-up, as they have to be created manually. However, it is possible to use renv without an R project, as discussed in this GitHub issue. This can be done by using setwd() to set your repository as the current working directory, and then continuing with the steps below, running renv::init().\n\n2. Initialise renv. In your R console:\nrenv::init()\nThis creates:\n\nrenv/: stores packages for the project.\nrenv.lock: records packages and the exact versions used.\n.Rprofile: ensures renv activates when the project opens.\n\n\nWith renv initialised, you will now have an empty project library, just containing renv, as you can see from viewing renv.lock (example below). This is isolated from your previous projects, and from here, you can install the packages relevant for your current project.\nExample renv.lock:\n{\n  \"R\": {\n    \"Version\": \"4.4.1\",\n    \"Repositories\": [\n      {\n        \"Name\": \"CRAN\",\n        \"URL\": \"https://packagemanager.posit.co/cran/latest\"\n      }\n    ]\n  },\n  \"Packages\": {\n    \"renv\": {\n      \"Package\": \"renv\",\n      \"Version\": \"1.0.7\",\n      \"Source\": \"Repository\",\n      \"Repository\": \"CRAN\",\n      \"Requirements\": [\n        \"utils\"\n      ],\n      \"Hash\": \"397b7b2a265bc5a7a06852524dabae20\"\n    }\n  }\n}\n\n\n\n\n\nIt is possible to simply install packages using renv::install(\"packagename\") or install.packages(\"packagename\").\nHowever, we recommend using a DESCRIPTION file. This is because it allows you to keep a record of the main packages you installed. You‚Äôll generate a clear, readable summary of the main dependencies for your project.\nWhy use a DESCRIPTION file?\n\nClear requirements. The DESCRIPTION file provides a clear summary of your project‚Äôs main pages. This is much easier to read than renv.lock, which lists all the packages and their dependencies, making it cumbersome if you just want to see the key packages.\nConsistency with package development. If your project is (or might become) an R package, the DESCRIPTION file is the standard way to declare dependencies.\nAlternative for environment recreation. While renv.lock is the primary tool for restoring the exact environment, having a DESCRIPTION file is a valuable backup. If you encounter issues with renv.lock, you (or collaborators) can use DESCRIPTION to reinstall the main dependencies - with more information on this below in the section on recreating environments.\nExplicit snapshots. If you want precise control over what is included in renv.lock, a DESCRIPTION file enables you to use ‚Äúexplicit‚Äù snapshots. These mean only the packages listed in DESCRIPTION (and their dependencies) are recorded - as is covered below in the step on updating your renv.lock file.\n\n\n\n1. Create DESCRIPTION file. Run in the terminal:\ntouch DESCRIPTION\nOpen the file and copy in the template below. You can customise some of the meta-data (e.g.¬†package, title, authors, description).\nThis is a standard template. You can create an identical file with usethis by running usethis::use_description(). However, we can just create it from scratch, which helps to minimise our dependencies.\nPackage: packagename\nTitle: What the Package Does (One Line, Title Case)\nVersion: 0.0.0.9000\nAuthors@R: \n    person(\"First\", \"Last\", , \"first.last@example.com\", role = c(\"aut\", \"cre\"))\nDescription: What the package does (one paragraph).\nLicense: `use_mit_license()`, `use_gpl3_license()` or friends to pick a\n    license\nEncoding: UTF-8\nRoxygen: list(markdown = TRUE)\nRoxygenNote: 7.0.0\n\n2. List dependencies. In DESCRIPTION, dependencies are listed under two headings:\n\nImports: for required packages.\nSuggests: for optional/development packages.\n\nFor most projects (especially non-packages), it‚Äôs fine to just list all your dependencies under Imports for simplicity. The distinction betweeen Imports and Suggests is more relevant when constructing your research as an R package, as it will distinguish between those necessary for the core simulation and those for other analysis and tests (see ?@sec-package). \nAt the very beginning of your project, your DESCRIPTION file might only include a few packages. For example, if you are starting with just the simmer package, your Imports section would look like this:\nImports:\n    simmer\nAs your project develops and you find yourself using additional packages, simply add each new dependency to the Imports section of your DESCRIPTION file.\nIf you are following along with this book, you can use the following DESCRIPTION snippet to include all the packages needed to run the provided code in this book:\nImports:\n    simmer,\n    magrittr,\n    dplyr,\n    purrr,\n    rlang,\n    tidyr,\n    tidyselect,\n    future,\n    future.apply,\n    ggplot2,\n    tibble,\n    gridExtra,\n    R6\nSuggests:\n    testthat (&gt;= 3.0.0),\n    patrick,\n    lintr,\n    devtools,\n    xtable,\n    data.table,\n    mockery\nConfig/testthat/edition: 3\nNote: In the R Packages book, they recommend that versions are not specified in DESCRIPTION. Instead, they suggest that no version is specified - or that a minimum version is specified, if you know that an older version of specific package/s would break the code. This is why it is important to also create an renv.lock file (as below), so you do have a record of the exact versions used.\n\n3. Install packages from DESCRIPTION. Run the following command in your console. This will install the packages from DESCRIPTION, and will determine and install the dependencies of those packages too.\nrenv::install()\n\n4. Update renv.lock. To take a snapshot of your environment and update your renv.lock file, run:\nrenv::snapshot()\nThis will update the lock file with a full list of the exact packages and dependencies, including the versions you have installed, providing a clear record of your working environment.\nThere are three snapshot types:\n\nImplicit - records any packages (and their dependencies) listed in DESCRIPTION or used in your code.\nExplicit - only records packages (and their dependencies) listed in DESCRIPTION.\nAll - records all packages in your environment.\n\nThe default snapshot type is implicit, and we recommend this approach. This is because it will catch any packages you are using but that you have forgot to add to DESCRIPTION (although it is best to remember to record these in DESCRIPTION, so you have a nice clear list of packages, and don‚Äôt have to delve into renv.lock if you‚Äôre having issues).\nThe downside to this snapshot type if that it may include unnecessary packages if you include old scripts in your repository that use packages you no longer need. However, this can be avoided by removing old scripts (good practice!).\nIf you want to check your snapshot type, run this command in the R console:\nrenv::settings$snapshot.type()\nYou can then change it if desired using one of:\nrenv::settings$snapshot.type(\"implicit\")\nrenv::settings$snapshot.type(\"explicit\")\nrenv::settings$snapshot.type(\"all\")\n\n\n\n\nWhen you want to recreate an environment‚Äîsuch as for an old project or when collaborating‚Äîyou have two main options, depending on which files are available:\n\n\n\n\n\n\n\n\nMethod\nWhat does it install?\nBest for‚Ä¶\n\n\n\n\nFrom renv.lock\nInstalls the exact package versions (and their dependencies) used previously\nFull reproducibility; restoring the original environment exactly\n\n\nFrom DESCRIPTION\nInstalls the main packages listed (and their dependencies), but uses the latest available versions (unless specified)\nGetting started quickly or if renv.lock is missing or problematic\n\n\n\n\n1. Restoring from renv.lock (preferred). If the project includes an renv.lock file, use this as your first option. This file records the exact versions of all packages used, enabling you to recreate the environment as it was originally (*except for R version and operating system differences).\nTo restore the environment, run in your R console:\nrenv::restore()\nThis will attempt to install all packages at the precise versions specified.\nOccasionally, you may encounter issues - such as conflicts, unavailable packages, or operating system differences - especially with older projects or across different systems.\n\n2. Rebuilding from DESCRIPTION. If renv.lock is unavailable or causes problems, you can use the DESCRIPTION file. This file lists the main package dependencies, but typically does not specify exact versions (unless you have set minimum versions for specific needs).\nTo install packages listed in DESCRIPTION, run:\nrenv::install()\nThis will install the latest available versions of the listed packages and their dependencies. This approach is less precise than using renv.lock, so results may differ slightly from the original environment, especially if package updates have introduced changes.\nIn theory, the R ecosystem aspires to maintain backwards compatability, meaning that code written for older package versions should continue to work with newer ones. However, in practice, there is no strict guarantee of backward compatibility in R, either for the core language or for contributed packages.\nAs discussed in the R packages book:\n\n‚ÄúIf we‚Äôre being honest, most R users don‚Äôt manage package versions in a very intentional way. Given the way update.packages() and install.packages() work, it‚Äôs quite easy to upgrade a package to a new major version without really meaning to, especially for dependencies of the target package. This, in turn, can lead to unexpected exposure to breaking changes in code that previously worked. This unpleasantness has implications both for users and for maintainers.‚Äù\n\nHence, using a lockfile like renv.lock is the only reliable way to ensure that your environment is recreated exactly as it was - but DESCRIPTION can serve as a valuable back-up when this doesn‚Äôt work, and otherwise just as a handy summary of the main packages.\n\n3. If neither file is provided. If you have neither a renv.lock nor a DESCRIPTION file, you can try to create a DESCRIPTION file based on your knowledge of the project and its required packages.\n\n\n\n\nSome R packages require external system libraries. The exact requirements will depend on which packages you use, what operating system you have, and whether you have used R before.\nIf these system libraries are missing, package installation may fail, even if you have the correct R package versions.\nFor example, working on Ubuntu, we found that we had to install the following system dependencies for igraph:\nsudo apt install build-essential gfortran\nsudo apt install libglpk-dev libxml2-dev\nYou should list any system dependencies that you are aware of in your project‚Äôs README or setup instructions.\n\n\n\n\nWe can use renv to create a reproducible environment in R. In the process above, we generated two key files:\n\nDESCRIPTION - lists project‚Äôs primary packages with any minimum version requirements, but not exact versions.\nrenv.lock - complements this by recording the precise versions of all packages and their dependencies.\n\nThese work together to both provide (a) a comprehensive record of your project environment, and (b) enable yourself or others to reconstruct the environment.\n\n\n\n\n\n‚Äú9. DESCRIPTION‚Äù from R Packages by Hadley Wickham and Jennifer Bryan.\n‚Äú21. Lifecycle‚Äù from R Packages by Hadley Wickham and Jennifer Bryan.",
    "crumbs": [
      "Setup",
      "Dependency management"
    ]
  },
  {
    "objectID": "pages/setup/environment.html#python-tools-for-dependency-management",
    "href": "pages/setup/environment.html#python-tools-for-dependency-management",
    "title": "Dependency management",
    "section": "",
    "text": "There are lots of tools available for managing dependencies in isolated environments in python. A few of the most popular tools include venv, conda, and poetry.\nIn this book, we will use conda, as it allows us to specify a python version, so each environment we create can have a different specific version of python. This is not possible with venv and poetry, which just manage the package dependencies, and will just use the system python.\n\n\n\n\n\n\nMamba\n\n\n\nMamba is a drop-in replacement for conda that is often preferred as it is:\n\nFaster than conda.\nBetter at dealing with dependency conflicts, providing more helpful messages in cases where environments fail to builds due to clashing requirements of different packages.\n\nTo use mamba, simply replace conda in all the commands below with mamba.",
    "crumbs": [
      "Setup",
      "Dependency management"
    ]
  },
  {
    "objectID": "pages/setup/environment.html#installing-conda-or-mamba",
    "href": "pages/setup/environment.html#installing-conda-or-mamba",
    "title": "Dependency management",
    "section": "",
    "text": "Refer to the conda or mamba documentation for the latest instructions on installing these for your operating system (windows, mac or linux).\n\nConda installation instructions.\nMamba installation instructions.",
    "crumbs": [
      "Setup",
      "Dependency management"
    ]
  },
  {
    "objectID": "pages/setup/environment.html#setting-up-our-conda-environment",
    "href": "pages/setup/environment.html#setting-up-our-conda-environment",
    "title": "Dependency management",
    "section": "",
    "text": "1. Create an environment file. In the project root, we create environment.yaml.\ntouch environment.yaml\nWithin this file, we add three sections:\n\nName. The environment name.\nChannels. Where to find packages (e.g.¬†conda-forge). \nDependencies. The packages you need.\n\nWhen first creating our environment, we just list the dependencies we know we need at this point - we can always add more later! At the start of a project, you might only know one: python.\nAs an example, we will add simpy and python.\nname: des-example\nchannels:\n  - conda-forge\ndependencies:\n  - python\n  - simpy\n\n2. Build and activate the environment. In the command line, run the following to create your environment:\nconda env create --file environment.yaml\nYou can then activate it (replacing des-example with your environment name):\nconda activate des-example\nTo confirm your environment contains the expected packages, run:\nconda list\nThis will output a list of packages, versions, builds and channels. For example, it may look similar to:\n(des-example) amy@xps:~/Documents/hospital-des$ conda list\n# packages in environment at /home/amy/mambaforge/envs/des-example:\n#\n# Name                    Version                   Build  Channel\n_libgcc_mutex             0.1                 conda_forge    conda-forge\n_openmp_mutex             4.5                       2_gnu    conda-forge\nbzip2                     1.0.8                h4bc722e_7    conda-forge\nca-certificates           2025.1.31            hbd8a1cb_1    conda-forge\nld_impl_linux-64          2.43                 h712a8e2_4    conda-forge\nlibexpat                  2.7.0                h5888daf_0    conda-forge\nlibffi                    3.4.6                h2dba641_1    conda-forge\nlibgcc                    14.2.0               h767d61c_2    conda-forge\nlibgcc-ng                 14.2.0               h69a702a_2    conda-forge\nlibgomp                   14.2.0               h767d61c_2    conda-forge\nliblzma                   5.8.1                hb9d3cd8_0    conda-forge\nlibmpdec                  4.0.0                h4bc722e_0    conda-forge\nlibsqlite                 3.49.1               hee588c1_2    conda-forge\nlibuuid                   2.38.1               h0b41bf4_0    conda-forge\nlibzlib                   1.3.1                hb9d3cd8_2    conda-forge\nncurses                   6.5                  h2d0b736_3    conda-forge\nopenssl                   3.5.0                h7b32b05_0    conda-forge\npip                       25.0.1             pyh145f28c_0    conda-forge\npython                    3.13.3          hf636f53_101_cp313    conda-forge\npython_abi                3.13                    7_cp313    conda-forge\nreadline                  8.2                  h8c095d6_2    conda-forge\nsimpy                     4.1.1              pyhd8ed1ab_1    conda-forge\ntk                        8.6.13          noxft_h4845f30_101    conda-forge\ntzdata                    2025b                h78e105d_0    conda-forge\n\n3. Specify versions. For reproducibility, it‚Äôs best to specify the exact package versions in you environment.yaml. If you‚Äôre starting from scratch, you may not know which versions you need, so you can leave them out initially, as we did in step 1.\nHowever, now that we have built our environment (which used the latest versions as none were specified), it is important to then record your versions in the environment.yaml. These are the versions you saw when running conda list. For example:\nname: des-example\nchannels:\n  - conda-forge\ndependencies:\n  - python=3.13.3\n  - simpy=4.1.1\n\n4. Setting up the full environment for this book. When working on a project from scratch, you will often build up your environment organically and iteratively as you find more packages you want to use. However, to follow along with this book and ensure everything works as expected, you can use the full environment provided below. Copy this into your environment.yaml (feel free to alter the name!):\nname: des-example\nchannels:\n  - conda-forge\ndependencies:\n  - ipykernel=6.29.5\n  - jinja2=3.1.5\n  - joblib=1.4.2\n  - nbconvert=7.16.6\n  - nbformat=5.10.4\n  - nbqa=1.9.0\n  - numpy=2.2.2\n  - pandas=2.2.3\n  - pip=25.0\n  - plotly_express=0.4.1\n  - pylint=3.3.4\n  - pytest=8.3.4\n  - pytest-xdist=3.6.1\n  - python=3.13.1\n  - rich=13.9.4\n  - simpy=4.1.1\n  - pip:\n    - kaleido==0.2.1\n    - sim-tools==0.8.0\nThen update your environment to include these packages (after running conda activate des-example) with:\nconda env update --file environment.yaml --prune\n\n\n\n\n\n‚ÄúAn unbiased evaluation of environment management and packaging tools‚Äù from Anna-Lena Popkes 2024\n‚ÄúPython dependency management is a dumpster fire‚Äù from Niels Cautaerts 2024",
    "crumbs": [
      "Setup",
      "Dependency management"
    ]
  },
  {
    "objectID": "pages/setup/environment.html#r-tools-for-dependency-management",
    "href": "pages/setup/environment.html#r-tools-for-dependency-management",
    "title": "Dependency management",
    "section": "",
    "text": "The most popular tool for managing dependencies in R is renv. This replaced and improved upon the previous tool, Packrat.\nRenv will create isolated environments with the specific packages and their versions for a project. However, it won‚Äôt manage the version of R used - the version of R used is simply whatever is installed on your system.",
    "crumbs": [
      "Setup",
      "Dependency management"
    ]
  },
  {
    "objectID": "pages/setup/environment.html#setting-up-renv",
    "href": "pages/setup/environment.html#setting-up-renv",
    "title": "Dependency management",
    "section": "",
    "text": "1. Create an R project. It‚Äôs best to use renv within an R project. In RStudio, select File &gt; New Project‚Ä¶, and choose Existing Directory.\n\n\n\n\n\nNavigate to your project directory, then select Create Project.\n\n\n\n\n\nThis creates:\n\n.Rproj: project file (contains some settings for the project).\n.Rproj.user: hidden folder with temporary project files (e.g.¬†auto-saved source documents).\n\nNote: R projects are commonly created and managed by RStudio. If you are not using RStudio, they can be difficult to set-up, as they have to be created manually. However, it is possible to use renv without an R project, as discussed in this GitHub issue. This can be done by using setwd() to set your repository as the current working directory, and then continuing with the steps below, running renv::init().\n\n2. Initialise renv. In your R console:\nrenv::init()\nThis creates:\n\nrenv/: stores packages for the project.\nrenv.lock: records packages and the exact versions used.\n.Rprofile: ensures renv activates when the project opens.\n\n\nWith renv initialised, you will now have an empty project library, just containing renv, as you can see from viewing renv.lock (example below). This is isolated from your previous projects, and from here, you can install the packages relevant for your current project.\nExample renv.lock:\n{\n  \"R\": {\n    \"Version\": \"4.4.1\",\n    \"Repositories\": [\n      {\n        \"Name\": \"CRAN\",\n        \"URL\": \"https://packagemanager.posit.co/cran/latest\"\n      }\n    ]\n  },\n  \"Packages\": {\n    \"renv\": {\n      \"Package\": \"renv\",\n      \"Version\": \"1.0.7\",\n      \"Source\": \"Repository\",\n      \"Repository\": \"CRAN\",\n      \"Requirements\": [\n        \"utils\"\n      ],\n      \"Hash\": \"397b7b2a265bc5a7a06852524dabae20\"\n    }\n  }\n}",
    "crumbs": [
      "Setup",
      "Dependency management"
    ]
  },
  {
    "objectID": "pages/setup/environment.html#adding-packages-to-the-environment",
    "href": "pages/setup/environment.html#adding-packages-to-the-environment",
    "title": "Dependency management",
    "section": "",
    "text": "It is possible to simply install packages using renv::install(\"packagename\") or install.packages(\"packagename\").\nHowever, we recommend using a DESCRIPTION file. This is because it allows you to keep a record of the main packages you installed. You‚Äôll generate a clear, readable summary of the main dependencies for your project.\nWhy use a DESCRIPTION file?\n\nClear requirements. The DESCRIPTION file provides a clear summary of your project‚Äôs main pages. This is much easier to read than renv.lock, which lists all the packages and their dependencies, making it cumbersome if you just want to see the key packages.\nConsistency with package development. If your project is (or might become) an R package, the DESCRIPTION file is the standard way to declare dependencies.\nAlternative for environment recreation. While renv.lock is the primary tool for restoring the exact environment, having a DESCRIPTION file is a valuable backup. If you encounter issues with renv.lock, you (or collaborators) can use DESCRIPTION to reinstall the main dependencies - with more information on this below in the section on recreating environments.\nExplicit snapshots. If you want precise control over what is included in renv.lock, a DESCRIPTION file enables you to use ‚Äúexplicit‚Äù snapshots. These mean only the packages listed in DESCRIPTION (and their dependencies) are recorded - as is covered below in the step on updating your renv.lock file.\n\n\n\n1. Create DESCRIPTION file. Run in the terminal:\ntouch DESCRIPTION\nOpen the file and copy in the template below. You can customise some of the meta-data (e.g.¬†package, title, authors, description).\nThis is a standard template. You can create an identical file with usethis by running usethis::use_description(). However, we can just create it from scratch, which helps to minimise our dependencies.\nPackage: packagename\nTitle: What the Package Does (One Line, Title Case)\nVersion: 0.0.0.9000\nAuthors@R: \n    person(\"First\", \"Last\", , \"first.last@example.com\", role = c(\"aut\", \"cre\"))\nDescription: What the package does (one paragraph).\nLicense: `use_mit_license()`, `use_gpl3_license()` or friends to pick a\n    license\nEncoding: UTF-8\nRoxygen: list(markdown = TRUE)\nRoxygenNote: 7.0.0\n\n2. List dependencies. In DESCRIPTION, dependencies are listed under two headings:\n\nImports: for required packages.\nSuggests: for optional/development packages.\n\nFor most projects (especially non-packages), it‚Äôs fine to just list all your dependencies under Imports for simplicity. The distinction betweeen Imports and Suggests is more relevant when constructing your research as an R package, as it will distinguish between those necessary for the core simulation and those for other analysis and tests (see ?@sec-package). \nAt the very beginning of your project, your DESCRIPTION file might only include a few packages. For example, if you are starting with just the simmer package, your Imports section would look like this:\nImports:\n    simmer\nAs your project develops and you find yourself using additional packages, simply add each new dependency to the Imports section of your DESCRIPTION file.\nIf you are following along with this book, you can use the following DESCRIPTION snippet to include all the packages needed to run the provided code in this book:\nImports:\n    simmer,\n    magrittr,\n    dplyr,\n    purrr,\n    rlang,\n    tidyr,\n    tidyselect,\n    future,\n    future.apply,\n    ggplot2,\n    tibble,\n    gridExtra,\n    R6\nSuggests:\n    testthat (&gt;= 3.0.0),\n    patrick,\n    lintr,\n    devtools,\n    xtable,\n    data.table,\n    mockery\nConfig/testthat/edition: 3\nNote: In the R Packages book, they recommend that versions are not specified in DESCRIPTION. Instead, they suggest that no version is specified - or that a minimum version is specified, if you know that an older version of specific package/s would break the code. This is why it is important to also create an renv.lock file (as below), so you do have a record of the exact versions used.\n\n3. Install packages from DESCRIPTION. Run the following command in your console. This will install the packages from DESCRIPTION, and will determine and install the dependencies of those packages too.\nrenv::install()\n\n4. Update renv.lock. To take a snapshot of your environment and update your renv.lock file, run:\nrenv::snapshot()\nThis will update the lock file with a full list of the exact packages and dependencies, including the versions you have installed, providing a clear record of your working environment.\nThere are three snapshot types:\n\nImplicit - records any packages (and their dependencies) listed in DESCRIPTION or used in your code.\nExplicit - only records packages (and their dependencies) listed in DESCRIPTION.\nAll - records all packages in your environment.\n\nThe default snapshot type is implicit, and we recommend this approach. This is because it will catch any packages you are using but that you have forgot to add to DESCRIPTION (although it is best to remember to record these in DESCRIPTION, so you have a nice clear list of packages, and don‚Äôt have to delve into renv.lock if you‚Äôre having issues).\nThe downside to this snapshot type if that it may include unnecessary packages if you include old scripts in your repository that use packages you no longer need. However, this can be avoided by removing old scripts (good practice!).\nIf you want to check your snapshot type, run this command in the R console:\nrenv::settings$snapshot.type()\nYou can then change it if desired using one of:\nrenv::settings$snapshot.type(\"implicit\")\nrenv::settings$snapshot.type(\"explicit\")\nrenv::settings$snapshot.type(\"all\")",
    "crumbs": [
      "Setup",
      "Dependency management"
    ]
  },
  {
    "objectID": "pages/setup/environment.html#subsec-recreate-env",
    "href": "pages/setup/environment.html#subsec-recreate-env",
    "title": "Dependency management",
    "section": "",
    "text": "When you want to recreate an environment‚Äîsuch as for an old project or when collaborating‚Äîyou have two main options, depending on which files are available:\n\n\n\n\n\n\n\n\nMethod\nWhat does it install?\nBest for‚Ä¶\n\n\n\n\nFrom renv.lock\nInstalls the exact package versions (and their dependencies) used previously\nFull reproducibility; restoring the original environment exactly\n\n\nFrom DESCRIPTION\nInstalls the main packages listed (and their dependencies), but uses the latest available versions (unless specified)\nGetting started quickly or if renv.lock is missing or problematic\n\n\n\n\n1. Restoring from renv.lock (preferred). If the project includes an renv.lock file, use this as your first option. This file records the exact versions of all packages used, enabling you to recreate the environment as it was originally (*except for R version and operating system differences).\nTo restore the environment, run in your R console:\nrenv::restore()\nThis will attempt to install all packages at the precise versions specified.\nOccasionally, you may encounter issues - such as conflicts, unavailable packages, or operating system differences - especially with older projects or across different systems.\n\n2. Rebuilding from DESCRIPTION. If renv.lock is unavailable or causes problems, you can use the DESCRIPTION file. This file lists the main package dependencies, but typically does not specify exact versions (unless you have set minimum versions for specific needs).\nTo install packages listed in DESCRIPTION, run:\nrenv::install()\nThis will install the latest available versions of the listed packages and their dependencies. This approach is less precise than using renv.lock, so results may differ slightly from the original environment, especially if package updates have introduced changes.\nIn theory, the R ecosystem aspires to maintain backwards compatability, meaning that code written for older package versions should continue to work with newer ones. However, in practice, there is no strict guarantee of backward compatibility in R, either for the core language or for contributed packages.\nAs discussed in the R packages book:\n\n‚ÄúIf we‚Äôre being honest, most R users don‚Äôt manage package versions in a very intentional way. Given the way update.packages() and install.packages() work, it‚Äôs quite easy to upgrade a package to a new major version without really meaning to, especially for dependencies of the target package. This, in turn, can lead to unexpected exposure to breaking changes in code that previously worked. This unpleasantness has implications both for users and for maintainers.‚Äù\n\nHence, using a lockfile like renv.lock is the only reliable way to ensure that your environment is recreated exactly as it was - but DESCRIPTION can serve as a valuable back-up when this doesn‚Äôt work, and otherwise just as a handy summary of the main packages.\n\n3. If neither file is provided. If you have neither a renv.lock nor a DESCRIPTION file, you can try to create a DESCRIPTION file based on your knowledge of the project and its required packages.",
    "crumbs": [
      "Setup",
      "Dependency management"
    ]
  },
  {
    "objectID": "pages/setup/environment.html#system-dependencies",
    "href": "pages/setup/environment.html#system-dependencies",
    "title": "Dependency management",
    "section": "",
    "text": "Some R packages require external system libraries. The exact requirements will depend on which packages you use, what operating system you have, and whether you have used R before.\nIf these system libraries are missing, package installation may fail, even if you have the correct R package versions.\nFor example, working on Ubuntu, we found that we had to install the following system dependencies for igraph:\nsudo apt install build-essential gfortran\nsudo apt install libglpk-dev libxml2-dev\nYou should list any system dependencies that you are aware of in your project‚Äôs README or setup instructions.",
    "crumbs": [
      "Setup",
      "Dependency management"
    ]
  },
  {
    "objectID": "pages/setup/environment.html#summary",
    "href": "pages/setup/environment.html#summary",
    "title": "Dependency management",
    "section": "",
    "text": "We can use renv to create a reproducible environment in R. In the process above, we generated two key files:\n\nDESCRIPTION - lists project‚Äôs primary packages with any minimum version requirements, but not exact versions.\nrenv.lock - complements this by recording the precise versions of all packages and their dependencies.\n\nThese work together to both provide (a) a comprehensive record of your project environment, and (b) enable yourself or others to reconstruct the environment.",
    "crumbs": [
      "Setup",
      "Dependency management"
    ]
  },
  {
    "objectID": "pages/setup/environment.html#further-information-1",
    "href": "pages/setup/environment.html#further-information-1",
    "title": "Dependency management",
    "section": "",
    "text": "‚Äú9. DESCRIPTION‚Äù from R Packages by Hadley Wickham and Jennifer Bryan.\n‚Äú21. Lifecycle‚Äù from R Packages by Hadley Wickham and Jennifer Bryan.",
    "crumbs": [
      "Setup",
      "Dependency management"
    ]
  },
  {
    "objectID": "pages/setup/code_structure.html",
    "href": "pages/setup/code_structure.html",
    "title": "Code organisation",
    "section": "",
    "text": "üîó Reproducibility guidelines:\n\nHeather et al.¬†2025: Minimise code duplication.\nNHS Levels of RAP (ü•à): Reusable functions and/or classes are used where appropriate.\n\n\nIt might seem convenient to write all your code in a single script and simply copy-and-paste sections when you need to reuse something. However, this approach quickly becomes unmanageable as your project grows.\nThe main tools you can use to organise your code are functions and classes. These can then be stored in seperate files. Using these has has several benefits:\n\nEasier to read and collaborate on. Code is clearer and easier to understand for you and others.\nSimpler to maintain with fewer errors. Complex processes are broken into manageable parts. This makes them easier to update, test, and debug.\nNo duplication. Writing reusable code (like functions and classes) means changes only need to be made in one place, such as when you are updating or fixing a bug. In contrast, duplicated code requires updates everywhere it appears, and you may miss some.\n\n\nThis page provides a basic introduction to functions, classes, and the associated programming paradigms.\n\n\n\n\nFunctions group code into reusable blocks that perform a specific task. You define inputs (parameters), a sequence of steps (the function body), and outputs (return values).\nFunctions are ideal when you want to reuse actions to perform an operation or calculation.\nExample:\n\ndef add(a, b):\n    return a + b\n\n\nadd &lt;- function(a, b) {\n  a + b\n}\n\n\n\n\n\nClasses bundle together data (‚Äúattributes‚Äù) and behaviour (‚Äúmethods‚Äù). They become useful when you:\n\nNeed to keep track of state. This means you want to remember information about an object over time. For example, if you have a Patient class, each patient object can keep track of its patient ID, patient type, etc. You can update or check these attributes whenever you want.\nHave several related functions that operate on the same kind of data. Instead of writing separate functions that all work on the same data, you can put these inside a class as methods. This keeps your code organised and easier to use.\n\nYou first initialise the class with a special method called __init__. This methods runs when you create an object from the class. You pass parameters to it to set-up the initial attributes for that object.\nYou then have methods, which are like mini functions inside the class. They can access and change the class attributes, and can also accept additional inputs.\nYou create an instance of the class (called an ‚Äúobject‚Äù) to use it. Each attribute has its own copy of the attributes and can use the methods to work with its own data.\n\nExample:\nclass Dog:\n    def __init__(self, name):\n        self.name = name\n    def bark(self):\n        print(f\"{self.name} says woof!\")\n\nfido = Dog(\"Fido\")\nfido.bark()  # Output: Fido says woof!\n\n\nClasses are less common in R, but are used sometimes for complex or structure tasks. R supports several class systems, including S3, S4 and R6. The R6 class is shown as an example below, and is the most similar to classes in Python.\n\nExample:\nlibrary(R6)\n\nDog &lt;- R6Class(\"Dog\",\n  public = list(\n    name = NULL,\n    initialize = function(name) {\n      self$name &lt;- name\n    },\n    bark = function() {\n      cat(self$name, \"says woof!\\n\")\n    }\n  )\n)\n\n# Create an instance and use it:\nfido &lt;- Dog$new(\"Fido\")\nfido$bark()  # Output: Fido says woof!\n\n\n\n\nA subclass (or ‚Äúchild class‚Äù) is a class that inherits from another class (the ‚Äúparent‚Äù or ‚Äúsuperclass‚Äù). Subclasses can reuse or extend the behaviour of their parent.\nExample:\n\nclass Animal:\n    def __init__(self, name, diet):\n        self.name = name\n        self.diet = diet\n    \n    def describe(self):\n        return f\"{self.name} is a {self.diet}\"\n\n\nclass Cheetah(Animal):\n    def __init__(self, name):\n        super().__init__(name, diet=\"carnivore\")\n    \n    def activity(self):\n        base = self.describe()\n        return f\"{base}. Sprinting at 70 mph!\"\n\n\ncharlie = Cheetah(\"Charlie\")\nprint(charlie.activity())\n# Output: Charlie is a carnivore. Sprinting at 70 mph!\n\n\nlibrary(R6)\n\n\nAnimal &lt;- R6Class(\"Animal\",\n  public = list(\n    name = NULL,\n    diet = NULL,\n    initialize = function(name, diet) {\n      self$name &lt;- name\n      self$diet &lt;- diet\n    },\n    describe = function() {\n      paste(self$name, \"is a\", self$diet)\n    },\n  )\n)\n\n\nCheetah &lt;- R6Class(\"Cheetah\",\n  inherit = Animal,\n  public = list(\n    initialize = function(name) {\n      super$initialize(name, diet = \"carnivore\")\n    },\n    activity = function() {\n      base &lt;- super$describe()\n      paste(base, \"Sprinting at 70 mph!\")\n    }\n  )\n)\n\n\ncharlie &lt;- Cheetah$new(\"Charlie\")\ncat(charlie$activity())\n# Output: Charlie is a carnivore Sprinting at 70 mph!\n\n\n\n\n\n\nA programming paradigm is a general style or approach to organising and structuring code. The most common paradigms in Python and R are:\n\nUsing functions - procedural programming; functional programming.\nUsing classes - object-oriented programming (OOP).\n\n\n\n\n\n\n\nClick here to find out more about programming paradigms‚Ä¶\n\n\n\n\n\nThis table provides a brief overview of the programming paradigms. This is just a quick overview - if you want to find out more, check out the resources linked in Section¬†1.4.\n\n\n\n\n\n\n\n\nParadigm\nMain object used\nKey characteristics\n\n\n\n\nProcedural programming\nFunctions\n‚Ä¢ Code runs step-by-step using functions, often passing data from one to the next‚Ä¢ Data structures (like lists) are usually mutable (i.e.¬†can be changed directly).‚Ä¢ Use loops (for, while) to repeat actions.\n\n\nFunctional programming\nFunctions\n‚Ä¢ Uses pure functions (always same output for same input, no side effects - i.e.¬†doesn‚Äôt change variables elsewhere, only effect is to return value)‚Ä¢ Functions are ‚Äúfirst-class citizens‚Äù (i.e.¬†can be assigned to variables, passed as arguments, or returned from functions).‚Ä¢ Data structures are immutable (can‚Äôt be changed; new ones are created for each change).‚Ä¢ Repeats actions with recursion (functions calling themselves) and higher-order functions (e.g.¬†Python‚Äôs map, R‚Äôs sapply).‚Ä¢These features can help make your code more robust and easier to maintain than procedural programming.\n\n\nObject-oriented programming (OOP)\nClasses/Objects\n‚Ä¢ Orgnaises code into objects (instances of classes) that bundle data (attributes) and behaviour (methods).‚Ä¢ Data and methods are bundled together (‚Äúencapsulation‚Äù).‚Ä¢ Can hide details through private attributes and methods (‚Äúabstraction‚Äù).‚Ä¢ A class can inherit from another class.‚Ä¢ The same object can behave diferently depending on context (‚Äúpolymorphism‚Äù).\n\n\n\n\n\n\nNormally, a mix of programming paradigms will be used.\nIn this book, you‚Äôll see both functions and classes used, depending on what made sense for the task. Generally, we‚Äôve used more classes in Python and more functions in R - as is typical, since R code often relies less on classes - but neither approach is exclusive, and often either could have been used. The choice is mostly a matter of language style, clarity, and what fits best for the problem at hand.\n\n\n\n\n\n‚ÄúIntroduction to Programming Paradigms‚Äù by Samuel Shaibu (Datacamp, 2024).\n‚ÄúOOP vs Functional vs Procedural‚Äù (Scaler Topics, 2022).",
    "crumbs": [
      "Setup",
      "Code organisation"
    ]
  },
  {
    "objectID": "pages/setup/code_structure.html#functions",
    "href": "pages/setup/code_structure.html#functions",
    "title": "Code organisation",
    "section": "",
    "text": "Functions group code into reusable blocks that perform a specific task. You define inputs (parameters), a sequence of steps (the function body), and outputs (return values).\nFunctions are ideal when you want to reuse actions to perform an operation or calculation.\nExample:\n\ndef add(a, b):\n    return a + b\n\n\nadd &lt;- function(a, b) {\n  a + b\n}",
    "crumbs": [
      "Setup",
      "Code organisation"
    ]
  },
  {
    "objectID": "pages/setup/code_structure.html#classes",
    "href": "pages/setup/code_structure.html#classes",
    "title": "Code organisation",
    "section": "",
    "text": "Classes bundle together data (‚Äúattributes‚Äù) and behaviour (‚Äúmethods‚Äù). They become useful when you:\n\nNeed to keep track of state. This means you want to remember information about an object over time. For example, if you have a Patient class, each patient object can keep track of its patient ID, patient type, etc. You can update or check these attributes whenever you want.\nHave several related functions that operate on the same kind of data. Instead of writing separate functions that all work on the same data, you can put these inside a class as methods. This keeps your code organised and easier to use.\n\nYou first initialise the class with a special method called __init__. This methods runs when you create an object from the class. You pass parameters to it to set-up the initial attributes for that object.\nYou then have methods, which are like mini functions inside the class. They can access and change the class attributes, and can also accept additional inputs.\nYou create an instance of the class (called an ‚Äúobject‚Äù) to use it. Each attribute has its own copy of the attributes and can use the methods to work with its own data.\n\nExample:\nclass Dog:\n    def __init__(self, name):\n        self.name = name\n    def bark(self):\n        print(f\"{self.name} says woof!\")\n\nfido = Dog(\"Fido\")\nfido.bark()  # Output: Fido says woof!\n\n\nClasses are less common in R, but are used sometimes for complex or structure tasks. R supports several class systems, including S3, S4 and R6. The R6 class is shown as an example below, and is the most similar to classes in Python.\n\nExample:\nlibrary(R6)\n\nDog &lt;- R6Class(\"Dog\",\n  public = list(\n    name = NULL,\n    initialize = function(name) {\n      self$name &lt;- name\n    },\n    bark = function() {\n      cat(self$name, \"says woof!\\n\")\n    }\n  )\n)\n\n# Create an instance and use it:\nfido &lt;- Dog$new(\"Fido\")\nfido$bark()  # Output: Fido says woof!\n\n\n\n\nA subclass (or ‚Äúchild class‚Äù) is a class that inherits from another class (the ‚Äúparent‚Äù or ‚Äúsuperclass‚Äù). Subclasses can reuse or extend the behaviour of their parent.\nExample:\n\nclass Animal:\n    def __init__(self, name, diet):\n        self.name = name\n        self.diet = diet\n    \n    def describe(self):\n        return f\"{self.name} is a {self.diet}\"\n\n\nclass Cheetah(Animal):\n    def __init__(self, name):\n        super().__init__(name, diet=\"carnivore\")\n    \n    def activity(self):\n        base = self.describe()\n        return f\"{base}. Sprinting at 70 mph!\"\n\n\ncharlie = Cheetah(\"Charlie\")\nprint(charlie.activity())\n# Output: Charlie is a carnivore. Sprinting at 70 mph!\n\n\nlibrary(R6)\n\n\nAnimal &lt;- R6Class(\"Animal\",\n  public = list(\n    name = NULL,\n    diet = NULL,\n    initialize = function(name, diet) {\n      self$name &lt;- name\n      self$diet &lt;- diet\n    },\n    describe = function() {\n      paste(self$name, \"is a\", self$diet)\n    },\n  )\n)\n\n\nCheetah &lt;- R6Class(\"Cheetah\",\n  inherit = Animal,\n  public = list(\n    initialize = function(name) {\n      super$initialize(name, diet = \"carnivore\")\n    },\n    activity = function() {\n      base &lt;- super$describe()\n      paste(base, \"Sprinting at 70 mph!\")\n    }\n  )\n)\n\n\ncharlie &lt;- Cheetah$new(\"Charlie\")\ncat(charlie$activity())\n# Output: Charlie is a carnivore Sprinting at 70 mph!",
    "crumbs": [
      "Setup",
      "Code organisation"
    ]
  },
  {
    "objectID": "pages/setup/code_structure.html#programming-paradigms",
    "href": "pages/setup/code_structure.html#programming-paradigms",
    "title": "Code organisation",
    "section": "",
    "text": "A programming paradigm is a general style or approach to organising and structuring code. The most common paradigms in Python and R are:\n\nUsing functions - procedural programming; functional programming.\nUsing classes - object-oriented programming (OOP).\n\n\n\n\n\n\n\nClick here to find out more about programming paradigms‚Ä¶\n\n\n\n\n\nThis table provides a brief overview of the programming paradigms. This is just a quick overview - if you want to find out more, check out the resources linked in Section¬†1.4.\n\n\n\n\n\n\n\n\nParadigm\nMain object used\nKey characteristics\n\n\n\n\nProcedural programming\nFunctions\n‚Ä¢ Code runs step-by-step using functions, often passing data from one to the next‚Ä¢ Data structures (like lists) are usually mutable (i.e.¬†can be changed directly).‚Ä¢ Use loops (for, while) to repeat actions.\n\n\nFunctional programming\nFunctions\n‚Ä¢ Uses pure functions (always same output for same input, no side effects - i.e.¬†doesn‚Äôt change variables elsewhere, only effect is to return value)‚Ä¢ Functions are ‚Äúfirst-class citizens‚Äù (i.e.¬†can be assigned to variables, passed as arguments, or returned from functions).‚Ä¢ Data structures are immutable (can‚Äôt be changed; new ones are created for each change).‚Ä¢ Repeats actions with recursion (functions calling themselves) and higher-order functions (e.g.¬†Python‚Äôs map, R‚Äôs sapply).‚Ä¢These features can help make your code more robust and easier to maintain than procedural programming.\n\n\nObject-oriented programming (OOP)\nClasses/Objects\n‚Ä¢ Orgnaises code into objects (instances of classes) that bundle data (attributes) and behaviour (methods).‚Ä¢ Data and methods are bundled together (‚Äúencapsulation‚Äù).‚Ä¢ Can hide details through private attributes and methods (‚Äúabstraction‚Äù).‚Ä¢ A class can inherit from another class.‚Ä¢ The same object can behave diferently depending on context (‚Äúpolymorphism‚Äù).\n\n\n\n\n\n\nNormally, a mix of programming paradigms will be used.\nIn this book, you‚Äôll see both functions and classes used, depending on what made sense for the task. Generally, we‚Äôve used more classes in Python and more functions in R - as is typical, since R code often relies less on classes - but neither approach is exclusive, and often either could have been used. The choice is mostly a matter of language style, clarity, and what fits best for the problem at hand.",
    "crumbs": [
      "Setup",
      "Code organisation"
    ]
  },
  {
    "objectID": "pages/setup/code_structure.html#sec-code_structure_further_reading",
    "href": "pages/setup/code_structure.html#sec-code_structure_further_reading",
    "title": "Code organisation",
    "section": "",
    "text": "‚ÄúIntroduction to Programming Paradigms‚Äù by Samuel Shaibu (Datacamp, 2024).\n‚ÄúOOP vs Functional vs Procedural‚Äù (Scaler Topics, 2022).",
    "crumbs": [
      "Setup",
      "Code organisation"
    ]
  },
  {
    "objectID": "pages/intro/rap.html",
    "href": "pages/intro/rap.html",
    "title": "Reproducibility and RAPs",
    "section": "",
    "text": "Reproducibility = the ability to regenerate published results using the provided code and data.\n\n\n\nIt‚Äôs helpful to distinguish reproducibility from related concepts:\n\nReusability refers to whether code can be adapted and applied to new contexts.\nReplicability means others can independently implement your methods (without your code) and get the same results.\n\n\n\n\n\n\n\n\n\n‚úÖ Reuse. Reproducibility makes it much easier for you to revisit and reuse your own code, whether you‚Äôre updating outputs or conducting new analyses.\n\n\nExample: Peer review. The mean time from submission to publication in biomedical research ranges from 27 to 639 days (Andersen et al.¬†2021) - so it‚Äôs crucial that your code remains functional and understandable long after it‚Äôs initial execution.\n\n\n‚úÖ Quality. Aiming for reproducibility encourages you to structure code clearly and document your workflow thoroughly. This will help reduce the risk of errors and ambiguities, and improve code quality.\n‚úÖ Time. Even if you retain access to your code, failure to record exact parameters for every scenario or document the computational environment used, for example, could make the code non-reproducible when revisited, even if only a few months later. Troubleshooting non-reproducible code can be time-consuming or even impossible if required information is lost.\n\nMany of these advantages apply regardless fo whether code is shared publicly or remains propreitrary.\n\n\n\n\n‚úÖ Trust. If work is reproducibility, it builds trust, as others can independently confirm that your results are reliable and your methods are transparent.\n‚úÖ Reuse. If others can reproduce your work, this verifies that the code is working as expected when they run it. Knowing that the underlying code works as expected, they are then able to confidently reuse and adapt the code for new contexts and projects.\n\n\n\n\n\n\n\nReproducible analytical pipeline (RAP) = systematic approach to data analysis and modelling in which every step of the process (end-to-end) is transparent, automated, and repeatable.\n\nRather than relying on manual processes or undocumented decisions, a RAP ensures that the entire workflow - from data ingestion and cleaning, to modelling, and through to analysis (and sometimes also reporting) - is scripted, documented, and reproducible.\nThere should be no (or minimal) manual intervention required to generate the results. The requirements of a RAP are outlined in the NHS RAP Maturity Framework (as detailed in ?@sec-guidelines).\n\n\n\n\n\nYou can do reproducible work without building a full RAP - but a RAP takes things further. Reproducible analysis may still involve manual steps, selective re-running of scripts, or undocumented decisions. This leaves room for human error and inefficiency, and makes it harder to check and re-run the work.\nA RAP is distinct because it automates the entire workflow, minimising manual intervention and using software engineering best practices like version control, peer review, and modular code. This makes your analysis more efficient and robust, and easier to re-run and review.\n\n\n\n\nThis book does both! As detailed in the ?@sec-guidelines page:\n\nIt aims to make your work reproducible, drawing on recommendations from Heather et al.¬†(2025) for reproducible discrete-event simulation (DES)\nIt also guides you through building a full RAP, following the NHS ‚ÄúLevels of RAP‚Äù maturity framework.",
    "crumbs": [
      "Introduction",
      "Reproducibility and RAPs"
    ]
  },
  {
    "objectID": "pages/intro/rap.html#reproducibility",
    "href": "pages/intro/rap.html#reproducibility",
    "title": "Reproducibility and RAPs",
    "section": "",
    "text": "Reproducibility = the ability to regenerate published results using the provided code and data.\n\n\n\nIt‚Äôs helpful to distinguish reproducibility from related concepts:\n\nReusability refers to whether code can be adapted and applied to new contexts.\nReplicability means others can independently implement your methods (without your code) and get the same results.",
    "crumbs": [
      "Introduction",
      "Reproducibility and RAPs"
    ]
  },
  {
    "objectID": "pages/intro/rap.html#why-does-reproducibility-matter",
    "href": "pages/intro/rap.html#why-does-reproducibility-matter",
    "title": "Reproducibility and RAPs",
    "section": "",
    "text": "‚úÖ Reuse. Reproducibility makes it much easier for you to revisit and reuse your own code, whether you‚Äôre updating outputs or conducting new analyses.\n\n\nExample: Peer review. The mean time from submission to publication in biomedical research ranges from 27 to 639 days (Andersen et al.¬†2021) - so it‚Äôs crucial that your code remains functional and understandable long after it‚Äôs initial execution.\n\n\n‚úÖ Quality. Aiming for reproducibility encourages you to structure code clearly and document your workflow thoroughly. This will help reduce the risk of errors and ambiguities, and improve code quality.\n‚úÖ Time. Even if you retain access to your code, failure to record exact parameters for every scenario or document the computational environment used, for example, could make the code non-reproducible when revisited, even if only a few months later. Troubleshooting non-reproducible code can be time-consuming or even impossible if required information is lost.\n\nMany of these advantages apply regardless fo whether code is shared publicly or remains propreitrary.\n\n\n\n\n‚úÖ Trust. If work is reproducibility, it builds trust, as others can independently confirm that your results are reliable and your methods are transparent.\n‚úÖ Reuse. If others can reproduce your work, this verifies that the code is working as expected when they run it. Knowing that the underlying code works as expected, they are then able to confidently reuse and adapt the code for new contexts and projects.",
    "crumbs": [
      "Introduction",
      "Reproducibility and RAPs"
    ]
  },
  {
    "objectID": "pages/intro/rap.html#reproducible-analytical-pipelines-rap",
    "href": "pages/intro/rap.html#reproducible-analytical-pipelines-rap",
    "title": "Reproducibility and RAPs",
    "section": "",
    "text": "Reproducible analytical pipeline (RAP) = systematic approach to data analysis and modelling in which every step of the process (end-to-end) is transparent, automated, and repeatable.\n\nRather than relying on manual processes or undocumented decisions, a RAP ensures that the entire workflow - from data ingestion and cleaning, to modelling, and through to analysis (and sometimes also reporting) - is scripted, documented, and reproducible.\nThere should be no (or minimal) manual intervention required to generate the results. The requirements of a RAP are outlined in the NHS RAP Maturity Framework (as detailed in ?@sec-guidelines).",
    "crumbs": [
      "Introduction",
      "Reproducibility and RAPs"
    ]
  },
  {
    "objectID": "pages/intro/rap.html#reproducible-work-v.s.-raps",
    "href": "pages/intro/rap.html#reproducible-work-v.s.-raps",
    "title": "Reproducibility and RAPs",
    "section": "",
    "text": "You can do reproducible work without building a full RAP - but a RAP takes things further. Reproducible analysis may still involve manual steps, selective re-running of scripts, or undocumented decisions. This leaves room for human error and inefficiency, and makes it harder to check and re-run the work.\nA RAP is distinct because it automates the entire workflow, minimising manual intervention and using software engineering best practices like version control, peer review, and modular code. This makes your analysis more efficient and robust, and easier to re-run and review.",
    "crumbs": [
      "Introduction",
      "Reproducibility and RAPs"
    ]
  },
  {
    "objectID": "pages/intro/rap.html#focus-of-this-book",
    "href": "pages/intro/rap.html#focus-of-this-book",
    "title": "Reproducibility and RAPs",
    "section": "",
    "text": "This book does both! As detailed in the ?@sec-guidelines page:\n\nIt aims to make your work reproducible, drawing on recommendations from Heather et al.¬†(2025) for reproducible discrete-event simulation (DES)\nIt also guides you through building a full RAP, following the NHS ‚ÄúLevels of RAP‚Äù maturity framework.",
    "crumbs": [
      "Introduction",
      "Reproducibility and RAPs"
    ]
  },
  {
    "objectID": "pages/intro/foss.html",
    "href": "pages/intro/foss.html",
    "title": "Open-source languages",
    "section": "",
    "text": "üîó Reproducibility guidelines:\n\nNHS Levels of RAP (ü•â): Data produced by code in an open-source language (e.g., Python, R, SQL).\n\n\n\n\nFree and open-source software (FOSS) is a type of software that anyone can use, change and share with others.\n\n‚ÄúFree‚Äù means you have the freedom to do what you want with the software - i.e.¬†you are free to use, modify and distribute the software.\n‚ÄúOpen-source‚Äù means that the software‚Äôs source code is publicly accessible, allowing anyone to look at, change and share it.\n\n\n\n\n\nTechnically, you can build a reproducible analytical pipeline (RAP) using proprietary software. However, FOSS is strongly preferred - and often a requirement (such as in the NHS Levels of RAP Bronze tier ü•â).\nThis is because there is:\n\nNo licence barriers. Anyone can run and check the pipeline, without needing to buy or manage licences.\nNo restrictions on distribution. You can freely share your code and workflow.\nNo risk of losing access. There‚Äôs no danger of losing the analysis if a company changes its policies or you can‚Äôt renew a licence.\nTransparency. FOSS code is always open for inspection. While you can share proprietary code, the underlying software itself is often a ‚Äúblack box,‚Äù making full transparency harder to guarantee.\n\n\n\n\n\n\nThe two most widely used FOSS programming languages for healthcare DES are Python and R. Within both languages, you can either:\n\nBuild simulations from scratch using general-purpose libraries, or-\nUse purpose-built simulation packages - for example:\n\nPython - SimPy, Ciw, salabim.\nR - simmer, descem\n\n\nBuilding from scratch offers maximum flexibility and control, but is much more complex, requiring significant time and expertise. You are responsible for thoroughly testing all aspects of your code, including both the core simulation mechanics (such as event handling and resource management) and your specific model logic.\nUsing a simulation package is often preferable, especially if it is well-established and widely used. Such packages benefit from peer review, community testing, and ongoing maintenance. With a package, the core simulation components should already be well-tested by the package developers and user community, allowing you to focus your testing on your model‚Äôs logic. This reduces development effort and the risk of errors. However, it is important to choose a package that is actively maintained, as older or less popular options may become outdated or unsupported.\nSimPy (Python) and simmer (R) were selected for this work because they are the most established, well-maintained, and widely used FOSS DES packages in their respective languages. Both have extensive documentation, active user communities, and proven track records in healthcare and operations research.\n\n\n\n\n\n‚ÄúWhat is FOSS?‚Äù from FOSS United 2024.",
    "crumbs": [
      "Introduction",
      "Open-source languages"
    ]
  },
  {
    "objectID": "pages/intro/foss.html#what-is-it",
    "href": "pages/intro/foss.html#what-is-it",
    "title": "Open-source languages",
    "section": "",
    "text": "Free and open-source software (FOSS) is a type of software that anyone can use, change and share with others.\n\n‚ÄúFree‚Äù means you have the freedom to do what you want with the software - i.e.¬†you are free to use, modify and distribute the software.\n‚ÄúOpen-source‚Äù means that the software‚Äôs source code is publicly accessible, allowing anyone to look at, change and share it.",
    "crumbs": [
      "Introduction",
      "Open-source languages"
    ]
  },
  {
    "objectID": "pages/intro/foss.html#using-foss-for-reproducible-analytical-pipelines-raps",
    "href": "pages/intro/foss.html#using-foss-for-reproducible-analytical-pipelines-raps",
    "title": "Open-source languages",
    "section": "",
    "text": "Technically, you can build a reproducible analytical pipeline (RAP) using proprietary software. However, FOSS is strongly preferred - and often a requirement (such as in the NHS Levels of RAP Bronze tier ü•â).\nThis is because there is:\n\nNo licence barriers. Anyone can run and check the pipeline, without needing to buy or manage licences.\nNo restrictions on distribution. You can freely share your code and workflow.\nNo risk of losing access. There‚Äôs no danger of losing the analysis if a company changes its policies or you can‚Äôt renew a licence.\nTransparency. FOSS code is always open for inspection. While you can share proprietary code, the underlying software itself is often a ‚Äúblack box,‚Äù making full transparency harder to guarantee.",
    "crumbs": [
      "Introduction",
      "Open-source languages"
    ]
  },
  {
    "objectID": "pages/intro/foss.html#discrete-event-simulation-des-in-foss",
    "href": "pages/intro/foss.html#discrete-event-simulation-des-in-foss",
    "title": "Open-source languages",
    "section": "",
    "text": "The two most widely used FOSS programming languages for healthcare DES are Python and R. Within both languages, you can either:\n\nBuild simulations from scratch using general-purpose libraries, or-\nUse purpose-built simulation packages - for example:\n\nPython - SimPy, Ciw, salabim.\nR - simmer, descem\n\n\nBuilding from scratch offers maximum flexibility and control, but is much more complex, requiring significant time and expertise. You are responsible for thoroughly testing all aspects of your code, including both the core simulation mechanics (such as event handling and resource management) and your specific model logic.\nUsing a simulation package is often preferable, especially if it is well-established and widely used. Such packages benefit from peer review, community testing, and ongoing maintenance. With a package, the core simulation components should already be well-tested by the package developers and user community, allowing you to focus your testing on your model‚Äôs logic. This reduces development effort and the risk of errors. However, it is important to choose a package that is actively maintained, as older or less popular options may become outdated or unsupported.\nSimPy (Python) and simmer (R) were selected for this work because they are the most established, well-maintained, and widely used FOSS DES packages in their respective languages. Both have extensive documentation, active user communities, and proven track records in healthcare and operations research.",
    "crumbs": [
      "Introduction",
      "Open-source languages"
    ]
  },
  {
    "objectID": "pages/intro/foss.html#further-information",
    "href": "pages/intro/foss.html#further-information",
    "title": "Open-source languages",
    "section": "",
    "text": "‚ÄúWhat is FOSS?‚Äù from FOSS United 2024.",
    "crumbs": [
      "Introduction",
      "Open-source languages"
    ]
  },
  {
    "objectID": "pages/experiments/scenarios.html",
    "href": "pages/experiments/scenarios.html",
    "title": "Scenario analysis",
    "section": "",
    "text": "Scenario analysis\n\nüîó Reproducibility guidelines:\n\nHeather et al.¬†2025 (‚≠ê): Provide code for all scenarios and sensitivity analyses.",
    "crumbs": [
      "Experimentation",
      "Scenario analysis"
    ]
  },
  {
    "objectID": "pages/style_docs/documentation.html",
    "href": "pages/style_docs/documentation.html",
    "title": "Documentation",
    "section": "",
    "text": "Documentation\n\nüîó Reproducibility guidelines:\n\nHeather et al.¬†2025: Include run instructions.\nHeather et al.¬†2025: State run times and machine specifications.\nNHS Levels of RAP (ü•â): Repository includes a README.md file (or equivalent) that clearly details steps a user must follow to reproduce the code (use NHS Open Source Policy section on Readmes as a guide).\nNHS Levels of RAP (ü•à): Code is well-documented including user guidance, explanation of code structure & methodology and docstrings for functions.",
    "crumbs": [
      "Style & documentation",
      "Documentation"
    ]
  },
  {
    "objectID": "pages/style_docs/docstrings.html",
    "href": "pages/style_docs/docstrings.html",
    "title": "Docstrings",
    "section": "",
    "text": "Docstrings\n\nüîó Reproducibility guidelines:\n\nHeather et al.¬†2025: Comment sufficiently.\nNHS Levels of RAP (ü•à): Code is well-documented including user guidance, explanation of code structure & methodology and docstrings for functions.",
    "crumbs": [
      "Style & documentation",
      "Docstrings"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome",
    "section": "",
    "text": "This is a guide to writing discrete-event simulation (DES) models in Python and R which are reproducible and form a reproducible analytical pipeline (RAP).\nIt guides you through creating these models whilst meeting requirements from:\n\nüåø Heather et al.¬†2025 ‚ÄúOn the reproducibility of discrete-event simulation studies in health research: an empirical study using open models‚Äù\nüè• The ‚ÄúLevels of RAP‚Äù framework from the NHS RAP Community of Practice\n\nThe models are created using:\n\nSimPy for Python - https://simpy.readthedocs.io/en/latest/.\nsimmer for R - https://r-simmer.org/.\n\n\nHow to use this book?\n\nGo step-by-step. Follow the book from beginning to end to build a reproducible DES model from scratch; or-\nPick and choose. Skip around and dive into sections that are most helpful for your RAP or project.\n\nNote: Even though the main example is about healthcare simulation, the tips on reproducibility and coding practices apply to all kinds of models, fields, and analysis techniques.\n\nIt has been developed as part of the STARS project:\n\nAll code in this book is provided under the MIT licence. The accompanying text is licensed under a Creative Commons Attribution-ShareAlike 4.0 International license.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "pages/style_docs/github_actions.html",
    "href": "pages/style_docs/github_actions.html",
    "title": "GitHub actions",
    "section": "",
    "text": "GitHub actions\n\nüîó Reproducibility guidelines:\n\nNHS Levels of RAP (ü•á): Repository automatically runs tests etc. via CI/CD or a different integration/deployment tool e.g.¬†GitHub Actions.",
    "crumbs": [
      "Style & documentation",
      "GitHub actions"
    ]
  },
  {
    "objectID": "pages/style_docs/linting.html",
    "href": "pages/style_docs/linting.html",
    "title": "Linting",
    "section": "",
    "text": "Linting\n\nüîó Reproducibility guidelines:\n\nNHS Levels of RAP (ü•à): Code adheres to agreed coding standards (e.g PEP8, style guide for Pyspark).",
    "crumbs": [
      "Style & documentation",
      "Linting"
    ]
  },
  {
    "objectID": "pages/experiments/sensitivity.html",
    "href": "pages/experiments/sensitivity.html",
    "title": "Sensitivity analysis",
    "section": "",
    "text": "Sensitivity analysis\n\nüîó Reproducibility guidelines:\n\nHeather et al.¬†2025 (‚≠ê): Provide code for all scenarios and sensitivity analyses.",
    "crumbs": [
      "Experimentation",
      "Sensitivity analysis"
    ]
  },
  {
    "objectID": "pages/intro/examples.html",
    "href": "pages/intro/examples.html",
    "title": "Example conceptual models",
    "section": "",
    "text": "To illustrate how to construct reproducible simulation projects, we will use two examples:\n\nNurse visit simulation. A classic queueing model, serves as a straightforward, ‚Äúdummy‚Äù example to demonstrate basic concepts.\nStroke pathway simulation. A real-world healthcare pathway model taken from a publish study, showing how similar methods can be applied in practice.\n\n\n\n\nThis model represents a typical scenario where patients arrive at a clinic, wait to see a nurse, receive a consultation, and then leave. The aim is to understand resource utilisation (nurses) and patient waiting times.\nStructure:\n\nType: Discrete-event simulation of a queueing system.\nKey features:\n\nArrivals: Patients arrival randomly, following a Poisson process. This means that the number of arrivals in a given period follows a Poisson distribution, and the time between each arrival follows an exponential distribution- two ways of describing the same process.\nService: Each nurse serves one patient at a time. Service times are exponentially distributed.\nServers: Multiple nurses work in parallel.\n\nInputs:\n\nAverage patient arrival rate.\nAverage nurse service duration.\nNumber of servers.\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThis is an example of an M/M/s queueing model. An M/M/s model as Markovian assumptions (‚ÄúM/M‚Äù) that arrivals are poisson distributed and server times exponentially distributed, and the ‚Äús‚Äù refers to the number of parallel servers available. It is a classic and widely used queueing model, and is also known as a ‚Äúmulti-server‚Äù or ‚ÄúErlang delay‚Äù model.\nThis model could be applied to a range of contexts, including:\n\n\n\n\n\n\n\nQueue\nServer/Resource\n\n\n\n\nPatients in a waiting room\nDoctor‚Äôs consultation\n\n\nPatients waiting for an ICU bed\nAvailable ICU beds\n\n\nPrescriptions waiting to be processed\nPharmacists preparing and dispensing medications\n\n\n\nFor further information on M/M/s models, see:\n\nGanesh, A. (2012). Simple queueing models. University of Bristol. https://people.maths.bris.ac.uk/~maajg/teaching/iqn/queues.pdf.\nGreen, L. (2011). Queueing theory and modeling. In Handbook of Healthcare Delivery Systems. Taylor & Francis. https://business.columbia.edu/faculty/research/queueing-theory-and-modeling.\n\n\n\n\n\n\n\nThis model is based on a published study of stroke care. It simulates patient flow through acute stroke and rehabilitation units to support capacity planning and estimate the likelihood of admission delays for different patient groups. It is described in:\n\nMonks T, Worthington D, Allen M, Pitt M, Stein K, James MA. A modelling tool for capacity planning in acute and community stroke services. BMC Health Serv Res. 2016 Sep 29;16(1):530. doi: 10.1186/s12913-016-1789-4. PMID: 27688152; PMCID: PMC5043535.\n\nThe original study implemented the model using SIMUL8 software. Based on the published description of the model‚Äôs logic and parameters, we have recreated it in both Python and R.\nStructure:\n\nType: Discrete-event simulation of a patient pathway.\nArrivals: Multiple patient classes (e.g., stroke, TIA, complex neurological, other) each have their own arrival processes to the acute stroke unit and rehabilitation unit, with inter-arrival times drawn from specified distributions.\nService: Patients experience lengths of stay in each unit based on distributions specific to their class and care type (e.g., lognormal for length of stay).\nRouting: After acute care, patients are routed to rehabilitation, early supported discharge, or exit the system, according to probabilities defined for each patient class.\nResources: The model assumes infinite capacity for both acute and rehabilitation beds (i.e., no explicit queueing)\n\nPatient pathway / process flow diagram:",
    "crumbs": [
      "Introduction",
      "Example conceptual models"
    ]
  },
  {
    "objectID": "pages/intro/examples.html#example-1-nurse-visit-simulation",
    "href": "pages/intro/examples.html#example-1-nurse-visit-simulation",
    "title": "Example conceptual models",
    "section": "",
    "text": "This model represents a typical scenario where patients arrive at a clinic, wait to see a nurse, receive a consultation, and then leave. The aim is to understand resource utilisation (nurses) and patient waiting times.\nStructure:\n\nType: Discrete-event simulation of a queueing system.\nKey features:\n\nArrivals: Patients arrival randomly, following a Poisson process. This means that the number of arrivals in a given period follows a Poisson distribution, and the time between each arrival follows an exponential distribution- two ways of describing the same process.\nService: Each nurse serves one patient at a time. Service times are exponentially distributed.\nServers: Multiple nurses work in parallel.\n\nInputs:\n\nAverage patient arrival rate.\nAverage nurse service duration.\nNumber of servers.\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThis is an example of an M/M/s queueing model. An M/M/s model as Markovian assumptions (‚ÄúM/M‚Äù) that arrivals are poisson distributed and server times exponentially distributed, and the ‚Äús‚Äù refers to the number of parallel servers available. It is a classic and widely used queueing model, and is also known as a ‚Äúmulti-server‚Äù or ‚ÄúErlang delay‚Äù model.\nThis model could be applied to a range of contexts, including:\n\n\n\n\n\n\n\nQueue\nServer/Resource\n\n\n\n\nPatients in a waiting room\nDoctor‚Äôs consultation\n\n\nPatients waiting for an ICU bed\nAvailable ICU beds\n\n\nPrescriptions waiting to be processed\nPharmacists preparing and dispensing medications\n\n\n\nFor further information on M/M/s models, see:\n\nGanesh, A. (2012). Simple queueing models. University of Bristol. https://people.maths.bris.ac.uk/~maajg/teaching/iqn/queues.pdf.\nGreen, L. (2011). Queueing theory and modeling. In Handbook of Healthcare Delivery Systems. Taylor & Francis. https://business.columbia.edu/faculty/research/queueing-theory-and-modeling.",
    "crumbs": [
      "Introduction",
      "Example conceptual models"
    ]
  },
  {
    "objectID": "pages/intro/examples.html#example-2-stroke-pathway-simulation",
    "href": "pages/intro/examples.html#example-2-stroke-pathway-simulation",
    "title": "Example conceptual models",
    "section": "",
    "text": "This model is based on a published study of stroke care. It simulates patient flow through acute stroke and rehabilitation units to support capacity planning and estimate the likelihood of admission delays for different patient groups. It is described in:\n\nMonks T, Worthington D, Allen M, Pitt M, Stein K, James MA. A modelling tool for capacity planning in acute and community stroke services. BMC Health Serv Res. 2016 Sep 29;16(1):530. doi: 10.1186/s12913-016-1789-4. PMID: 27688152; PMCID: PMC5043535.\n\nThe original study implemented the model using SIMUL8 software. Based on the published description of the model‚Äôs logic and parameters, we have recreated it in both Python and R.\nStructure:\n\nType: Discrete-event simulation of a patient pathway.\nArrivals: Multiple patient classes (e.g., stroke, TIA, complex neurological, other) each have their own arrival processes to the acute stroke unit and rehabilitation unit, with inter-arrival times drawn from specified distributions.\nService: Patients experience lengths of stay in each unit based on distributions specific to their class and care type (e.g., lognormal for length of stay).\nRouting: After acute care, patients are routed to rehabilitation, early supported discharge, or exit the system, according to probabilities defined for each patient class.\nResources: The model assumes infinite capacity for both acute and rehabilitation beds (i.e., no explicit queueing)\n\nPatient pathway / process flow diagram:",
    "crumbs": [
      "Introduction",
      "Example conceptual models"
    ]
  },
  {
    "objectID": "pages/intro/guidelines.html",
    "href": "pages/intro/guidelines.html",
    "title": "Guidelines",
    "section": "",
    "text": "In guiding you to develop reproducible discrete-event simulation (DES) models, this book draws from two relevant guidelines:\n\n\nRecommendations for reproducible DES from Heather et al.¬†(2025).\nA framework for reproducible analytical pipelines (RAP) from the NHS RAP Community of Practice.\n\n\nEach chapter demonstrates how to put these guidelines into practice. Use the links below (üîó) to jump straight to relevant pages for each item in the frameworks.\nThis page may be particularly helpful if you not necessarily building a healthcare DES model, but are simply interested in improving reproducibility more generally.\n\n\n\n\n\nShow/Hide Recommendations\n\nAs part of the project STARS (Sharing Tools and Artefacts for Reproducible Simulations), a series of computational reproducibility assessments were conducted by Heather et al.¬†2025. From these, several recommendations were shared to support reproducibility of healthcare discrete-event simulation (DES) models, as described in:\n\nHeather, A. Monks, T. Harper, A. Mustafee, N. Mayne, A. On the reproducibility of discrete-event simulation studies in health research: an empirical study using open models (2025). arxiv. https://doi.org/10.48550/arXiv.2501.13137.\n\nThose marked with a star (‚≠ê) were identified as having the greatest impact in Heather et al.¬†2025.\n\n\n\n\n\n\n\n\n\nRecommendation\nChapter\n\n\n\n\nSet-up\n\n\n\nShare code with an open licence ‚≠ê\n?@sec-licence\n\n\nLink publication to a specific version of the code\n?@sec-changelog\n\n\nList dependencies and versions\n?@sec-environment\n\n\nRunning the model\n\n\n\nProvide code for all scenarios and sensitivity analyses ‚≠ê\n?@sec-scenarios\n\n\nEnsure model parameters are correct ‚≠ê\n?@sec-full_run\n\n\nControl randomness\n?@sec-distributions\n\n\nOutputs\n\n\n\nInclude code to calculate all required model outputs ‚≠ê\n?@sec-outputs\n\n\nInclude code to generate the tables, figures, and other reported results ‚≠ê\n?@sec-tables_figures\n\n\n\n\n\n\n\n\n\n\n\n\n\nRecommendation\nChapter\n\n\n\n\nDesign\n\n\n\nSeparate model code from applications\n‚ùì\n\n\nAvoid hard-coded parameters\n?@sec-param_script ?@sec-param_file\n\n\nMinimise code duplication\n?@sec-code_structure\n\n\nClarity\n\n\n\nComment sufficiently\n?@sec-docstrings\n\n\nEnsure clarity and consistency in the model results tables\n?@sec-outputs\n\n\nInclude run instructions\n?@sec-documentation\n\n\nState run times and machine specifications\n?@sec-documentation\n\n\nFunctionality\n\n\n\nOptimise model run time\n?@sec-parallel\n\n\nSave outputs to a file\n?@sec-tables_figures\n\n\nAvoid excessive output files\n‚ùì\n\n\nAddress large file sizes\n‚ùì",
    "crumbs": [
      "Introduction",
      "Guidelines"
    ]
  },
  {
    "objectID": "pages/intro/guidelines.html#heather-et-al.-2025-recommendations",
    "href": "pages/intro/guidelines.html#heather-et-al.-2025-recommendations",
    "title": "Guidelines",
    "section": "",
    "text": "Show/Hide Recommendations\n\nAs part of the project STARS (Sharing Tools and Artefacts for Reproducible Simulations), a series of computational reproducibility assessments were conducted by Heather et al.¬†2025. From these, several recommendations were shared to support reproducibility of healthcare discrete-event simulation (DES) models, as described in:\n\nHeather, A. Monks, T. Harper, A. Mustafee, N. Mayne, A. On the reproducibility of discrete-event simulation studies in health research: an empirical study using open models (2025). arxiv. https://doi.org/10.48550/arXiv.2501.13137.\n\nThose marked with a star (‚≠ê) were identified as having the greatest impact in Heather et al.¬†2025.\n\n\n\n\n\n\n\n\n\nRecommendation\nChapter\n\n\n\n\nSet-up\n\n\n\nShare code with an open licence ‚≠ê\n?@sec-licence\n\n\nLink publication to a specific version of the code\n?@sec-changelog\n\n\nList dependencies and versions\n?@sec-environment\n\n\nRunning the model\n\n\n\nProvide code for all scenarios and sensitivity analyses ‚≠ê\n?@sec-scenarios\n\n\nEnsure model parameters are correct ‚≠ê\n?@sec-full_run\n\n\nControl randomness\n?@sec-distributions\n\n\nOutputs\n\n\n\nInclude code to calculate all required model outputs ‚≠ê\n?@sec-outputs\n\n\nInclude code to generate the tables, figures, and other reported results ‚≠ê\n?@sec-tables_figures\n\n\n\n\n\n\n\n\n\n\n\n\n\nRecommendation\nChapter\n\n\n\n\nDesign\n\n\n\nSeparate model code from applications\n‚ùì\n\n\nAvoid hard-coded parameters\n?@sec-param_script ?@sec-param_file\n\n\nMinimise code duplication\n?@sec-code_structure\n\n\nClarity\n\n\n\nComment sufficiently\n?@sec-docstrings\n\n\nEnsure clarity and consistency in the model results tables\n?@sec-outputs\n\n\nInclude run instructions\n?@sec-documentation\n\n\nState run times and machine specifications\n?@sec-documentation\n\n\nFunctionality\n\n\n\nOptimise model run time\n?@sec-parallel\n\n\nSave outputs to a file\n?@sec-tables_figures\n\n\nAvoid excessive output files\n‚ùì\n\n\nAddress large file sizes\n‚ùì",
    "crumbs": [
      "Introduction",
      "Guidelines"
    ]
  },
  {
    "objectID": "pages/intro/guidelines.html#nhs-levels-of-rap-maturity-framework",
    "href": "pages/intro/guidelines.html#nhs-levels-of-rap-maturity-framework",
    "title": "Guidelines",
    "section": "NHS ‚ÄòLevels of RAP‚Äô Maturity Framework",
    "text": "NHS ‚ÄòLevels of RAP‚Äô Maturity Framework\n\n\nShow/Hide Framework\n\nThe following framework has been directly copied from the RAP Community of Practice repository/website: NHS RAP Levels of RAP Framework.\nThis framework is maintained by the NHS RAP Community of Practice and is ¬© 2024 Crown Copyright (NHS England), shared by them under the terms of the Open Government 3.0 licence.\nThe specific version of the framework copied below is that from commit 2549256 (9th September 2024).\n\nü•â Baseline\nRAP fundamentals offering resilience against future change.\n\n\n\nCriteria\nChapter\n\n\n\n\nData produced by code in an open-source language (e.g., Python, R, SQL).\n?@sec-foss\n\n\nCode is version controlled (see Git basics and using Git collaboratively guides).\n?@sec-version\n\n\nRepository includes a README.md file (or equivalent) that clearly details steps a user must follow to reproduce the code (use NHS Open Source Policy section on Readmes as a guide).\n?@sec-documentation\n\n\nCode has been peer reviewed.\n?@sec-peer_review\n\n\nCode is published in the open and linked to & from accompanying publication (if relevant).\n?@sec-archive\n\n\n\n\n\nü•à Silver\nImplementing best practice by following good analytical and software engineering standards.\nMeeting all of the above requirements, plus:\n\n\n\nCriteria\nChapter\n\n\n\n\nOutputs are produced by code with minimal manual intervention.\n?@sec-full_run\n\n\nCode is well-documented including user guidance, explanation of code structure & methodology and docstrings for functions.\n?@sec-documentation ?@sec-docstrings\n\n\nCode is well-organised following standard directory format.\n?@sec-package\n\n\nReusable functions and/or classes are used where appropriate.\n?@sec-code_structure\n\n\nCode adheres to agreed coding standards (e.g PEP8, style guide for Pyspark).\n?@sec-linting\n\n\nPipeline includes a testing framework (unit tests, back tests).\n?@sec-tests\n\n\nRepository includes dependency information (e.g.¬†requirements.txt, PipFile, environment.yml).\n?@sec-environment\n\n\nLogs are automatically recorded by the pipeline to ensure outputs are as expected.\n?@sec-logs\n\n\nData is handled and output in a Tidy data format.\n?@sec-outputs\n\n\n\n\n\nü•á Gold\nAnalysis as a product to further elevate your analytical work and enhance its reusability to the public.\nMeeting all of the above requirements, plus:\n\n\n\nCriteria\nChapter\n\n\n\n\nCode is fully packaged.\n?@sec-package\n\n\nRepository automatically runs tests etc. via CI/CD or a different integration/deployment tool e.g.¬†GitHub Actions.\n?@sec-github_actions\n\n\nProcess runs based on event-based triggers (e.g., new data in database) or on a schedule.\nN/A\n\n\nChanges to the RAP are clearly signposted. E.g. a changelog in the package, releases etc. (See gov.uk info on Semantic Versioning).\n?@sec-changelog",
    "crumbs": [
      "Introduction",
      "Guidelines"
    ]
  },
  {
    "objectID": "pages/setup/version.html",
    "href": "pages/setup/version.html",
    "title": "Version control",
    "section": "",
    "text": "üîó Reproducibility guidelines:\n\nNHS Levels of RAP (ü•â): Code is version controlled.\n\n\n\n\nVersion control is a way to track changes to your documents over time. You can‚Ä¶\n\nTrack changes to files: see what changes, when it changed, and who changed it.\n‚ÄúRoll back‚Äù to earlier versions of your files if something goes wrong.\nWork simultaneously with others without overwriting each other.\nTry new ideas without risking your main work.\nBack up and share work online using platforms like GitHub.\n\nThis is super handy, whether you‚Äôre working by yourself or in a team!\n\nüí° Recommendation: Use version control from the start of the project.\n\n\n\n\n\nThe most popular version control system is Git.\nWhen using Git, we refer to our work as a repository. This is simply a folder containing your project files, as well as a special hidden .git/ folder which stores all the version history.\nWe take snapshots of the files at specific points in time, and these are called commits.\nGit can just be used on your local computer, but most people will use an online platform to store their repositories. The most popular options are:\n\nGitHub - https://github.com/.\nGitLab - https://gitlab.com/.\nGBitBucket - https://bitbucket.org/.\n\nThis tutorial will focus on GitHub as it is the most widely used and beginner-friendly.\n\n\n\n\n1. Create an account. Go to https://github.com/signup and sign up with your email and a username.\n\n\n2. Create a new repository. Click the ‚Äú+‚Äù button in the top right corner, and select ‚ÄúNew repository‚Äù.\n\n\n\n\n\nThen fill out the form to create a repository‚Ä¶\n\nRepository name: e.g.¬†emergency-des\nDescription: e.g.¬†Discrete-event simulation of a hospital emergency department.\nVisibility: Choose whether to make the repository public (anyone can see it) or private (only visible to you and people your invite).\nREADME: Select ‚ÄúAdd a README file‚Äù, which will create a blank README we can populate later (see ?@sec-documentation).\n.gitignore: Select the ‚ÄúPython‚Äù or ‚ÄúR‚Äù .gitignore template. This will create a .gitignore file in your repository that tells Git which files not to track.\nLicence: Select an appropriate licence for your repository (see ?@sec-licence for more information and advice).\n\n\n\n\n\n\n\nSharing your work openly\n\n\n\nWe would encourage you to make your work open access - i.e.¬†creating a public GitHub repository, and using a permissive open licence (e.g.¬†MIT). As described by The Turing Way, benefits to making your work open include:\n\nSharing: Easy to reference and share in papers and presentations.\nTransparency: Clearly shows how you conducted your analyses.\nReproducibility: Others can verify and reproduce your results.\nQuality: Knowing it‚Äôs public encourages good practice.\nReuse and learning: Others can learn from and build on your work, reducing research waste.\nExtends impact: Your work can continue to have impact after the project ends. This could include for you, if you change jobs, and want to be able to look back on old code!\nCollaboration: Creates opportunities for collaboration, if others come across your code and are interested in working together.\nFunder requirements: For research projects, some grants now mandate open code.\n\nIncluding a license (?@sec-licence) and citation instructions (?@sec-citation) enables others to use your code while giving you credit.\nThat said, it‚Äôs your code and your choice - if you have specific concerns or proprietary work, a private repository is always an option.\n\n\n\n\n\n\n\nYou have created a remote repository on GitHub - we now want to clone it, which means we create a local copy on your computer, and can sync between the local and remote repositories.\n\n1. Get the URL. On the main page of you repository, click the green ‚Äú&lt;&gt; Code‚Äù button, then copy the HTTPS url.\n\n\n2. Clone the repository. Depending on your operating system, open either the terminal (Linux or Mac) or Git Bash (windows). Navigate to the location where you would like to create the folder containing your repository. Then enter git clone and the pasted URL - for example:\ngit clone https://github.com/amyheather/hospital-des.git\n\nThis will have created a local copy of the repository, which you can open with your preferred development environment (e.g.¬†VSCode, RStudio).\n\n\n\n\nIt is best practice to work in branches. A branch is like a separate workspace where you can safely experiment with changes without affecting the main project.\nAs in the diagram below, you can make several commits in this branch, and when you‚Äôre ready, merge back into the main project (e.g.¬†when new feature complete, or reached a stable point where everything is working properly).\nThis is valuable even when you‚Äôre the only person working on the repository - but with multiple collaborator, becomes essential! Every person should work on their own branch.\n\nTo work in a branch and push changes‚Ä¶\n\n1. Create a branch. Open your terminal or Git Bash, and make sure your current working directory is your Git repository. To create a new branch (here, named dev), then run:\ngit branch dev\nTo move into this branch:\ngit checkout dev\nYou should see a message ‚ÄúSwitched to branch ‚Äòdev‚Äô‚Äù.\nWe can add this branch to the remote GitHub repository by running:\ngit push -u origin dev\n\n2. Make some changes. This could be any changes to code, documentation or other artefacts in the repository. So we can test this out, let‚Äôs make a simple change to our README -\nBefore:\n# hospital-des\nDiscrete-event simulation of a hospital emergency department.\nAfter:\n# Hospital DES\n\nDiscrete-event simulation of a hospital emergency department.\n\nAuthor: Amy Heather\n\nWork in progress!\n\n3. Commit the changes. To save this new version of our file to Git, we need to commit the changes. We use git add to choose which files to commit. We then write a descriptive commit message using git commit. Finally, we push the changes using git push.\ngit add README.md\ngit commit -m \"docs(README): add author + work-in-progress notice\"\ngit push\n\n4. Merge the changes. One of the easiest ways to merge changes into main is using the GitHub website. Open your repository. You should see that you now have ‚Äú2 Branches‚Äù. Navigate to your new branch‚Ä¶\n\nWe can see this is now 1 commit ahead of main. We can browse the files in this branch, and can click on the right hand ‚Äú2 Commits‚Äù to view the version history.\n\nThis just has 2 commits - the creation of our repository, and the change to our README file:\n\nTo merge the changes with main, go back to the page for that branch, and select either:\n\n‚ÄúThis branch is 1 commit ahead of main‚Äù, or\n‚ÄúCompare & pull request‚Äù\n\n\nThis will open a page which shows all the new commits in your branch, and side-by-side changes the files you have modified.\nClick the green ‚ÄúCreate pull request‚Äù button.\n\nA pull request is a request to merge changes from one branch to another. This provides an opportunity, for example, for others to review the changes and make sure they are happy before these are merged.\nWe can modify the message - or just leave as is, and select the ‚ÄúCreate pull request‚Äù button.\n\nIf new changes have been made to the main branch since you created your branch (e.g., new commits, merges from other branches, or contributions from other collaborators) and you‚Äôve modified the same files, you may encounter merge conflicts that need to be resolved.\nIf not though, you can just select ‚ÄúMerge pull request‚Äù and then ‚ÄúConfirm merge‚Äù.\n\n\n\n5. Close your branch. We can now delete the branch-\n\nOn our local machine, switch back to main:\ngit checkout main\nThen pull the updated main branch, which contains our merged changes:\ngit pull\nWe can then delete our local branch:\ngit branch -d dev\nAnd also get the latest list of branches from the remote repository, with our old branch now removed:\ngit remote prune origin\nTo continue working in the repository, simply create a new branch and continue as above.\n\n\n\n\nWhen creating a GitHub repository, by default, it will set the owner to be your personal user account. While this works well for person projects, you will often find it better to create repositories within a GitHub organisation instead.\nA GitHub organisation acts like a shared account which multiple people can own, access and collaborate on. Advantages of this are:\n\nOne place for each project/team. Related repositories are grouped into one organisation. For example, you could have different organisations for different teams or projects, with all the relevant repositories for each.\nShared ownership. Multiple people can have administrative access.\nContinuity. If you leave a project, repositories remain accessible to and owned by the team.\nProfessional branding. Repositories appear under the organisation name rather than your personal username.\n\nTo create a GitHub organisation‚Ä¶\n\n1. Open settings. In the top-right corner, select your profile photo, then click ‚ÄúSettings‚Äù.\n\n\n\n\n\nThen select ‚ÄúOrganizations‚Äù under the Access section in the sidebar.\n\n\n\n\n\n\n2. Make new organisation. In the top-right corner, select ‚ÄúNew organization‚Äù.\n\n3. Choose a plan. For example, simply select the Free plan.\n\n4. Enter your organisation details. It will ask for a name and contact email. You can typically then select that the organisation belongs to your personal account. Follow the prompts to create the organisation. You can then add other GitHub users as owners or collaborators.",
    "crumbs": [
      "Setup",
      "Version control"
    ]
  },
  {
    "objectID": "pages/setup/version.html#version-control",
    "href": "pages/setup/version.html#version-control",
    "title": "Version control",
    "section": "",
    "text": "Version control is a way to track changes to your documents over time. You can‚Ä¶\n\nTrack changes to files: see what changes, when it changed, and who changed it.\n‚ÄúRoll back‚Äù to earlier versions of your files if something goes wrong.\nWork simultaneously with others without overwriting each other.\nTry new ideas without risking your main work.\nBack up and share work online using platforms like GitHub.\n\nThis is super handy, whether you‚Äôre working by yourself or in a team!\n\nüí° Recommendation: Use version control from the start of the project.",
    "crumbs": [
      "Setup",
      "Version control"
    ]
  },
  {
    "objectID": "pages/setup/version.html#git",
    "href": "pages/setup/version.html#git",
    "title": "Version control",
    "section": "",
    "text": "The most popular version control system is Git.\nWhen using Git, we refer to our work as a repository. This is simply a folder containing your project files, as well as a special hidden .git/ folder which stores all the version history.\nWe take snapshots of the files at specific points in time, and these are called commits.\nGit can just be used on your local computer, but most people will use an online platform to store their repositories. The most popular options are:\n\nGitHub - https://github.com/.\nGitLab - https://gitlab.com/.\nGBitBucket - https://bitbucket.org/.\n\nThis tutorial will focus on GitHub as it is the most widely used and beginner-friendly.",
    "crumbs": [
      "Setup",
      "Version control"
    ]
  },
  {
    "objectID": "pages/setup/version.html#setting-up-a-github-repository",
    "href": "pages/setup/version.html#setting-up-a-github-repository",
    "title": "Version control",
    "section": "",
    "text": "1. Create an account. Go to https://github.com/signup and sign up with your email and a username.\n\n\n2. Create a new repository. Click the ‚Äú+‚Äù button in the top right corner, and select ‚ÄúNew repository‚Äù.\n\n\n\n\n\nThen fill out the form to create a repository‚Ä¶\n\nRepository name: e.g.¬†emergency-des\nDescription: e.g.¬†Discrete-event simulation of a hospital emergency department.\nVisibility: Choose whether to make the repository public (anyone can see it) or private (only visible to you and people your invite).\nREADME: Select ‚ÄúAdd a README file‚Äù, which will create a blank README we can populate later (see ?@sec-documentation).\n.gitignore: Select the ‚ÄúPython‚Äù or ‚ÄúR‚Äù .gitignore template. This will create a .gitignore file in your repository that tells Git which files not to track.\nLicence: Select an appropriate licence for your repository (see ?@sec-licence for more information and advice).\n\n\n\n\n\n\n\nSharing your work openly\n\n\n\nWe would encourage you to make your work open access - i.e.¬†creating a public GitHub repository, and using a permissive open licence (e.g.¬†MIT). As described by The Turing Way, benefits to making your work open include:\n\nSharing: Easy to reference and share in papers and presentations.\nTransparency: Clearly shows how you conducted your analyses.\nReproducibility: Others can verify and reproduce your results.\nQuality: Knowing it‚Äôs public encourages good practice.\nReuse and learning: Others can learn from and build on your work, reducing research waste.\nExtends impact: Your work can continue to have impact after the project ends. This could include for you, if you change jobs, and want to be able to look back on old code!\nCollaboration: Creates opportunities for collaboration, if others come across your code and are interested in working together.\nFunder requirements: For research projects, some grants now mandate open code.\n\nIncluding a license (?@sec-licence) and citation instructions (?@sec-citation) enables others to use your code while giving you credit.\nThat said, it‚Äôs your code and your choice - if you have specific concerns or proprietary work, a private repository is always an option.",
    "crumbs": [
      "Setup",
      "Version control"
    ]
  },
  {
    "objectID": "pages/setup/version.html#cloning-the-repository",
    "href": "pages/setup/version.html#cloning-the-repository",
    "title": "Version control",
    "section": "",
    "text": "You have created a remote repository on GitHub - we now want to clone it, which means we create a local copy on your computer, and can sync between the local and remote repositories.\n\n1. Get the URL. On the main page of you repository, click the green ‚Äú&lt;&gt; Code‚Äù button, then copy the HTTPS url.\n\n\n2. Clone the repository. Depending on your operating system, open either the terminal (Linux or Mac) or Git Bash (windows). Navigate to the location where you would like to create the folder containing your repository. Then enter git clone and the pasted URL - for example:\ngit clone https://github.com/amyheather/hospital-des.git\n\nThis will have created a local copy of the repository, which you can open with your preferred development environment (e.g.¬†VSCode, RStudio).",
    "crumbs": [
      "Setup",
      "Version control"
    ]
  },
  {
    "objectID": "pages/setup/version.html#creating-a-branch-and-pushing-changes",
    "href": "pages/setup/version.html#creating-a-branch-and-pushing-changes",
    "title": "Version control",
    "section": "",
    "text": "It is best practice to work in branches. A branch is like a separate workspace where you can safely experiment with changes without affecting the main project.\nAs in the diagram below, you can make several commits in this branch, and when you‚Äôre ready, merge back into the main project (e.g.¬†when new feature complete, or reached a stable point where everything is working properly).\nThis is valuable even when you‚Äôre the only person working on the repository - but with multiple collaborator, becomes essential! Every person should work on their own branch.\n\nTo work in a branch and push changes‚Ä¶\n\n1. Create a branch. Open your terminal or Git Bash, and make sure your current working directory is your Git repository. To create a new branch (here, named dev), then run:\ngit branch dev\nTo move into this branch:\ngit checkout dev\nYou should see a message ‚ÄúSwitched to branch ‚Äòdev‚Äô‚Äù.\nWe can add this branch to the remote GitHub repository by running:\ngit push -u origin dev\n\n2. Make some changes. This could be any changes to code, documentation or other artefacts in the repository. So we can test this out, let‚Äôs make a simple change to our README -\nBefore:\n# hospital-des\nDiscrete-event simulation of a hospital emergency department.\nAfter:\n# Hospital DES\n\nDiscrete-event simulation of a hospital emergency department.\n\nAuthor: Amy Heather\n\nWork in progress!\n\n3. Commit the changes. To save this new version of our file to Git, we need to commit the changes. We use git add to choose which files to commit. We then write a descriptive commit message using git commit. Finally, we push the changes using git push.\ngit add README.md\ngit commit -m \"docs(README): add author + work-in-progress notice\"\ngit push\n\n4. Merge the changes. One of the easiest ways to merge changes into main is using the GitHub website. Open your repository. You should see that you now have ‚Äú2 Branches‚Äù. Navigate to your new branch‚Ä¶\n\nWe can see this is now 1 commit ahead of main. We can browse the files in this branch, and can click on the right hand ‚Äú2 Commits‚Äù to view the version history.\n\nThis just has 2 commits - the creation of our repository, and the change to our README file:\n\nTo merge the changes with main, go back to the page for that branch, and select either:\n\n‚ÄúThis branch is 1 commit ahead of main‚Äù, or\n‚ÄúCompare & pull request‚Äù\n\n\nThis will open a page which shows all the new commits in your branch, and side-by-side changes the files you have modified.\nClick the green ‚ÄúCreate pull request‚Äù button.\n\nA pull request is a request to merge changes from one branch to another. This provides an opportunity, for example, for others to review the changes and make sure they are happy before these are merged.\nWe can modify the message - or just leave as is, and select the ‚ÄúCreate pull request‚Äù button.\n\nIf new changes have been made to the main branch since you created your branch (e.g., new commits, merges from other branches, or contributions from other collaborators) and you‚Äôve modified the same files, you may encounter merge conflicts that need to be resolved.\nIf not though, you can just select ‚ÄúMerge pull request‚Äù and then ‚ÄúConfirm merge‚Äù.\n\n\n\n5. Close your branch. We can now delete the branch-\n\nOn our local machine, switch back to main:\ngit checkout main\nThen pull the updated main branch, which contains our merged changes:\ngit pull\nWe can then delete our local branch:\ngit branch -d dev\nAnd also get the latest list of branches from the remote repository, with our old branch now removed:\ngit remote prune origin\nTo continue working in the repository, simply create a new branch and continue as above.",
    "crumbs": [
      "Setup",
      "Version control"
    ]
  },
  {
    "objectID": "pages/setup/version.html#github-organisations",
    "href": "pages/setup/version.html#github-organisations",
    "title": "Version control",
    "section": "",
    "text": "When creating a GitHub repository, by default, it will set the owner to be your personal user account. While this works well for person projects, you will often find it better to create repositories within a GitHub organisation instead.\nA GitHub organisation acts like a shared account which multiple people can own, access and collaborate on. Advantages of this are:\n\nOne place for each project/team. Related repositories are grouped into one organisation. For example, you could have different organisations for different teams or projects, with all the relevant repositories for each.\nShared ownership. Multiple people can have administrative access.\nContinuity. If you leave a project, repositories remain accessible to and owned by the team.\nProfessional branding. Repositories appear under the organisation name rather than your personal username.\n\nTo create a GitHub organisation‚Ä¶\n\n1. Open settings. In the top-right corner, select your profile photo, then click ‚ÄúSettings‚Äù.\n\n\n\n\n\nThen select ‚ÄúOrganizations‚Äù under the Access section in the sidebar.\n\n\n\n\n\n\n2. Make new organisation. In the top-right corner, select ‚ÄúNew organization‚Äù.\n\n3. Choose a plan. For example, simply select the Free plan.\n\n4. Enter your organisation details. It will ask for a name and contact email. You can typically then select that the organisation belongs to your personal account. Follow the prompts to create the organisation. You can then add other GitHub users as owners or collaborators.",
    "crumbs": [
      "Setup",
      "Version control"
    ]
  },
  {
    "objectID": "pages/setup/package.html",
    "href": "pages/setup/package.html",
    "title": "Structuring as a package",
    "section": "",
    "text": "üîó Reproducibility guidelines:\n\nNHS Levels of RAP (ü•à): Code is well-organised following standard directory format. &gt; * NHS Levels of RAP (ü•á): Code is fully packaged.\n\n\nBuilding our simulation model as a package has several advantages‚Ä¶\n\nThe model is installed in our environment and can then be easily used anywhere else in our directory (or even from other directories) without needing to specify a system path.\nIt encourages us to create a well-organised repository following standardised established package structures.\nIt helps keep the model and analysis code separate, improving maintainability, reusability, and reducing the risk of unintended side effects.\nIt supports automated testing frameworks which can verify functionality (?@sec-tests).\n\n\n\n\n\n\nFirst, let‚Äôs create the basic directory structure for our simulation package.\n\n1. Create folder. Create a folder called simulation/ in your project directory.\nmkdir simulation\n\n2. Make init file. Inside the simulation/ folder, create an __init__.py file.\ntouch simulation/__init__.py\nOpen this file and copy in some basic metadata.\n\"\"\"SimPy Discrete-Event Simulation (DES) Model.\n\"\"\"\n\n__version__ = \"0.1.0\"\n\n3. Add a .py file with a function. Within simulation/, create a file called model.py.\ntouch simulation/model.py\nIn this, we will add our model code. For now, just copy in this simple function that generates a list of numbers. We will add some real code for our model later.\n\"\"\"Core simulation model functionality.\"\"\"\n\n\ndef run_simulation(duration=100):\n    \"\"\"\n    Run a simple dummy simulation for the specified duration.\n\n    Parameters\n    ----------\n    duration: int\n        The length of time to run the simulation.\n\n    Returns:\n        dict:\n            Dummy simulation results.\n    \"\"\"\n    return {\n        \"duration\": duration,\n        \"status\": \"completed\",\n        \"results\": [i for i in range(duration) if i % 10 == 0]\n    }\n\n4. Make pyproject file. Create a pyproject.toml file in the project root directory (outside the simulation folder).\ntouch pyproject.toml\nCopy the text below into this file. This provides instructions for building the package. We‚Äôre using flit as our build tool because of its simplicity. The dynamic field tells flit to extract the version and description from our __init__.py file.\n[build-system]\nrequires = [\"flit\"]\nbuild-backend = \"flit_core.buildapi\"\n\n[project]\nname = \"simulation\"\ndynamic = [\"version\", \"description\"]\n\nOur directory is now recognisable as a package - it contains an __init__.py file, and there is information about the package in pyproject.toml.\nYour directory should now look like this:\nour_directory/\n‚îú‚îÄ‚îÄ simulation/             # Main package directory\n‚îÇ   ‚îú‚îÄ‚îÄ __init__.py         # Makes the directory a package\n‚îÇ   ‚îî‚îÄ‚îÄ model.py            # Our function\n‚îú‚îÄ‚îÄ .gitignore\n‚îú‚îÄ‚îÄ environment.yaml\n‚îú‚îÄ‚îÄ LICENSE\n‚îú‚îÄ‚îÄ pyproject.toml          # Package metadata\n‚îî‚îÄ‚îÄ README.md\n\n\n\n\n\n1. Add package to environment file. Edit the environment.yaml file created in ?@sec-environment to add the local package. We use the -e flag to install it in ‚Äúeditable‚Äù mode, which means it will update with any changes to the source code in simulation/.\nWe add the following line within the pip installs:\n  - pip:\n    - -e .[dev]\nSo our full environment file is now:\nname: des-example\nchannels:\n  - conda-forge\ndependencies:\n  - ipykernel=6.29.5\n  - jinja2=3.1.5\n  - joblib=1.4.2\n  - nbconvert=7.16.6\n  - nbformat=5.10.4\n  - nbqa=1.9.0\n  - numpy=2.2.2\n  - pandas=2.2.3\n  - pip=25.0\n  - plotly_express=0.4.1\n  - pylint=3.3.4\n  - pytest=8.3.4\n  - pytest-xdist=3.6.1\n  - python=3.13.1\n  - rich=13.9.4\n  - simpy=4.1.1\n  - pip:\n    - kaleido==0.2.1\n    - sim-tools==0.8.0\n    - -e .[dev]\n\n2. Update the environment. To update our environment, adding our new package, execute the following in the command line (after running conda activate des-example):\nconda env update --file environment.yaml --prune\nIf you run conda list, you should now see our simulation package listed as a dependency like so:\n# Name                    Version                   Build  Channel\nsimulation                0.1.0                    pypi_0    pypi\n\n\n\n\nWe will run our model and analyse results within Jupyter notebooks, as they allow us to combine code, results and explanations in one document. This can help make our methods and results easier to share, read through and understand.\n\n1. Create directory and notebook. We create a dedicated directory for our notebooks in the project root (notebooks/), and add a notebook (simulation_test.ipynb). These can be created via your operating system‚Äôs file manager, or with the command line:\nmkdir notebooks\ntouch notebooks/simulation_test.ipynb\n\n2. Check our package works. Open notebooks/simulation_test.ipynb and add the following code. This will test that we are able to import our package and use the basic function we had created.\n# Import our simulation package\nfrom simulation.model import run_simulation\n\n# Run the simulation with default parameters\nresults = run_simulation()\nprint(f\"Simulation completed with duration: {results['duration']}\")\nprint(f\"Results: {results['results']}\")\n\n# Run with a different duration\nlong_results = run_simulation(duration=200)\nprint(f\"Longer simulation completed with duration: {long_results['duration']}\")\nprint(f\"Number of results: {len(long_results['results'])}\")\nWhen you run the cell, you should see an output confirming that the simulation function runs and returns results, if everything is set up correctly.\nSimulation completed with duration: 100\nResults: [0, 10, 20, 30, 40, 50, 60, 70, 80, 90]\nLonger simulation completed with duration: 200\nNumber of results: 20\n\n\n\n\nTo structure your simulation project as a python package:\n\nPlace all core python functions in the simulation/ directory (e.g.¬†model.py).\nCreate simulation/__init__.py and pyproject.toml to set-up as a package.\nOrganise analysis into a seperate directory (e.g.¬†notebooks/).\n\nFor example:\nour_directory/\n‚îú‚îÄ‚îÄ simulation/             # Main package directory\n‚îÇ   ‚îú‚îÄ‚îÄ __init__.py         # Makes the directory a package\n‚îÇ   ‚îî‚îÄ‚îÄ model.py            # Our function\n‚îú‚îÄ‚îÄ .gitignore\n‚îú‚îÄ‚îÄ environment.yaml\n‚îú‚îÄ‚îÄ LICENSE\n‚îú‚îÄ‚îÄ pyproject.toml          # Package metadata\n‚îî‚îÄ‚îÄ README.md\n\n\n\n\n\n‚ÄúHow I use Python to organize my data analyses‚Äù from Joshua Cook 2024\n\n\n\n\n\n\nFor this section, you will need roxygen2, usethis and devtools. If you used the environment provided in ?@sec-environment then you should have these already - but if you do not, make sure to install them! You can check if they are in your environment by running this command from the R console:\npackageVersion(\"devtools\")\npackageVersion(\"usethis\")\npackageVersion(\"roxygen2\")\nIf will print the version number of each package if installed.\n\n\n\nFirst, let‚Äôs create the basic directory structure for our simulation package.\n\n1. Create folder Create a folder called R/ in your project directory - for example, by running this command in the terminal:\nmkdir R\n\n2. Add a .R file with a function. Within R/, create a file called model.R.\ntouch R/model.R\nIn this file, we will add our model code. For now, just copy in this simple function that generates a list of numbers. We will add some real code for our model later.\n# Core simulation model functionality\n\nrun_simulation &lt;- function(duration = 100) {\n  #' Run a simple dummy simulation for the specified duration.\n  #'\n  #' @param duration int. The length of time to run the simulation.\n  #' @return list. Dummy simulation results.\n  \n  results &lt;- seq(0, duration - 1)\n  results &lt;- results[results %% 10 == 0]\n  \n  return(list(\n    duration = duration,\n    status = \"completed\",\n    results = results\n  ))\n}\n\n3. Make a DESCRIPTION file. If you have worked through the ?@sec-environment page, then you should already have a DESCRIPTION file listing dependencies, though this step will walk you through more of the meta-data in that file important when structuring our work as a package.\nIf you haven‚Äôt already, then create a DESCRIPTION file by running this command in the terminal:\ntouch DESCRIPTION\nOpen the file and copy in the template below. This is similar to the standard template generated by usethis::use_description(), but with a few extra sections.\nPackage: packagename\nType: Package\nTitle: What the Package Does (One Line, Title Case)\nVersion: 0.0.0.9000\nURL: ...\nAuthors@R: \n    person(\"First\", \"Last\", , \"first.last@example.com\", role = c(\"aut\", \"cre\"))\nDescription: What the package does (one paragraph).\nLicense: `use_mit_license()`, `use_gpl3_license()` or friends to pick a\n    license\nEncoding: UTF-8\nRoxygen: list(markdown = TRUE)\nRoxygenNote: 7.0.0\nImports:\n    ...\nSuggests:\n    ...\nWe will then fill in the template with relevant information for our project. You don‚Äôt need to change Type, Encoding, or Roxygen. For the other arguments:\n\nPackage: When using devtools to work with our package (as below), it will prompt you to use a name that complies with CRAN (the main R package repository). They require that the name is only made up of letters, numbers and periods (.) - and that it must start with a letter and cannot end with a period. When structuring our research project as a package, this is not often with the aim of uploading it to CRAN, but it can be simple/good practice to follow these guidelines anyway, and means you avoid devtools error messages! \nTitle: Capitalised single line description of the package which does not end with a period (.).\nVersion: The package version. For R packages, this is usually set to 0.0.0.9000 during early development - though some developers prefer to set it to 0.1.0, as we have done. The version number is used to track changes to the package over time. It typically follows semantic versioning, with three numbers representing major, minor and patch changes. For more about how and when to update the version, see the ?@sec-changelog page.\nAuthors: List of author names, emails and roles. The main role options are the current maintainer (creator, cre), people who have made significant contributions (author, aut), those who have made smaller contributions (contributor, ctb), copyright holders (cph) and funders (fnd). You can add additional information using the comment argument, like your ORCID ID.\nURL: Link to your repository. If you don‚Äôt have one, we‚Äôd strongly recommend making one - check out the ?@sec-version page.\nDescription: Single paragraph describing project.\nLicense: A license tells others how they can use your code. The usethis package makes it easy to add a license: just call the function for your chosen license, for example:\nusethis::use_mit_license()\nThis will update the License field in DESCRIPTION and create both LICENSE (with the year and copyright holder) and LICENSE.md (with the full licence text). Note: it will prompt you to overwrite any existing licence files.\nR packages use this two-file structure, while GitHub typically expects a single LICENSE file containing the full text. Unless you plan to submit to CRAN - which requires the R package structure - either approach is fine. For simplicity, we recommend sticking with the standard R package setup using usethis, and agreeing if prompted to overwrite old license files.\nFor more information, see the ?@sec-licence page in this book, and the R Packages book.\nRoxygenNote: roxygen2 is used when documenting code. Update this to the version of roxygen2 which you have installed - to check, run:\npackageVersion(\"roxygen2\")\nImports: These are packages necessary for your package. In other words, if it‚Äôs used by code in R/, then list it here.\nSuggests: These are any other packages needed. For example, you might include those for development (devtools), testing (testthat), linting (lintr) - or packages used in your analysis (i.e.¬†any code not in R/).\n\nAs an example:\nPackage: simulation\nType: Package\nTitle: Simulation\nVersion: 0.1.0\nAuthors@R: c(\n    person(\n      \"Amy\", \"Heather\",\n      email = \"a.heather2@exeter.ac.uk\",\n      role = c(\"aut\", \"cre\"),\n      comment = c(ORCID = \"0000-0002-6983-2759\")\n    )\n  )\nURL: https://github.com/pythonhealthdatascience/rap_template_r_des\nDescription: Template reproducible analytical pipeline (RAP) for simple R\n    discrete-event simulation (DES) model.\nLicense: MIT + file LICENSE\nEncoding: UTF-8\nLazyData: true\nRoxygenNote: 7.3.2\nImports:\n    simmer,\n    magrittr,\n    dplyr,\n    purrr,\n    rlang,\n    tidyr,\n    tidyselect,\n    future,\n    future.apply,\n    ggplot2,\n    tibble,\n    gridExtra,\n    R6\nSuggests:\n    testthat (&gt;= 3.0.0),\n    patrick,\n    lintr,\n    devtools,\n    xtable,\n    data.table,\n    mockery\nConfig/testthat/edition: 3\n\n\n\n\n1. Build package documentation. The function we created in model.R had a docstring (for more info on writing docstrings, see ?@sec-docstrings). We can create the documentation for this by calling:\ndevtools::document()\nThis will create:\n\nman/: folder with roxygen2 documentation for each function in package.\nNAMESPACE: file which will list all the functions and packages used within your package.\n\n\n2. Check the package. You can check that the package is set-up correctly by running:\ndevtools::check()\nThis will load it and perform standard checks. If all is well, you should get an output similar to:\n&gt; devtools::check()\n‚ïê‚ïê Documenting ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n‚Ñπ Updating simulation documentation\n‚Ñπ Loading simulation\nWriting NAMESPACE\nWriting run_simulation.Rd\n\n‚ïê‚ïê Building ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nSetting env vars:\n‚Ä¢ CFLAGS    : -Wall -pedantic -fdiagnostics-color=always\n‚Ä¢ CXXFLAGS  : -Wall -pedantic -fdiagnostics-color=always\n‚Ä¢ CXX11FLAGS: -Wall -pedantic -fdiagnostics-color=always\n‚Ä¢ CXX14FLAGS: -Wall -pedantic -fdiagnostics-color=always\n‚Ä¢ CXX17FLAGS: -Wall -pedantic -fdiagnostics-color=always\n‚Ä¢ CXX20FLAGS: -Wall -pedantic -fdiagnostics-color=always\n‚îÄ‚îÄ R CMD build ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n‚úî  checking for file ‚Äò/home/amy/Documents/stars/hospital-des-r/DESCRIPTION‚Äô ...\n‚îÄ  preparing ‚Äòsimulation‚Äô:\n‚úî  checking DESCRIPTION meta-information ...\n‚îÄ  checking for LF line-endings in source and make files and shell scripts\n‚îÄ  checking for empty or unneeded directories\n‚îÄ  building ‚Äòsimulation_0.1.0.tar.gz‚Äô\n   \n‚ïê‚ïê Checking ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nSetting env vars:\n‚Ä¢ _R_CHECK_CRAN_INCOMING_USE_ASPELL_           : TRUE\n‚Ä¢ _R_CHECK_CRAN_INCOMING_REMOTE_               : FALSE\n‚Ä¢ _R_CHECK_CRAN_INCOMING_                      : FALSE\n‚Ä¢ _R_CHECK_FORCE_SUGGESTS_                     : FALSE\n‚Ä¢ _R_CHECK_PACKAGES_USED_IGNORE_UNUSED_IMPORTS_: FALSE\n‚Ä¢ NOT_CRAN                                     : true\n‚îÄ‚îÄ R CMD check ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n‚îÄ  using log directory ‚Äò/tmp/RtmpyQepIc/file4b07699e20de/simulation.Rcheck‚Äô\n‚îÄ  using R version 4.4.1 (2024-06-14)\n‚îÄ  using platform: x86_64-pc-linux-gnu\n‚îÄ  R was compiled by\n       gcc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n       GNU Fortran (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n‚îÄ  running under: Ubuntu 24.04.2 LTS\n‚îÄ  using session charset: UTF-8\n‚îÄ  using options ‚Äò--no-manual --as-cran‚Äô\n‚úî  checking for file ‚Äòsimulation/DESCRIPTION‚Äô\n‚îÄ  this is package ‚Äòsimulation‚Äô version ‚Äò0.1.0‚Äô\n‚îÄ  package encoding: UTF-8\n‚úî  checking package namespace information\n‚úî  checking package dependencies (1.4s)\n‚úî  checking if this is a source package ...\n‚úî  checking if there is a namespace\n‚úî  checking for executable files\n‚úî  checking for hidden files and directories\n‚úî  checking for portable file names\n‚úî  checking for sufficient/correct file permissions\n‚úî  checking serialization versions\n‚úî  checking whether package ‚Äòsimulation‚Äô can be installed (771ms)\n‚úî  checking installed package size ...\n‚úî  checking package directory\n‚úî  checking for future file timestamps\n‚úî  checking DESCRIPTION meta-information ...\n‚úî  checking top-level files\n‚úî  checking for left-over files\n‚úî  checking index information\n‚úî  checking package subdirectories ...\n‚úî  checking code files for non-ASCII characters ...\n‚úî  checking R files for syntax errors ...\n‚úî  checking whether the package can be loaded ...\n‚úî  checking whether the package can be loaded with stated dependencies ...\n‚úî  checking whether the package can be unloaded cleanly ...\n‚úî  checking whether the namespace can be loaded with stated dependencies ...\n‚úî  checking whether the namespace can be unloaded cleanly ...\n‚úî  checking loading without being on the library search path ...\nN  checking dependencies in R code ...\n   Namespaces in Imports field not imported from:\n     ‚ÄòR6‚Äô ‚Äòdplyr‚Äô ‚Äòfuture‚Äô ‚Äòfuture.apply‚Äô ‚Äòggplot2‚Äô ‚ÄògridExtra‚Äô ‚Äòmagrittr‚Äô\n     ‚Äòpurrr‚Äô ‚Äòrlang‚Äô ‚Äòsimmer‚Äô ‚Äòtibble‚Äô ‚Äòtidyr‚Äô ‚Äòtidyselect‚Äô\n     All declared Imports should be used.\n‚úî  checking S3 generic/method consistency ...\n‚úî  checking replacement functions ...\n‚úî  checking foreign function calls ...\n‚úî  checking R code for possible problems (1.5s)\n‚úî  checking Rd files ...\n‚úî  checking Rd metadata ...\n‚úî  checking Rd line widths ...\n‚úî  checking Rd cross-references ...\n‚úî  checking for missing documentation entries ...\n‚úî  checking for code/documentation mismatches (343ms)\n‚úî  checking Rd \\usage sections ...\n‚úî  checking Rd contents ...\n‚úî  checking for unstated dependencies in examples ...\n‚îÄ  checking examples ... NONE\n‚úî  checking for non-standard things in the check directory\n‚úî  checking for detritus in the temp directory\n   \n   See\n     ‚Äò/tmp/RtmpyQepIc/file4b07699e20de/simulation.Rcheck/00check.log‚Äô\n   for details.\n   \n‚îÄ‚îÄ R CMD check results ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ simulation 0.1.0 ‚îÄ‚îÄ‚îÄ‚îÄ\nDuration: 7.3s\n\n‚ùØ checking dependencies in R code ... NOTE\n  Namespaces in Imports field not imported from:\n    ‚ÄòR6‚Äô ‚Äòdplyr‚Äô ‚Äòfuture‚Äô ‚Äòfuture.apply‚Äô ‚Äòggplot2‚Äô ‚ÄògridExtra‚Äô ‚Äòmagrittr‚Äô\n    ‚Äòpurrr‚Äô ‚Äòrlang‚Äô ‚Äòsimmer‚Äô ‚Äòtibble‚Äô ‚Äòtidyr‚Äô ‚Äòtidyselect‚Äô\n    All declared Imports should be used.\n\n0 errors ‚úî | 0 warnings ‚úî | 1 note ‚úñ\nThe note in this case is because we are importing packages not currently used in R/ - but won‚Äôt worry about this for now, as will do later when build our simulation model.\n\n3. Install the package. We can also verify that our package is working by trying to install it. From the console, run:\ndevtools::install()\n\n\n\n\nWe will run our model and analyse results within R markdown (.Rmd) files, as they combine code, results and explanations in one document. This can help make our methods and results easier to share, read through and understand.\n\n1. Create directory and R markdown file. We create a dedicated directory for our R markdown files in the project root (rmarkdown/), and add a file (simulation_test.Rmd). These can be created via the terminal by running:\nmkdir rmarkdown\ntouch rmarkdown/simulation_test.Rmd\nAs part of an R package, you can create a vignettes folder, which is typically used to hold guides, tutorials, or extended examples demonstrating how to use the package. Some people suggest putting your analysis in vignettes/, as this ensures everything is run from scratch each time you call devtools::check(), helping maintain reproducibility.\nHowever, we usually prefer to use an rmarkdown/ folder when structuring research as a package because running analyses as vignettes is very inefficient for simulations with longer run times, and saving outputs from vignettes can lead to file path errors during the package build process.\n\n2. Check our package works. Open rmarkdown/simulation_test.Rmd and add the following code into a cell. This means you put ```{r} in the line before the code, and ``` in the line after.\nThis code will test that we are able to import our package and use the basic function we had created.\n# Load the package from the local directory\ndevtools::load_all()\n\n# Load the package\nlibrary(simulation)\n\n# Run the simulation with default parameters\nresults &lt;- run_simulation()\ncat(sprintf(\"Simulation completed with duration: %s\\n\", results$duration))\ncat(sprintf(\"Results: %s\\n\", toString(results$results)))\n\n# Run with a different duration\nlong_results &lt;- run_simulation(duration = 200)\ncat(sprintf(\"Longer simulation completed with duration: %s\\n\", long_results$duration))\ncat(sprintf(\"Number of results: %d\\n\", length(long_results$results)))\nWhen you run the cell, you should see an output confirming that the simulation function runs and returns results, if everything is set up correctly.\n‚Ñπ Loading simulation\nSimulation completed with duration: 100\nResults: 0, 10, 20, 30, 40, 50, 60, 70, 80, 90\nLonger simulation completed with duration: 200\nNumber of results: 20\n\n\n\n\nTo structure your simulation project as an R package:\n\nPlace all core R functions in the R/ directory (e.g.¬†model.R).\nCreate a DESCRIPTION file to define package metadata.\nUse devtools and roxygen2 to generate documentation (man/, NAMESPACE) and check your package.\nOrganise analysis into a seperate directory (e.g.¬†rmarkdown/).\n\nFor example:\nour_directory/\n‚îú‚îÄ‚îÄ R/                  # Directory containing R functions\n‚îÇ   ‚îú‚îÄ‚îÄ model.R         # Code (e.g. for the model)\n‚îÇ   ‚îî‚îÄ‚îÄ ...             # Other .R files\n‚îú‚îÄ‚îÄ man/                # Documentation (generated from roxygen comments)\n‚îú‚îÄ‚îÄ DESCRIPTION         # Package metadata\n‚îú‚îÄ‚îÄ NAMESPACE           # Package exports (usually generated)\n‚îî‚îÄ‚îÄ rmarkdown/          # Analysis notebooks\n\n\n\n\n‚ÄúOpen, Reproducible, and Distributable Research With R Packages‚Äù from the DANTE Project - for example, this page on vignettes.\n‚ÄúSharing and organizing research products as R packages‚Äù from Vuorre and Crump 2020\n‚Äú4 Fundamental development workflows‚Äù from the ‚ÄúR Packages‚Äù book by Hadley Wickham and Jennifer Bryan\n‚Äú9 DESCRIPTION‚Äù from the ‚ÄúR Packages‚Äù book by Hadley Wickham and Jennifer Bryan",
    "crumbs": [
      "Setup",
      "Structuring as a package"
    ]
  },
  {
    "objectID": "pages/setup/package.html#create-the-python-package-structure",
    "href": "pages/setup/package.html#create-the-python-package-structure",
    "title": "Structuring as a package",
    "section": "",
    "text": "First, let‚Äôs create the basic directory structure for our simulation package.\n\n1. Create folder. Create a folder called simulation/ in your project directory.\nmkdir simulation\n\n2. Make init file. Inside the simulation/ folder, create an __init__.py file.\ntouch simulation/__init__.py\nOpen this file and copy in some basic metadata.\n\"\"\"SimPy Discrete-Event Simulation (DES) Model.\n\"\"\"\n\n__version__ = \"0.1.0\"\n\n3. Add a .py file with a function. Within simulation/, create a file called model.py.\ntouch simulation/model.py\nIn this, we will add our model code. For now, just copy in this simple function that generates a list of numbers. We will add some real code for our model later.\n\"\"\"Core simulation model functionality.\"\"\"\n\n\ndef run_simulation(duration=100):\n    \"\"\"\n    Run a simple dummy simulation for the specified duration.\n\n    Parameters\n    ----------\n    duration: int\n        The length of time to run the simulation.\n\n    Returns:\n        dict:\n            Dummy simulation results.\n    \"\"\"\n    return {\n        \"duration\": duration,\n        \"status\": \"completed\",\n        \"results\": [i for i in range(duration) if i % 10 == 0]\n    }\n\n4. Make pyproject file. Create a pyproject.toml file in the project root directory (outside the simulation folder).\ntouch pyproject.toml\nCopy the text below into this file. This provides instructions for building the package. We‚Äôre using flit as our build tool because of its simplicity. The dynamic field tells flit to extract the version and description from our __init__.py file.\n[build-system]\nrequires = [\"flit\"]\nbuild-backend = \"flit_core.buildapi\"\n\n[project]\nname = \"simulation\"\ndynamic = [\"version\", \"description\"]\n\nOur directory is now recognisable as a package - it contains an __init__.py file, and there is information about the package in pyproject.toml.\nYour directory should now look like this:\nour_directory/\n‚îú‚îÄ‚îÄ simulation/             # Main package directory\n‚îÇ   ‚îú‚îÄ‚îÄ __init__.py         # Makes the directory a package\n‚îÇ   ‚îî‚îÄ‚îÄ model.py            # Our function\n‚îú‚îÄ‚îÄ .gitignore\n‚îú‚îÄ‚îÄ environment.yaml\n‚îú‚îÄ‚îÄ LICENSE\n‚îú‚îÄ‚îÄ pyproject.toml          # Package metadata\n‚îî‚îÄ‚îÄ README.md",
    "crumbs": [
      "Setup",
      "Structuring as a package"
    ]
  },
  {
    "objectID": "pages/setup/package.html#update-the-environment",
    "href": "pages/setup/package.html#update-the-environment",
    "title": "Structuring as a package",
    "section": "",
    "text": "1. Add package to environment file. Edit the environment.yaml file created in ?@sec-environment to add the local package. We use the -e flag to install it in ‚Äúeditable‚Äù mode, which means it will update with any changes to the source code in simulation/.\nWe add the following line within the pip installs:\n  - pip:\n    - -e .[dev]\nSo our full environment file is now:\nname: des-example\nchannels:\n  - conda-forge\ndependencies:\n  - ipykernel=6.29.5\n  - jinja2=3.1.5\n  - joblib=1.4.2\n  - nbconvert=7.16.6\n  - nbformat=5.10.4\n  - nbqa=1.9.0\n  - numpy=2.2.2\n  - pandas=2.2.3\n  - pip=25.0\n  - plotly_express=0.4.1\n  - pylint=3.3.4\n  - pytest=8.3.4\n  - pytest-xdist=3.6.1\n  - python=3.13.1\n  - rich=13.9.4\n  - simpy=4.1.1\n  - pip:\n    - kaleido==0.2.1\n    - sim-tools==0.8.0\n    - -e .[dev]\n\n2. Update the environment. To update our environment, adding our new package, execute the following in the command line (after running conda activate des-example):\nconda env update --file environment.yaml --prune\nIf you run conda list, you should now see our simulation package listed as a dependency like so:\n# Name                    Version                   Build  Channel\nsimulation                0.1.0                    pypi_0    pypi",
    "crumbs": [
      "Setup",
      "Structuring as a package"
    ]
  },
  {
    "objectID": "pages/setup/package.html#create-a-notebook-to-test-your-package",
    "href": "pages/setup/package.html#create-a-notebook-to-test-your-package",
    "title": "Structuring as a package",
    "section": "",
    "text": "We will run our model and analyse results within Jupyter notebooks, as they allow us to combine code, results and explanations in one document. This can help make our methods and results easier to share, read through and understand.\n\n1. Create directory and notebook. We create a dedicated directory for our notebooks in the project root (notebooks/), and add a notebook (simulation_test.ipynb). These can be created via your operating system‚Äôs file manager, or with the command line:\nmkdir notebooks\ntouch notebooks/simulation_test.ipynb\n\n2. Check our package works. Open notebooks/simulation_test.ipynb and add the following code. This will test that we are able to import our package and use the basic function we had created.\n# Import our simulation package\nfrom simulation.model import run_simulation\n\n# Run the simulation with default parameters\nresults = run_simulation()\nprint(f\"Simulation completed with duration: {results['duration']}\")\nprint(f\"Results: {results['results']}\")\n\n# Run with a different duration\nlong_results = run_simulation(duration=200)\nprint(f\"Longer simulation completed with duration: {long_results['duration']}\")\nprint(f\"Number of results: {len(long_results['results'])}\")\nWhen you run the cell, you should see an output confirming that the simulation function runs and returns results, if everything is set up correctly.\nSimulation completed with duration: 100\nResults: [0, 10, 20, 30, 40, 50, 60, 70, 80, 90]\nLonger simulation completed with duration: 200\nNumber of results: 20",
    "crumbs": [
      "Setup",
      "Structuring as a package"
    ]
  },
  {
    "objectID": "pages/setup/package.html#summary",
    "href": "pages/setup/package.html#summary",
    "title": "Structuring as a package",
    "section": "",
    "text": "To structure your simulation project as a python package:\n\nPlace all core python functions in the simulation/ directory (e.g.¬†model.py).\nCreate simulation/__init__.py and pyproject.toml to set-up as a package.\nOrganise analysis into a seperate directory (e.g.¬†notebooks/).\n\nFor example:\nour_directory/\n‚îú‚îÄ‚îÄ simulation/             # Main package directory\n‚îÇ   ‚îú‚îÄ‚îÄ __init__.py         # Makes the directory a package\n‚îÇ   ‚îî‚îÄ‚îÄ model.py            # Our function\n‚îú‚îÄ‚îÄ .gitignore\n‚îú‚îÄ‚îÄ environment.yaml\n‚îú‚îÄ‚îÄ LICENSE\n‚îú‚îÄ‚îÄ pyproject.toml          # Package metadata\n‚îî‚îÄ‚îÄ README.md",
    "crumbs": [
      "Setup",
      "Structuring as a package"
    ]
  },
  {
    "objectID": "pages/setup/package.html#further-information",
    "href": "pages/setup/package.html#further-information",
    "title": "Structuring as a package",
    "section": "",
    "text": "‚ÄúHow I use Python to organize my data analyses‚Äù from Joshua Cook 2024",
    "crumbs": [
      "Setup",
      "Structuring as a package"
    ]
  },
  {
    "objectID": "pages/setup/package.html#required-packages",
    "href": "pages/setup/package.html#required-packages",
    "title": "Structuring as a package",
    "section": "",
    "text": "For this section, you will need roxygen2, usethis and devtools. If you used the environment provided in ?@sec-environment then you should have these already - but if you do not, make sure to install them! You can check if they are in your environment by running this command from the R console:\npackageVersion(\"devtools\")\npackageVersion(\"usethis\")\npackageVersion(\"roxygen2\")\nIf will print the version number of each package if installed.",
    "crumbs": [
      "Setup",
      "Structuring as a package"
    ]
  },
  {
    "objectID": "pages/setup/package.html#create-the-r-package-structure",
    "href": "pages/setup/package.html#create-the-r-package-structure",
    "title": "Structuring as a package",
    "section": "",
    "text": "First, let‚Äôs create the basic directory structure for our simulation package.\n\n1. Create folder Create a folder called R/ in your project directory - for example, by running this command in the terminal:\nmkdir R\n\n2. Add a .R file with a function. Within R/, create a file called model.R.\ntouch R/model.R\nIn this file, we will add our model code. For now, just copy in this simple function that generates a list of numbers. We will add some real code for our model later.\n# Core simulation model functionality\n\nrun_simulation &lt;- function(duration = 100) {\n  #' Run a simple dummy simulation for the specified duration.\n  #'\n  #' @param duration int. The length of time to run the simulation.\n  #' @return list. Dummy simulation results.\n  \n  results &lt;- seq(0, duration - 1)\n  results &lt;- results[results %% 10 == 0]\n  \n  return(list(\n    duration = duration,\n    status = \"completed\",\n    results = results\n  ))\n}\n\n3. Make a DESCRIPTION file. If you have worked through the ?@sec-environment page, then you should already have a DESCRIPTION file listing dependencies, though this step will walk you through more of the meta-data in that file important when structuring our work as a package.\nIf you haven‚Äôt already, then create a DESCRIPTION file by running this command in the terminal:\ntouch DESCRIPTION\nOpen the file and copy in the template below. This is similar to the standard template generated by usethis::use_description(), but with a few extra sections.\nPackage: packagename\nType: Package\nTitle: What the Package Does (One Line, Title Case)\nVersion: 0.0.0.9000\nURL: ...\nAuthors@R: \n    person(\"First\", \"Last\", , \"first.last@example.com\", role = c(\"aut\", \"cre\"))\nDescription: What the package does (one paragraph).\nLicense: `use_mit_license()`, `use_gpl3_license()` or friends to pick a\n    license\nEncoding: UTF-8\nRoxygen: list(markdown = TRUE)\nRoxygenNote: 7.0.0\nImports:\n    ...\nSuggests:\n    ...\nWe will then fill in the template with relevant information for our project. You don‚Äôt need to change Type, Encoding, or Roxygen. For the other arguments:\n\nPackage: When using devtools to work with our package (as below), it will prompt you to use a name that complies with CRAN (the main R package repository). They require that the name is only made up of letters, numbers and periods (.) - and that it must start with a letter and cannot end with a period. When structuring our research project as a package, this is not often with the aim of uploading it to CRAN, but it can be simple/good practice to follow these guidelines anyway, and means you avoid devtools error messages! \nTitle: Capitalised single line description of the package which does not end with a period (.).\nVersion: The package version. For R packages, this is usually set to 0.0.0.9000 during early development - though some developers prefer to set it to 0.1.0, as we have done. The version number is used to track changes to the package over time. It typically follows semantic versioning, with three numbers representing major, minor and patch changes. For more about how and when to update the version, see the ?@sec-changelog page.\nAuthors: List of author names, emails and roles. The main role options are the current maintainer (creator, cre), people who have made significant contributions (author, aut), those who have made smaller contributions (contributor, ctb), copyright holders (cph) and funders (fnd). You can add additional information using the comment argument, like your ORCID ID.\nURL: Link to your repository. If you don‚Äôt have one, we‚Äôd strongly recommend making one - check out the ?@sec-version page.\nDescription: Single paragraph describing project.\nLicense: A license tells others how they can use your code. The usethis package makes it easy to add a license: just call the function for your chosen license, for example:\nusethis::use_mit_license()\nThis will update the License field in DESCRIPTION and create both LICENSE (with the year and copyright holder) and LICENSE.md (with the full licence text). Note: it will prompt you to overwrite any existing licence files.\nR packages use this two-file structure, while GitHub typically expects a single LICENSE file containing the full text. Unless you plan to submit to CRAN - which requires the R package structure - either approach is fine. For simplicity, we recommend sticking with the standard R package setup using usethis, and agreeing if prompted to overwrite old license files.\nFor more information, see the ?@sec-licence page in this book, and the R Packages book.\nRoxygenNote: roxygen2 is used when documenting code. Update this to the version of roxygen2 which you have installed - to check, run:\npackageVersion(\"roxygen2\")\nImports: These are packages necessary for your package. In other words, if it‚Äôs used by code in R/, then list it here.\nSuggests: These are any other packages needed. For example, you might include those for development (devtools), testing (testthat), linting (lintr) - or packages used in your analysis (i.e.¬†any code not in R/).\n\nAs an example:\nPackage: simulation\nType: Package\nTitle: Simulation\nVersion: 0.1.0\nAuthors@R: c(\n    person(\n      \"Amy\", \"Heather\",\n      email = \"a.heather2@exeter.ac.uk\",\n      role = c(\"aut\", \"cre\"),\n      comment = c(ORCID = \"0000-0002-6983-2759\")\n    )\n  )\nURL: https://github.com/pythonhealthdatascience/rap_template_r_des\nDescription: Template reproducible analytical pipeline (RAP) for simple R\n    discrete-event simulation (DES) model.\nLicense: MIT + file LICENSE\nEncoding: UTF-8\nLazyData: true\nRoxygenNote: 7.3.2\nImports:\n    simmer,\n    magrittr,\n    dplyr,\n    purrr,\n    rlang,\n    tidyr,\n    tidyselect,\n    future,\n    future.apply,\n    ggplot2,\n    tibble,\n    gridExtra,\n    R6\nSuggests:\n    testthat (&gt;= 3.0.0),\n    patrick,\n    lintr,\n    devtools,\n    xtable,\n    data.table,\n    mockery\nConfig/testthat/edition: 3",
    "crumbs": [
      "Setup",
      "Structuring as a package"
    ]
  },
  {
    "objectID": "pages/setup/package.html#use-devtools-to-build-documentation-and-run-checks",
    "href": "pages/setup/package.html#use-devtools-to-build-documentation-and-run-checks",
    "title": "Structuring as a package",
    "section": "",
    "text": "1. Build package documentation. The function we created in model.R had a docstring (for more info on writing docstrings, see ?@sec-docstrings). We can create the documentation for this by calling:\ndevtools::document()\nThis will create:\n\nman/: folder with roxygen2 documentation for each function in package.\nNAMESPACE: file which will list all the functions and packages used within your package.\n\n\n2. Check the package. You can check that the package is set-up correctly by running:\ndevtools::check()\nThis will load it and perform standard checks. If all is well, you should get an output similar to:\n&gt; devtools::check()\n‚ïê‚ïê Documenting ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n‚Ñπ Updating simulation documentation\n‚Ñπ Loading simulation\nWriting NAMESPACE\nWriting run_simulation.Rd\n\n‚ïê‚ïê Building ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nSetting env vars:\n‚Ä¢ CFLAGS    : -Wall -pedantic -fdiagnostics-color=always\n‚Ä¢ CXXFLAGS  : -Wall -pedantic -fdiagnostics-color=always\n‚Ä¢ CXX11FLAGS: -Wall -pedantic -fdiagnostics-color=always\n‚Ä¢ CXX14FLAGS: -Wall -pedantic -fdiagnostics-color=always\n‚Ä¢ CXX17FLAGS: -Wall -pedantic -fdiagnostics-color=always\n‚Ä¢ CXX20FLAGS: -Wall -pedantic -fdiagnostics-color=always\n‚îÄ‚îÄ R CMD build ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n‚úî  checking for file ‚Äò/home/amy/Documents/stars/hospital-des-r/DESCRIPTION‚Äô ...\n‚îÄ  preparing ‚Äòsimulation‚Äô:\n‚úî  checking DESCRIPTION meta-information ...\n‚îÄ  checking for LF line-endings in source and make files and shell scripts\n‚îÄ  checking for empty or unneeded directories\n‚îÄ  building ‚Äòsimulation_0.1.0.tar.gz‚Äô\n   \n‚ïê‚ïê Checking ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nSetting env vars:\n‚Ä¢ _R_CHECK_CRAN_INCOMING_USE_ASPELL_           : TRUE\n‚Ä¢ _R_CHECK_CRAN_INCOMING_REMOTE_               : FALSE\n‚Ä¢ _R_CHECK_CRAN_INCOMING_                      : FALSE\n‚Ä¢ _R_CHECK_FORCE_SUGGESTS_                     : FALSE\n‚Ä¢ _R_CHECK_PACKAGES_USED_IGNORE_UNUSED_IMPORTS_: FALSE\n‚Ä¢ NOT_CRAN                                     : true\n‚îÄ‚îÄ R CMD check ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n‚îÄ  using log directory ‚Äò/tmp/RtmpyQepIc/file4b07699e20de/simulation.Rcheck‚Äô\n‚îÄ  using R version 4.4.1 (2024-06-14)\n‚îÄ  using platform: x86_64-pc-linux-gnu\n‚îÄ  R was compiled by\n       gcc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n       GNU Fortran (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n‚îÄ  running under: Ubuntu 24.04.2 LTS\n‚îÄ  using session charset: UTF-8\n‚îÄ  using options ‚Äò--no-manual --as-cran‚Äô\n‚úî  checking for file ‚Äòsimulation/DESCRIPTION‚Äô\n‚îÄ  this is package ‚Äòsimulation‚Äô version ‚Äò0.1.0‚Äô\n‚îÄ  package encoding: UTF-8\n‚úî  checking package namespace information\n‚úî  checking package dependencies (1.4s)\n‚úî  checking if this is a source package ...\n‚úî  checking if there is a namespace\n‚úî  checking for executable files\n‚úî  checking for hidden files and directories\n‚úî  checking for portable file names\n‚úî  checking for sufficient/correct file permissions\n‚úî  checking serialization versions\n‚úî  checking whether package ‚Äòsimulation‚Äô can be installed (771ms)\n‚úî  checking installed package size ...\n‚úî  checking package directory\n‚úî  checking for future file timestamps\n‚úî  checking DESCRIPTION meta-information ...\n‚úî  checking top-level files\n‚úî  checking for left-over files\n‚úî  checking index information\n‚úî  checking package subdirectories ...\n‚úî  checking code files for non-ASCII characters ...\n‚úî  checking R files for syntax errors ...\n‚úî  checking whether the package can be loaded ...\n‚úî  checking whether the package can be loaded with stated dependencies ...\n‚úî  checking whether the package can be unloaded cleanly ...\n‚úî  checking whether the namespace can be loaded with stated dependencies ...\n‚úî  checking whether the namespace can be unloaded cleanly ...\n‚úî  checking loading without being on the library search path ...\nN  checking dependencies in R code ...\n   Namespaces in Imports field not imported from:\n     ‚ÄòR6‚Äô ‚Äòdplyr‚Äô ‚Äòfuture‚Äô ‚Äòfuture.apply‚Äô ‚Äòggplot2‚Äô ‚ÄògridExtra‚Äô ‚Äòmagrittr‚Äô\n     ‚Äòpurrr‚Äô ‚Äòrlang‚Äô ‚Äòsimmer‚Äô ‚Äòtibble‚Äô ‚Äòtidyr‚Äô ‚Äòtidyselect‚Äô\n     All declared Imports should be used.\n‚úî  checking S3 generic/method consistency ...\n‚úî  checking replacement functions ...\n‚úî  checking foreign function calls ...\n‚úî  checking R code for possible problems (1.5s)\n‚úî  checking Rd files ...\n‚úî  checking Rd metadata ...\n‚úî  checking Rd line widths ...\n‚úî  checking Rd cross-references ...\n‚úî  checking for missing documentation entries ...\n‚úî  checking for code/documentation mismatches (343ms)\n‚úî  checking Rd \\usage sections ...\n‚úî  checking Rd contents ...\n‚úî  checking for unstated dependencies in examples ...\n‚îÄ  checking examples ... NONE\n‚úî  checking for non-standard things in the check directory\n‚úî  checking for detritus in the temp directory\n   \n   See\n     ‚Äò/tmp/RtmpyQepIc/file4b07699e20de/simulation.Rcheck/00check.log‚Äô\n   for details.\n   \n‚îÄ‚îÄ R CMD check results ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ simulation 0.1.0 ‚îÄ‚îÄ‚îÄ‚îÄ\nDuration: 7.3s\n\n‚ùØ checking dependencies in R code ... NOTE\n  Namespaces in Imports field not imported from:\n    ‚ÄòR6‚Äô ‚Äòdplyr‚Äô ‚Äòfuture‚Äô ‚Äòfuture.apply‚Äô ‚Äòggplot2‚Äô ‚ÄògridExtra‚Äô ‚Äòmagrittr‚Äô\n    ‚Äòpurrr‚Äô ‚Äòrlang‚Äô ‚Äòsimmer‚Äô ‚Äòtibble‚Äô ‚Äòtidyr‚Äô ‚Äòtidyselect‚Äô\n    All declared Imports should be used.\n\n0 errors ‚úî | 0 warnings ‚úî | 1 note ‚úñ\nThe note in this case is because we are importing packages not currently used in R/ - but won‚Äôt worry about this for now, as will do later when build our simulation model.\n\n3. Install the package. We can also verify that our package is working by trying to install it. From the console, run:\ndevtools::install()",
    "crumbs": [
      "Setup",
      "Structuring as a package"
    ]
  },
  {
    "objectID": "pages/setup/package.html#create-a-.rmd-file-to-test-your-package",
    "href": "pages/setup/package.html#create-a-.rmd-file-to-test-your-package",
    "title": "Structuring as a package",
    "section": "",
    "text": "We will run our model and analyse results within R markdown (.Rmd) files, as they combine code, results and explanations in one document. This can help make our methods and results easier to share, read through and understand.\n\n1. Create directory and R markdown file. We create a dedicated directory for our R markdown files in the project root (rmarkdown/), and add a file (simulation_test.Rmd). These can be created via the terminal by running:\nmkdir rmarkdown\ntouch rmarkdown/simulation_test.Rmd\nAs part of an R package, you can create a vignettes folder, which is typically used to hold guides, tutorials, or extended examples demonstrating how to use the package. Some people suggest putting your analysis in vignettes/, as this ensures everything is run from scratch each time you call devtools::check(), helping maintain reproducibility.\nHowever, we usually prefer to use an rmarkdown/ folder when structuring research as a package because running analyses as vignettes is very inefficient for simulations with longer run times, and saving outputs from vignettes can lead to file path errors during the package build process.\n\n2. Check our package works. Open rmarkdown/simulation_test.Rmd and add the following code into a cell. This means you put ```{r} in the line before the code, and ``` in the line after.\nThis code will test that we are able to import our package and use the basic function we had created.\n# Load the package from the local directory\ndevtools::load_all()\n\n# Load the package\nlibrary(simulation)\n\n# Run the simulation with default parameters\nresults &lt;- run_simulation()\ncat(sprintf(\"Simulation completed with duration: %s\\n\", results$duration))\ncat(sprintf(\"Results: %s\\n\", toString(results$results)))\n\n# Run with a different duration\nlong_results &lt;- run_simulation(duration = 200)\ncat(sprintf(\"Longer simulation completed with duration: %s\\n\", long_results$duration))\ncat(sprintf(\"Number of results: %d\\n\", length(long_results$results)))\nWhen you run the cell, you should see an output confirming that the simulation function runs and returns results, if everything is set up correctly.\n‚Ñπ Loading simulation\nSimulation completed with duration: 100\nResults: 0, 10, 20, 30, 40, 50, 60, 70, 80, 90\nLonger simulation completed with duration: 200\nNumber of results: 20",
    "crumbs": [
      "Setup",
      "Structuring as a package"
    ]
  },
  {
    "objectID": "pages/setup/package.html#summary-1",
    "href": "pages/setup/package.html#summary-1",
    "title": "Structuring as a package",
    "section": "",
    "text": "To structure your simulation project as an R package:\n\nPlace all core R functions in the R/ directory (e.g.¬†model.R).\nCreate a DESCRIPTION file to define package metadata.\nUse devtools and roxygen2 to generate documentation (man/, NAMESPACE) and check your package.\nOrganise analysis into a seperate directory (e.g.¬†rmarkdown/).\n\nFor example:\nour_directory/\n‚îú‚îÄ‚îÄ R/                  # Directory containing R functions\n‚îÇ   ‚îú‚îÄ‚îÄ model.R         # Code (e.g. for the model)\n‚îÇ   ‚îî‚îÄ‚îÄ ...             # Other .R files\n‚îú‚îÄ‚îÄ man/                # Documentation (generated from roxygen comments)\n‚îú‚îÄ‚îÄ DESCRIPTION         # Package metadata\n‚îú‚îÄ‚îÄ NAMESPACE           # Package exports (usually generated)\n‚îî‚îÄ‚îÄ rmarkdown/          # Analysis notebooks\n\n\n\n\n‚ÄúOpen, Reproducible, and Distributable Research With R Packages‚Äù from the DANTE Project - for example, this page on vignettes.\n‚ÄúSharing and organizing research products as R packages‚Äù from Vuorre and Crump 2020\n‚Äú4 Fundamental development workflows‚Äù from the ‚ÄúR Packages‚Äù book by Hadley Wickham and Jennifer Bryan\n‚Äú9 DESCRIPTION‚Äù from the ‚ÄúR Packages‚Äù book by Hadley Wickham and Jennifer Bryan",
    "crumbs": [
      "Setup",
      "Structuring as a package"
    ]
  },
  {
    "objectID": "pages/sharing/peer_review.html",
    "href": "pages/sharing/peer_review.html",
    "title": "Peer review",
    "section": "",
    "text": "Peer review\n\nüîó Reproducibility guidelines:\n\nNHS Levels of RAP (ü•â): Code has been peer reviewed.",
    "crumbs": [
      "Collaboration & sharing",
      "Peer review"
    ]
  },
  {
    "objectID": "pages/sharing/licence.html",
    "href": "pages/sharing/licence.html",
    "title": "Licensing",
    "section": "",
    "text": "Licensing\n\nüîó Reproducibility guidelines:\n\nHeather et al.¬†2025 (‚≠ê): Share code with an open licence.",
    "crumbs": [
      "Collaboration & sharing",
      "Licensing"
    ]
  },
  {
    "objectID": "pages/sharing/citation.html",
    "href": "pages/sharing/citation.html",
    "title": "Citation guidance",
    "section": "",
    "text": "Citation guidance",
    "crumbs": [
      "Collaboration & sharing",
      "Citation guidance"
    ]
  },
  {
    "objectID": "pages/verification_validation/logs.html",
    "href": "pages/verification_validation/logs.html",
    "title": "Logging",
    "section": "",
    "text": "Logging\n\nüîó Reproducibility guidelines:\n\nNHS Levels of RAP (ü•à): Logs are automatically recorded by the pipeline to ensure outputs are as expected.",
    "crumbs": [
      "Verification & validation",
      "Logging"
    ]
  },
  {
    "objectID": "pages/model/distributions.html",
    "href": "pages/model/distributions.html",
    "title": "Randomness",
    "section": "",
    "text": "Randomness\n\nüîó Reproducibility guidelines:\n\nHeather et al.¬†2025: Control randomness.",
    "crumbs": [
      "Model building",
      "Randomness"
    ]
  },
  {
    "objectID": "pages/model/process.html",
    "href": "pages/model/process.html",
    "title": "Entity processing",
    "section": "",
    "text": "Entity processing",
    "crumbs": [
      "Model building",
      "Entity processing"
    ]
  },
  {
    "objectID": "pages/inputs/input_data.html",
    "href": "pages/inputs/input_data.html",
    "title": "Input data management",
    "section": "",
    "text": "Your reproducible analytical pipeline (RAP) should start from the earliest data you access - either with raw data (if you estimate parameters yourself) or with pre-defined parameters (if those are supplied). This ensures that every step in your process is transparent and reproducible. For example, if you obtain updated raw data, you would be able to re-estimate parameters, check that your chosen distributions are still appropriate, and re-run the simulation.\nExternal sharing of the full RAP may not always be possible. In healthcare simulations, the raw data may be sensitive or identifiable, and so cannot be shared due to privacy or ethical concerns. However, mainintaing a complete RAP internally is still essential for your team or organisation to ensure that the process is fully reproducible.\nThere are a few key files to consider for your RAP:\n\nRaw data\nInput modelling code\nParameters\n\n\n\n\n\nThis is data which reflects system you will be simulating. It is used to estimate parameters and fit distributions for your simulation model. For example:\n\n\n\n\n\n\n\n\n\n\n\n\nARRIVAL_DATE\nARRIVAL_TIME\nSERVICE_DATE\nSERVICE_TIME\nDEPARTURE_DATE\nDEPARTURE_TIME\n\n\n\n\n2025-01-01\n0001\n2025-01-01\n0007\n2025-01-01\n0012\n\n\n2025-01-01\n0002\n2025-01-01\n0004\n2025-01-01\n0007\n\n\n2025-01-01\n0003\n2025-01-01\n0010\n2025-01-01\n0030\n\n\n2025-01-01\n0007\n2025-01-01\n0014\n2025-01-01\n0022\n\n\n\n\n\n\n\nYou should either keep copies of the raw data or, if the data is in a secure database or very large, and cannot be exported, clearly describe how to access it (including the database location, access permissions, and navigation instructions).\nFor both options, you should also document when you obtained the data, and relevant metadata (e.g.¬†time period the data covers, number of records, any known issues or missing data). For example:\n\n‚ÄúData sourced from the XYZ database. Copies are available in this repository, or, to access directly, log in to the XYZ database and navigate to [path/to/data]. Data covers January 2012 to December 2017, with [number] records. Note: [details on missing data, known issues, etc.]. A copy of the data dictionary is available in the repository or online at [URL].‚Äù\n\nYou should keep a copy of the data dictionary. A data dictionary describes each field, its format, units, and any coding schemes used. If one is not provided for your data, then you should create your own, to ensure the raw data used for your simulation is clear and understandable. For example:\n\n\n\n\n\n\n\n\n\n\nField\nField name\nFormat\nDescription\n\n\n\n\nARRIVAL_DATE\nCLINIC ARRIVAL DATE\nDate(CCYY-MM-DD)\nThe date on which the patient arrived at the clinic\n\n\nARRIVAL_TIME\nCLINIC ARRIVAL TIME\nTime(HH:MM)\nThe time at which the patient arrived at the clinic\n\n\nDEPARTURE_DATE\nCLINIC DEPARTURE DATE\nDate(CCYY-MM-DD)\nThe date on which the patient left the clinic\n\n\nDEPARTURE_TIME\nCLINIC DEPARTURE TIME\nTime(HH:MM)\nThe time at which the patient left the clinic\n\n\nSERVICE_DATE\nNURSE SERVICE START DATE\nDate(CCYY-MM-DD)\nThe date on which the nurse consultation began\n\n\nSERVICE_TIME\nNURSE SERVICE START TIME\nTime(HH:MM)\nThe time at which the nurse consultation began\n\n\n\n\n\n\n\n\nIf you are able to share raw data externally:\n\n\n‚òëÔ∏è Make the data openly available, following FAIR principles (Findable, Accessible, Interoperable, Reusable).\n‚òëÔ∏è Include a data dictionary.\n‚òëÔ∏è Deposit the data in a trusted public archive (e.g., Zenodo, Figshare) or a code/data repository (e.g., GitHub, GitLab).\n‚òëÔ∏è Include an open data licence (e.g., CC0, CC-BY) to clarify usage rights and restrictions.\n‚òëÔ∏è Provide a clear citation or DOI for others to reference.\n\n\nIf you cannot share raw data:\n\n\n‚òëÔ∏è Describe the dataset in your documentation (e.g.¬†‚ÄúPatient records from XYZ database, covering 2012‚Äì2017, with fields for arrival, service, and departure times.‚Äù).\n‚òëÔ∏è Share the data dictionary (if possible) to demonstrate the structure and content of the dataset.\n‚òëÔ∏è Consider providing a synthetic dataset that mimics the structure and format of the real data. This allows others to understand the data layout and run processing scripts without exposing sensitive information.\n‚òëÔ∏è Explain the access restrictions for the dataset, and provide relevant contacts (e.g.¬†‚ÄúAccess to the dataset is restricted due to patient confidentiality. Researchers interested in accessing the data must submit a data access request to the XYZ Data Governance Committee. For more information, contact data.manager@xyz.org.‚Äù).\n\n\n\n\n\n\n\nThis code (e.g. ?@sec-input_modelling) is often not shared, but is an essential part of your simulation RAP. It ensures transparency in how distributions were chosen and allows you (or others) to re-run the process if new data becomes available or if you need to update your assumptions.\nIf you are able to share this code externally:\n\n\n‚òëÔ∏è Include it in your repository alongside your simulation code and scripts.\n\n\nIf you cannot share this code:\n\n\n‚òëÔ∏è Internally, store the code securely, ideally with version control. For example, if possible, within a private GitHub repository.\n‚òëÔ∏è Externally, document the input modelling process you followed.\n‚òëÔ∏è Externally, describe the reasons for not sharing (e.g.¬†code contains sensitive logic).\n\n\n\n\n\n\nParameters are the numerical values used in your model, like the arrival rates, service times or probabilities.\n\n\n\nYou should keep clearly structured parameter files that record all values used in your model. This could be:\n\nA CSV file (e.g. ?@sec-param_file).\nA script (e.g. ?@sec-param_script).\n\nFor each parameter, keep documentation (e.g.¬†data dictionary, docstrings). Ensure you document:\n\nThe parameter name and value.\nUnits (e.g.¬†minutes).\nAny abbreviations used.\n\nIf the determination of parameters is not captured within any scripts from ?@sec-input_modelling, then you should either:\n\nProvide scripts which calculate them, or\nDescribe the processing steps if these cannot be shared.\n\nIf the parameters were not calculated by you, but provided by someone else, clearly state the source and describe any processing or transformation steps you know about.\n\n\n\nYou must share parameters with your model so that it is possible for others to run it. Parameters are often less sensitive than raw data, so sharing is usually possible.\nIf you cannot share the real parameters, you should provide synthetic parameters, so others can still run your model. To create synthetic parameters:\n\nUse plausible values within the expected range for each parameter, or\nSimulate values based on published ranges or previous studies.\n\nClearly document that these are synthetic and explain how they were generated.\nAny files (data or scripts) should be shared as described above for data (Section¬†1.0.1) and code (Section¬†1.0.2).\n\n\n\n\n\n\nIt is likely that you may have some data and/or code that you need to keep private, and cannot share along with the simulation model. It‚Äôs important that both the private and public components are version controlled (?@sec-version). One way of managing this is to have two separate repositories: a private repository and a public repository.\nIf the public repository contains the real parameters and results, it‚Äôs quite simple: use the private repository for processing input data, then switch to the public repository for running the model.\nIf the public repository only contains synthetic parameters, you‚Äôll need to be able to run the simulation in the private repository with the real parameters and results, and also in the public repository with the synthetic parameters and results. To avoid duplicating the simulation code across both repositories, a good strategy is to develop your simulation code as a package (?@sec-package). This package can be published on GitHub, PyPI, or simply installed locally. Your private repository can then import and use this package, allowing you to maintain a single version of the simulation code while keeping sensitive parameters and data private. \n\n\n\n\nhttps://journals.sagepub.com/doi/full/10.1177/2515245920928007\nhttps://help.osf.io/article/217-how-to-make-a-data-dictionary\nhttps://open-science-training-handbook.github.io/Open-Science-Training-Handbook_EN/02OpenScienceBasics/02OpenResearchDataAndMaterials.html\nhttps://caltechlibrary.github.io/RDMworkbook/\nhttps://ddialliance.org/",
    "crumbs": [
      "Model inputs",
      "Input data management"
    ]
  },
  {
    "objectID": "pages/inputs/input_data.html#maintaining-a-private-and-public-version-of-your-model",
    "href": "pages/inputs/input_data.html#maintaining-a-private-and-public-version-of-your-model",
    "title": "Input data management",
    "section": "",
    "text": "It is likely that you may have some data and/or code that you need to keep private, and cannot share along with the simulation model. It‚Äôs important that both the private and public components are version controlled (?@sec-version). One way of managing this is to have two separate repositories: a private repository and a public repository.\nIf the public repository contains the real parameters and results, it‚Äôs quite simple: use the private repository for processing input data, then switch to the public repository for running the model.\nIf the public repository only contains synthetic parameters, you‚Äôll need to be able to run the simulation in the private repository with the real parameters and results, and also in the public repository with the synthetic parameters and results. To avoid duplicating the simulation code across both repositories, a good strategy is to develop your simulation code as a package (?@sec-package). This package can be published on GitHub, PyPI, or simply installed locally. Your private repository can then import and use this package, allowing you to maintain a single version of the simulation code while keeping sensitive parameters and data private.",
    "crumbs": [
      "Model inputs",
      "Input data management"
    ]
  },
  {
    "objectID": "pages/inputs/input_data.html#further-information",
    "href": "pages/inputs/input_data.html#further-information",
    "title": "Input data management",
    "section": "",
    "text": "https://journals.sagepub.com/doi/full/10.1177/2515245920928007\nhttps://help.osf.io/article/217-how-to-make-a-data-dictionary\nhttps://open-science-training-handbook.github.io/Open-Science-Training-Handbook_EN/02OpenScienceBasics/02OpenResearchDataAndMaterials.html\nhttps://caltechlibrary.github.io/RDMworkbook/\nhttps://ddialliance.org/",
    "crumbs": [
      "Model inputs",
      "Input data management"
    ]
  },
  {
    "objectID": "pages/inputs/parameters_script.html",
    "href": "pages/inputs/parameters_script.html",
    "title": "Parameters from script",
    "section": "",
    "text": "üîó Reproducibility guidelines:\n\nHeather et al.¬†2025: Avoid hard-coded parameters.\n\n\nDiscrete-event simulations (DES) rely on many parameters lik patient arrival rates, resource usage times, and transition probilities. Managing these parameters well is crucial, as you‚Äôll often want to change them to test different scenarios or run sensitivity analyses.\nGood parameter management means storing parameters in a way that makes them easy to update, track, and reuse.\n\n\n\nHardcoding means writing parameter values directly into your code. For example:\n\ndef model():\n    # Hardcoded parameter values\n    interarrival_time = 5\n    consultation_time = 20\n    transfer_prob = 0.3\n    # ...rest of the model...\n\nThis makes it very difficult to change the values. Modellers might choose to‚Ä¶\n1. Edit parameters directly in the script.\nThis involves manually changing parameter values in the script each time you want to run a new scenario. Problems with this approach include:\n\n‚ùå Not a reproducible analytical pipeline. Alot of manual intervention is required to re-run the model with different parameters.\n‚ùå Error-prone. It would be easy to make mistakes or forget to update all relevant values.\n‚ùå Parameters can get lost. If you lost your notes or forget to record what you used, you won‚Äôt know what values were used for past runs.\n\n2. Duplicate scripts for each scenario.\nThis involves copying the entire script, changing parameters in each copy, and running them separately. Problems with this approach include:\n\n‚ùå Code duplication. This means any changes - like bug fixes or improvements to the model - must be made to every copy. This is tedious and there is a risk that some copies are missed or updated incorrectly.\n‚ùå Hard to keep track. With multiple script copies, it can become difficult to know which scripts correspond to which scenarios, and which parameters were used (as have to delve into the model code to identify them).\n\n\n\n\n\nA better (but still limited) approach is to define all parameters at the top of your script:\nThey‚Äôre no longer hard coded. Can see what parameters were used, easier when changing.\n\n# Parameters for base case\nINTERARRIVAL_TIME = 5\nCONSULTATION_TIME = 20\nTRANSFER_PROB = 0.3\n\ndef model():\n    # Use the global parameters\n    ...\n\nFor scenarios, you would define the same global variables with alternative values:\n\n# Scenario 1\nINTERARRIVAL_TIME = 6\nCONSULTATION_TIME = 20\nTRANSFER_PROB = 0.3\n# Scenario 2\nINTERARRIVAL_TIME = 5\nCONSULTATION_TIME = 20\nTRANSFER_PROB = 0.4\n\nThe improvements are that parameters are:\n\n‚úÖ No longer hardcoded. Within the model, it refers to the variable name (e.g.¬†INTERARRIVAL_TIME) rather than a specific value (e.g.¬†5), which means we are able to now list the values in one outside the model logic.\n‚úÖ Centralised. All parameters are in one place, making them easier to find and change.\n\nHowever, there are still several disadvantages:\n\n‚ùå Still inflexible. In order to re-run the model with different scenarios, you would still need to do the approaches above - editing code directly or duplicating scripts for each scenario.\n‚ùå Not scalable. As the number of scenarios or parameters grows, managing all these global variables becomes messy.\n\n\n\n\n\nTo manage parameters effectively, you need to:\n\nGroup parameters into a dedicated object.\nPass these objects explicitly to your model.\n\nWhy?\n\n‚úÖ Clear parameter sets. Every scenario has its own object with all the parameters needed. This can be easily viewed, and comes in handy in logs (?@sec-logs) to easily print a copy of all parameters used for a scenario.\n‚úÖ No global variables. By explicitly passing our parameters, we avoid accidental parameter reuse between scenarios (which is a possibility with global variables!).\n‚úÖ Fewer inputs. If all parameters are in one object, then we can just pass that as a single input to our model function/class, reducing the number of arguments we need to pass.\n\n\nIt‚Äôs important to use both of these practices.\nIf you only do option 1 (group parameters, but use as globals), parameters might accidentally be modified elsewhere, or one scenario‚Äôs parameters might affect another‚Äôs.\n# BAD: Parameters are grouped but still global\nglobal_params = Parameters()\n\ndef simulate():\n    # Uses global_params.interarrival_time... üò¨\n    ...\nIf you only do option 2 (pass parameters, but don‚Äôt group them), you end up with messy, error-prone code that‚Äôs hard to maintain:\n# BAD: Parameters are passed but disorganized\ndef simulate(interarrival_time, consultation_time, transfer_prob, ...):\n    # 10+ parameters? Hard to track!\n    ...\n\nThere are three implementation options: dictionary, function or class.\n\n\n\n\nbase_params = {\n    \"interarrival_time\": 5,\n    \"consultation_time\": 20,\n    \"transfer_prob\": 0.3,\n}\n\n# Create a scenario by copying and tweaking only what's needed\nscenario1 = base_params.copy()\nscenario1[\"interarrival_time\"] = 6\nscenario2 = base_params.copy()\nscenario2[\"transfer_prob\"] = 0.4\n\n\n\n\n\n\ndef create_params(interarrival_time=5, consultation_time=20, transfer_prob=0.3):\n    return {\n        \"interarrival_time\": interarrival_time,\n        \"consultation_time\": consultation_time,\n        \"transfer_prob\": transfer_prob\n    }\n\nbase_params = create_params()\nscenario1 = create_params(interarrival_time=6)\nscenario2 = create_params(transfer_prob=0.4)\n\n\n\n\n\n\nclass Parameters:\n    def __init__(self, interarrival_time=5, consultation_time=20, transfer_prob=0.3):\n        self.interarrival_time = interarrival_time\n        self.consultation_time = consultation_time\n        self.transfer_prob = transfer_prob\n\n    def __repr__(self):\n        return (f\"Parameters(interarrival_time={self.interarrival_time}, \"\n                f\"consultation_time={self.consultation_time}, \"\n                f\"transfer_prob={self.transfer_prob})\")\n\n# Base case\nbase_params = Parameters()\n\n# Scenario 1: Only change interarrival_time\nscenario1 = Parameters(interarrival_time=6)\n\n# Scenario 2: Only change transfer_prob\nscenario2 = Parameters(transfer_prob=0.4)\n\n\n\n\n\nThe most robust approach is to use a function or class to manage your parameters.\n\n‚úÖ Functions and classes make it easy to create variations for different scenarios, since you simply change the inputs when you define a new scenario. For example, you can create a new scenario by only specifying the parameter you want to change, while all other parameters remain at their default values.\n‚ùå With a dictionary, you have to make a copy of the base dictionary and then manually change individual values for each scenario. This can become cumbersome as the number of parameters or scenarios grows - and is just a bit more clunky!\n\nYour choice may be further informed by options for parameter validation, where classes can be superior as you can incorporate validation within the class, as discussed on the page ‚Äú?@sec-param_validation‚Äù.\n\n\n\n\n\nYou may need to manage many parameters - for example, if you have several patient types and/or units each with their own arrival times, resource times, and so on.\nWe have suggested a few strategies you could use‚Ä¶\n\n\n\nThis can be convenient for smaller models, though can get unwieldly as the number of parameters grow, including potentially quick long parameter names!\nFunction example:\n\ndef create_params(\n    adult_interarrival=5, adult_consultation=20, adult_transfer=0.3,\n    child_interarrival=7, child_consultation=15, child_transfer=0.2,\n    elderly_interarrival=10, elderly_consultation=30, elderly_transfer=0.5\n):\n    return {\n        \"adult\": {\n            \"interarrival_time\": adult_interarrival,\n            \"consultation_time\": adult_consultation,\n            \"transfer_prob\": adult_transfer\n        },\n        \"child\": {\n            \"interarrival_time\": child_interarrival,\n            \"consultation_time\": child_consultation,\n            \"transfer_prob\": child_transfer\n        },\n        \"elderly\": {\n            \"interarrival_time\": elderly_interarrival,\n            \"consultation_time\": elderly_consultation,\n            \"transfer_prob\": elderly_transfer\n        }\n    }\n\nClass example:\n\nclass Parameters:\n    def __init__(\n        self,\n        adult_interarrival=5, adult_consultation=20, adult_transfer=0.3,\n        child_interarrival=7, child_consultation=15, child_transfer=0.2,\n        elderly_interarrival=10, elderly_consultation=30, elderly_transfer=0.5\n    ):\n        # Adult parameters\n        self.adult_interarrival = adult_interarrival\n        self.adult_consultation = adult_consultation\n        self.adult_transfer = adult_transfer\n        # Child parameters\n        self.child_interarrival = child_interarrival\n        self.child_consultation = child_consultation\n        self.child_transfer = child_transfer\n        # Elderly parameters\n        self.elderly_interarrival = elderly_interarrival\n        self.elderly_consultation = elderly_consultation\n        self.elderly_transfer = elderly_transfer\n\n\n\n\n\nAlternatively, you can split parameters into logical groups (e.g.¬†patient type, parameter type), each with it‚Äôs own function or class. These are then combined into single parameter set.\nWith a large number of parameters, this keeps each individual function/class simpler. seperate means its simpler with the inputs and stuff, just adult child elderly also, if doing validation, can do for each subclass\nFunction example:\n\ndef create_arrivals(adult=5, child=7, elderly=10):\n    return {\n        \"adult\": adult,\n        \"child\": child,\n        \"elderly\": elderly\n    }\n\ndef create_consultations(adult=20, child=15, elderly=30):\n    return {\n        \"adult\": adult,\n        \"child\": child,\n        \"elderly\": elderly\n    }\n\ndef create_transfers(adult=0.3, child=0.2, elderly=0.5):\n    return {\n        \"adult\": adult,\n        \"child\": child,\n        \"elderly\": elderly\n    }\n\ndef create_parameters(\n    arrivals=create_arrivals(),\n    consultations=create_consultations(),\n    transfers=create_transfers()\n):\n    return {\n        \"arrivals\": arrivals,\n        \"consultations\": consultations,\n        \"transfers\": transfers\n    }\n\nClass example:\n\n\n\nclass Arrivals:\n    def __init__(self, adult=5, child=7, elderly=10):\n        self.adult = adult\n        self.child = child\n        self.elderly = elderly\n\n\nclass Consultations:\n    def __init__(self, adult=20, child=15, elderly=30):\n        self.adult = adult\n        self.child = child\n        self.elderly = elderly\n\n\nclass Transfers:\n    def __init__(self, adult=0.3, child=0.2, elderly=0.5):\n        self.adult = adult\n        self.child = child\n        self.elderly = elderly\n\n\nclass Parameters():\n    def __init__(\n        self,\n        arrivals=Arrivals(),\n        consultations=Consultations(),\n        transfers=Transfers()\n    ):\n        self.arrivals = arrivals\n        self.consultations = consultations\n        self.transfers = transfers\n\n\n\n\n\nfor the ?@sec-examples‚Ä¶ parameter bits from those‚Ä¶ copy in full to have the parameters for models examples‚Ä¶\nnurse visit simulation does classes, with validation directly in class\nstroke pathway simulation has more parma so multiple classes combined, with validation from super class\n\n\n\nShow/Hide example 1: Nurse visit simulation\n\ntouch simulation/model.py\n# pylint: disable=too-many-instance-attributes,too-few-public-methods\nclass Param:\n    \"\"\"\n    Default parameters for simulation.\n\n    Attributes are described in initialisation docstring.\n    \"\"\"\n    # pylint: disable=too-many-arguments,too-many-positional-arguments\n    def __init__(\n        self,\n        patient_inter=4,\n        mean_n_consult_time=10,\n        number_of_nurses=5,\n        warm_up_period=1440*27,  # 27 days\n        data_collection_period=1440*30,  # 30 days\n        number_of_runs=31,\n        audit_interval=120,  # every 2 hours\n        scenario_name=0,\n        cores=-1,\n        logger=SimLogger(log_to_console=False, log_to_file=False)\n    ):\n        \"\"\"\n        Initialise instance of parameters class.\n\n        Arguments:\n            patient_inter (float):\n                Mean inter-arrival time between patients in minutes.\n            mean_n_consult_time (float):\n                Mean nurse consultation time in minutes.\n            number_of_nurses (float):\n                Number of available nurses.\n            warm_up_period (int):\n                Duration of the warm-up period in minutes - running simulation\n                but not yet collecting results.\n            data_collection_period (int):\n                Duration of data collection period in minutes (also known as\n                measurement interval) - which begins after any warm-up period.\n            number_of_runs (int):\n                The number of runs (i.e. replications), defining how many\n                times to re-run the simulation (with different random numbers).\n            audit_interval (int):\n                How frequently to audit resource utilisation, in minutes.\n            scenario_name (int|float|string):\n                Label for the scenario.\n            cores (int):\n                Number of CPU cores to use for parallel execution. Set to\n                desired number, or to -1 to use all available cores. For\n                sequential execution, set to 1 (default).\n            logger (logging.Logger):\n                The logging instance used for logging messages.\n        \"\"\"\n        # Disable restriction on attribute modification during initialisation\n        object.__setattr__(self, '_initialising', True)\n\n        self.patient_inter = patient_inter\n        self.mean_n_consult_time = mean_n_consult_time\n        self.number_of_nurses = number_of_nurses\n        self.warm_up_period = warm_up_period\n        self.data_collection_period = data_collection_period\n        self.number_of_runs = number_of_runs\n        self.audit_interval = audit_interval\n        self.scenario_name = scenario_name\n        self.cores = cores\n        self.logger = logger\n\n        # Re-enable attribute checks after initialisation\n        object.__setattr__(self, '_initialising', False)\n\n    def __setattr__(self, name, value):\n        \"\"\"\n        Prevent addition of new attributes.\n\n        This method overrides the default `__setattr__` behavior to restrict\n        the addition of new attributes to the instance. It allows modification\n        of existing attributes but raises an `AttributeError` if an attempt is\n        made to create a new attribute. This ensures that accidental typos in\n        attribute names do not silently create new attributes.\n\n        Arguments:\n            name (str):\n                The name of the attribute to set.\n            value (Any):\n                The value to assign to the attribute.\n\n        Raises:\n            AttributeError:\n                If `name` is not an existing attribute and an attempt is made\n                to add it to the instance.\n        \"\"\"\n        # Skip the check if the object is still initialising\n        # pylint: disable=maybe-no-member\n        if hasattr(self, '_initialising') and self._initialising:\n            super().__setattr__(name, value)\n        else:\n            # Check if attribute of that name is already present\n            if name in self.__dict__:\n                super().__setattr__(name, value)\n            else:\n                raise AttributeError(\n                    f'Cannot add new attribute \"{name}\" - only possible to ' +\n                    f'modify existing attributes: {self.__dict__.keys()}')\n\n\n\n\nShow/Hide example 2: stroke pathway simulation\n\nMake file.\ntouch simulation/parameters.py\nCopy in.\n\"\"\"\nStroke pathway simulation parameters.\n\nIt includes arrival rates, length of stay distributions, and routing\nprobabilities between different care settings.\n\"\"\"\n\nfrom simulation.logging import SimLogger\n\n\nclass RestrictAttributesMeta(type):\n    \"\"\"\n    Metaclass for attribute restriction.\n\n    A metaclass modifies class construction. It intercepts instance creation\n    via __call__, adding the _initialised flag after __init__ completes. This\n    is later used by RestrictAttributes to enforce attribute restrictions.\n    \"\"\"\n    def __call__(cls, *args, **kwargs):\n        # Create instance using the standard method\n        instance = super().__call__(*args, **kwargs)\n        # Set the \"_initialised\" flag to True, marking end of initialisation\n        instance.__dict__[\"_initialised\"] = True\n        return instance\n\n\nclass RestrictAttributes(metaclass=RestrictAttributesMeta):\n    \"\"\"\n    Base class that prevents the addition of new attributes after\n    initialisation.\n\n    This class uses RestrictAttributesMeta as its metaclass to implement\n    attribute restriction. It allows for safe initialisation of attributes\n    during the __init__ method, but prevents the addition of new attributes\n    afterwards.\n\n    The restriction is enforced through the custom __setattr__ method, which\n    checks if the attribute already exists before allowing assignment.\n    \"\"\"\n    def __setattr__(self, name, value):\n        \"\"\"\n        Prevent addition of new attributes.\n\n        Parameters\n        ----------\n        name: str\n            The name of the attribute to set.\n        value: any\n            The value to assign to the attribute.\n\n        Raises\n        ------\n        AttributeError\n            If `name` is not an existing attribute and an attempt is made\n            to add it to the class instance.\n        \"\"\"\n        # Check if the instance is initialised and the attribute doesn\"t exist\n        if hasattr(self, \"_initialised\") and not hasattr(self, name):\n            # Get a list of existing attributes for the error message\n            existing = \", \".join(self.__dict__.keys())\n            raise AttributeError(\n                f\"Cannot add new attribute '{name}' - only possible to \" +\n                f\"modify existing attributes: {existing}.\"\n            )\n        # If checks pass, set the attribute using the standard method\n        object.__setattr__(self, name, value)\n\n\nclass ASUArrivals(RestrictAttributes):\n    \"\"\"\n    Arrival rates for the acute stroke unit (ASU) by patient type.\n\n    These are the average time intervals (in days) between new admissions.\n    For example, a value of 1.2 means a new admission every 1.2 days.\n    \"\"\"\n    def __init__(self, stroke=1.2, tia=9.3, neuro=3.6, other=3.2):\n        \"\"\"\n        Parameters\n        ----------\n        stroke: float\n            Stroke patient.\n        tia: float\n            Transient ischaemic attack (TIA) patient.\n        neuro: float\n            Complex neurological patient.\n        other: float\n            Other patient types (including medical outliers).\n        \"\"\"\n        self.stroke = stroke\n        self.tia = tia\n        self.neuro = neuro\n        self.other = other\n\n\nclass RehabArrivals(RestrictAttributes):\n    \"\"\"\n    Arrival rates for the rehabiliation unit by patient type.\n\n    These are the average time intervals (in days) between new admissions.\n    For example, a value of 21.8 means a new admission every 21.8 days.\n    \"\"\"\n    def __init__(self, stroke=21.8, neuro=31.7, other=28.6):\n        \"\"\"\n        Parameters\n        ----------\n        stroke: float\n            Stroke patient.\n        neuro: float\n            Complex neurological patient.\n        other: float\n            Other patient types.\n        \"\"\"\n        self.stroke = stroke\n        self.neuro = neuro\n        self.other = other\n\n\nclass ASULOS(RestrictAttributes):\n    \"\"\"\n    Mean and standard deviation (SD) of length of stay (LOS) in days in the\n    acute stroke unit (ASU) by patient type.\n\n    Attributes\n    ----------\n    stroke_noesd: dict\n        Mean and SD of LOS for stroke patients without early support discharge.\n    stroke_esd: dict\n        Mean and SD of LOS for stroke patients with early support discharge.\n    tia: dict\n        Mean and SD of LOS for transient ischemic attack (TIA) patients.\n    neuro: dict\n        Mean and SD of LOS for complex neurological patients.\n    other: dict\n        Mean and SD of LOS for other patients.\n    \"\"\"\n    def __init__(\n        self,\n        stroke_no_esd_mean=7.4, stroke_no_esd_sd=8.61,\n        stroke_esd_mean=4.6, stroke_esd_sd=4.8,\n        tia_mean=1.8, tia_sd=2.3,\n        neuro_mean=4.0, neuro_sd=5.0,\n        other_mean=3.8, other_sd=5.2\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        stroke_no_esd_mean: float\n            Mean LOS for stroke patients without early support discharge (ESD)\n            services.\n        stroke_no_esd_sd: float\n            SD of LOS for stroke patients without ESD.\n        stroke_esd_mean: float\n            Mean LOS for stroke patients with ESD.\n        stroke_esd_sd: float\n            SD of LOS for stroke patients with ESD.\n        tia_mean: float\n            Mean LOS for TIA patients.\n        tia_sd: float\n            SD of LOS for TIA patients.\n        neuro_mean: float\n            Mean LOS for complex neurological patients.\n        neuro_sd: float\n            SD of LOS for complex neurological patients.\n        other_mean: float\n            Mean LOS for other patient types.\n        other_sd: float\n            SD of LOS for other patient types.\n        \"\"\"\n        self.stroke_noesd = {\n            \"mean\": stroke_no_esd_mean,\n            \"sd\": stroke_no_esd_sd\n        }\n        self.stroke_esd = {\n            \"mean\": stroke_esd_mean,\n            \"sd\": stroke_esd_sd\n        }\n        self.tia = {\n            \"mean\": tia_mean,\n            \"sd\": tia_sd\n        }\n        self.neuro = {\n            \"mean\": neuro_mean,\n            \"sd\": neuro_sd\n        }\n        self.other = {\n            \"mean\": other_mean,\n            \"sd\": other_sd\n        }\n\n\nclass RehabLOS(RestrictAttributes):\n    \"\"\"\n    Mean and standard deviation (SD) of length of stay (LOS) in days in the\n    rehabilitation unit by patient type.\n\n    Attributes\n    ----------\n    stroke_noesd: dict\n        Mean and SD of LOS for stroke patients without early support discharge.\n    stroke_esd: dict\n        Mean and SD of LOS for stroke patients with early support discharge.\n    tia: dict\n        Mean and SD of LOS for transient ischemic attack (TIA) patients.\n    neuro: dict\n        Mean and SD of LOS for complex neurological patients.\n    other: dict\n        Mean and SD of LOS for other patients.\n    \"\"\"\n    def __init__(\n        self,\n        stroke_no_esd_mean=28.4, stroke_no_esd_sd=27.2,\n        stroke_esd_mean=30.3, stroke_esd_sd=23.1,\n        tia_mean=18.7, tia_sd=23.5,\n        neuro_mean=27.6, neuro_sd=28.4,\n        other_mean=16.1, other_sd=14.1\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        stroke_no_esd_mean: float\n            Mean LOS for stroke patients without early support discharge (ESD)\n            services.\n        stroke_no_esd_sd: float\n            SD of LOS for stroke patients without ESD.\n        stroke_esd_mean: float\n            Mean LOS for stroke patients with ESD.\n        stroke_esd_sd: float\n            SD of LOS for stroke patients with ESD.\n        tia_mean: float\n            Mean LOS for TIA patients.\n        tia_sd: float\n            SD of LOS for TIA patients.\n        neuro_mean: float\n            Mean LOS for complex neurological patients.\n        neuro_sd: float\n            SD of LOS for complex neurological patients.\n        other_mean: float\n            Mean LOS for other patient types.\n        other_sd: float\n            SD of LOS for other patient types.\n        \"\"\"\n        self.stroke_noesd = {\n            \"mean\": stroke_no_esd_mean,\n            \"sd\": stroke_no_esd_sd\n        }\n        self.stroke_esd = {\n            \"mean\": stroke_esd_mean,\n            \"sd\": stroke_esd_sd\n        }\n        self.tia = {\n            \"mean\": tia_mean,\n            \"sd\": tia_sd\n        }\n        self.neuro = {\n            \"mean\": neuro_mean,\n            \"sd\": neuro_sd\n        }\n        self.other = {\n            \"mean\": other_mean,\n            \"sd\": other_sd\n        }\n\n\nclass ASURouting(RestrictAttributes):\n    \"\"\"\n    Probabilities of each patient type being transferred from the acute\n    stroke unit (ASU) to other destinations.\n\n    Attributes\n    ----------\n    stroke: dict\n        Routing probabilities for stroke patients.\n    tia: dict\n        Routing probabilities for transient ischemic attack (TIA) patients.\n    neuro: dict\n        Routing probabilities for complex neurological patients.\n    other: dict\n        Routing probabilities for other patients.\n    \"\"\"\n    def __init__(\n        self,\n        # Stroke patients\n        stroke_rehab=0.24, stroke_esd=0.13, stroke_other=0.63,\n        # TIA patients\n        tia_rehab=0.01, tia_esd=0.01, tia_other=0.98,\n        # Complex neurological patients\n        neuro_rehab=0.11, neuro_esd=0.05, neuro_other=0.84,\n        # Other patients\n        other_rehab=0.05, other_esd=0.10, other_other=0.85\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        stroke_rehab: float\n            Stroke patient to rehabilitation unit.\n        stroke_esd: float\n            Stroke patient to early support discharge (ESD) services.\n        stroke_other: float\n            Stroke patient to other destinations (e.g., own home, care\n            home, mortality).\n        tia_rehab: float\n            TIA patient to rehabilitation unit.\n        tia_esd: float\n            TIA patient to ESD.\n        tia_other: float\n            TIA patient to other destinations.\n        neuro_rehab: float\n            Complex neurological patient to rehabilitation unit.\n        neuro_esd: float\n            Complex neurological patient to ESD.\n        neuro_other: float\n            Complex neurological patient to other destinations.\n        other_rehab: float\n            Other patient type to rehabilitation unit.\n        other_esd: float\n            Other patient type to ESD.\n        other_other: float\n            Other patient type to other destinations.\n        \"\"\"\n        self.stroke = {\n            \"rehab\": stroke_rehab,\n            \"esd\": stroke_esd,\n            \"other\": stroke_other\n        }\n        self.tia = {\n            \"rehab\": tia_rehab,\n            \"esd\": tia_esd,\n            \"other\": tia_other\n        }\n        self.neuro = {\n            \"rehab\": neuro_rehab,\n            \"esd\": neuro_esd,\n            \"other\": neuro_other\n        }\n        self.other = {\n            \"rehab\": other_rehab,\n            \"esd\": other_esd,\n            \"other\": other_other\n        }\n\n\nclass RehabRouting(RestrictAttributes):\n    \"\"\"\n    Probabilities of each patient type being transferred from the rehabiliation\n    unit to other destinations.\n\n    Attributes\n    ----------\n    stroke: dict\n        Routing probabilities for stroke patients.\n    tia: dict\n        Routing probabilities for transient ischemic attack (TIA) patients.\n    neuro: dict\n        Routing probabilities for complex neurological patients.\n    other: dict\n        Routing probabilities for other patients.\n    \"\"\"\n    def __init__(\n        self,\n        # Stroke patients\n        stroke_esd=0.40, stroke_other=0.60,\n        # TIA patients\n        tia_esd=0, tia_other=1,\n        # Complex neurological patients\n        neuro_esd=0.09, neuro_other=0.91,\n        # Other patients\n        other_esd=0.13, other_other=0.88\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        stroke_esd: float\n            Stroke patient to early support discharge (ESD) services.\n        stroke_other: float\n            Stroke patient to other destinations (e.g., own home, care home,\n            mortality).\n        tia_esd: float\n            TIA patient to ESD.\n        tia_other: float\n            TIA patient to other destinations.\n        neuro_esd: float\n            Complex neurological patient to ESD.\n        neuro_other: float\n            Complex neurological patient to other destinations.\n        other_esd: float\n            Other patient type to ESD.\n        other_other: float\n            Other patient type to other destinations.\n        \"\"\"\n        self.stroke = {\n            \"esd\": stroke_esd,\n            \"other\": stroke_other\n        }\n        self.tia = {\n            \"esd\": tia_esd,\n            \"other\": tia_other\n        }\n        self.neuro = {\n            \"esd\": neuro_esd,\n            \"other\": neuro_other\n        }\n        self.other = {\n            \"esd\": other_esd,\n            \"other\": other_other\n        }\n\n\nclass Param(RestrictAttributes):\n    \"\"\"\n    Default parameters for simulation.\n    \"\"\"\n    def __init__(\n        self,\n        asu_arrivals=ASUArrivals(),\n        rehab_arrivals=RehabArrivals(),\n        asu_los=ASULOS(),\n        rehab_los=RehabLOS(),\n        asu_routing=ASURouting(),\n        rehab_routing=RehabRouting(),\n        warm_up_period=365*3,  # 3 years\n        data_collection_period=365*5,  # 5 years\n        number_of_runs=150,\n        audit_interval=1,\n        cores=1,\n        log_to_console=False,\n        log_to_file=False\n    ):\n        \"\"\"\n        Initialise a parameter set for the simulation.\n\n        Parameters\n        ----------\n        asu_arrivals: ASUArrivals\n            Arrival rates to the acute stroke unit (ASU) in days.\n        rehab_arrivals: RehabArrivals\n            Arrival rates to the rehabilitation unit in days.\n        asu_los: ASULOS\n            Length of stay (LOS) distributions for patients in the ASU in days.\n        rehab_los: RehabLOS\n            LOS distributions for patients in the rehabilitation unit in days.\n        asu_routing: ASURouting\n            Transfer probabilities from the ASU to other destinations.\n        rehab_routing: RehabRouting\n            Transfer probabilities from the rehabilitation unit to other\n            destinations.\n        warm_up_period: int\n            Length of the warm-up period.\n        data_collection_period: int\n            Length of the data collection period.\n        number_of_runs: int\n            The number of runs (i.e. replications), defining how many times to\n            re-run the simulation (with different random numbers).\n        audit_interval: float\n            Frequency of simulation audits in days.\n        cores: int\n            Number of CPU cores to use for parallel execution. Set to desired\n            number, or to -1 to use all available cores. For sequential\n            execution, set to 1.\n        log_to_console: boolean\n            Whether to print log messages to the console.\n        log_to_file: boolean\n            Whether to save log to a file.\n        \"\"\"\n        # Set parameters\n        self.asu_arrivals = asu_arrivals\n        self.rehab_arrivals = rehab_arrivals\n        self.asu_los = asu_los\n        self.rehab_los = rehab_los\n        self.asu_routing = asu_routing\n        self.rehab_routing = rehab_routing\n        self.warm_up_period = warm_up_period\n        self.data_collection_period = data_collection_period\n        self.number_of_runs = number_of_runs\n        self.audit_interval = audit_interval\n        self.cores = cores\n\n        # Set up logger\n        self.logger = SimLogger(log_to_console=log_to_console,\n                                log_to_file=log_to_file)\n\n    def check_param_validity(self):\n        \"\"\"\n        Check the validity of the provided parameters.\n\n        Validates all simulation parameters to ensure they meet requirements:\n        - Warm-up period and data collection period must be &gt;= 0\n        - Number of runs and audit interval must be &gt; 0\n        - Arrival rates must be &gt;= 0\n        - Length of stay parameters must be &gt;= 0\n        - Routing probabilities must sum to 1 and be between 0 and 1\n\n        Raises\n        ------\n        ValueError\n            If any parameter fails validation with a descriptive error message.\n        \"\"\"\n        # Validate parameters that must be &gt;= 0\n        for param in [\"warm_up_period\", \"data_collection_period\"]:\n            self.validate_param(\n                param, lambda x: x &gt;= 0,\n                \"must be greater than or equal to 0\")\n\n        # Validate parameters that must be &gt; 0\n        for param in [\"number_of_runs\", \"audit_interval\"]:\n            self.validate_param(\n                param, lambda x: x &gt; 0,\n                \"must be greater than 0\")\n\n        # Validate arrival parameters\n        for param in [\"asu_arrivals\", \"rehab_arrivals\"]:\n            self.validate_nested_param(\n                param, lambda x: x &gt;= 0,\n                \"must be greater than 0\")\n\n        # Validate length of stay parameters\n        for param in [\"asu_los\", \"rehab_los\"]:\n            self.validate_nested_param(\n                param, lambda x: x &gt;= 0,\n                \"must be greater than 0\", nested=True)\n\n        # Validate routing parameters\n        for param in [\"asu_routing\", \"rehab_routing\"]:\n            self.validate_routing(param)\n\n    def validate_param(self, param_name, condition, error_msg):\n        \"\"\"\n        Validate a single parameter against a condition.\n\n        Parameters\n        ----------\n        param_name: str\n            Name of the parameter being validated.\n        condition: callable\n            A function that returns True if the value is valid.\n        error_msg: str\n            Error message to display if validation fails.\n\n        Raises\n        ------\n        ValueError:\n            If the parameter fails the validation condition.\n        \"\"\"\n        value = getattr(self, param_name)\n        if not condition(value):\n            raise ValueError(\n                f\"Parameter '{param_name}' {error_msg}, but is: {value}\")\n\n    def validate_nested_param(\n        self, obj_name, condition, error_msg, nested=False\n    ):\n        \"\"\"\n        Validate parameters within a nested object structure.\n\n        Parameters\n        ----------\n        obj_name: str\n            Name of the object containing parameters.\n        condition: callable\n            A function that returns True if the value is valid.\n        error_msg: str\n            Error message to display if validation fails.\n        nested: bool, optional\n            If True, validates parameters in a doubly-nested structure. If\n            False, validates parameters in a singly-nested structure.\n\n        Raises\n        ------\n        ValueError:\n            If any nested parameter fails the validation condition.\n        \"\"\"\n        obj = getattr(self, obj_name)\n        for key, value in vars(obj).items():\n            if key == \"_initialised\":\n                continue\n            if nested:\n                for sub_key, sub_value in value.items():\n                    if not condition(sub_value):\n                        raise ValueError(\n                            f\"Parameter '{sub_key}' for '{key}' in \" +\n                            f\"'{obj_name}' {error_msg}, but is: {sub_value}\")\n            else:\n                if not condition(value):\n                    raise ValueError(\n                        f\"Parameter '{key}' from '{obj_name}' {error_msg}, \" +\n                        f\"but is: {value}\")\n\n    def validate_routing(self, obj_name):\n        \"\"\"\n        Validate routing probability parameters.\n\n        Performs two validations:\n        1. Checks that all probabilities for each routing option sum to 1.\n        2. Checks that individual probabilities are between 0 and 1 inclusive.\n\n        Parameters\n        ----------\n        obj_name: str\n            Name of the routing object.\n\n        Raises\n        ------\n        ValueError:\n            If the probabilities don't sum to 1, or if any probability is\n            outside [0,1].\n        \"\"\"\n        obj = getattr(self, obj_name)\n        for key, value in vars(obj).items():\n            if key == \"_initialised\":\n                continue\n\n            # Check that probabilities sum to 1\n            # Note: In the article, rehab other is 88% and 13%, so have\n            # allowed deviation of 1%\n            total_prob = sum(value.values())\n            if total_prob &lt; 0.99 or total_prob &gt; 1.01:\n                raise ValueError(\n                    f\"Routing probabilities for '{key}' in '{obj_name}' \" +\n                    f\"should sum to apx. 1 but sum to: {total_prob}\")\n\n            # Check that probabilities are between 0 and 1\n            for sub_key, sub_value in value.items():\n                if sub_value &lt; 0 or sub_value &gt; 1:\n                    raise ValueError(\n                        f\"Parameter '{sub_key}' for '{key}' in '{obj_name}'\" +\n                        f\"must be between 0 and 1, but is: {sub_value}\")\n\n\n\n\n\n\nuse function or class\nfeed that as input to model\nvalidation in the model code (for functions) or within class (either direct within or super class)",
    "crumbs": [
      "Model inputs",
      "Parameters from script"
    ]
  },
  {
    "objectID": "pages/inputs/parameters_script.html#what-not-to-do-hardcoding-parameters",
    "href": "pages/inputs/parameters_script.html#what-not-to-do-hardcoding-parameters",
    "title": "Parameters from script",
    "section": "",
    "text": "Hardcoding means writing parameter values directly into your code. For example:\n\ndef model():\n    # Hardcoded parameter values\n    interarrival_time = 5\n    consultation_time = 20\n    transfer_prob = 0.3\n    # ...rest of the model...\n\nThis makes it very difficult to change the values. Modellers might choose to‚Ä¶\n1. Edit parameters directly in the script.\nThis involves manually changing parameter values in the script each time you want to run a new scenario. Problems with this approach include:\n\n‚ùå Not a reproducible analytical pipeline. Alot of manual intervention is required to re-run the model with different parameters.\n‚ùå Error-prone. It would be easy to make mistakes or forget to update all relevant values.\n‚ùå Parameters can get lost. If you lost your notes or forget to record what you used, you won‚Äôt know what values were used for past runs.\n\n2. Duplicate scripts for each scenario.\nThis involves copying the entire script, changing parameters in each copy, and running them separately. Problems with this approach include:\n\n‚ùå Code duplication. This means any changes - like bug fixes or improvements to the model - must be made to every copy. This is tedious and there is a risk that some copies are missed or updated incorrectly.\n‚ùå Hard to keep track. With multiple script copies, it can become difficult to know which scripts correspond to which scenarios, and which parameters were used (as have to delve into the model code to identify them).",
    "crumbs": [
      "Model inputs",
      "Parameters from script"
    ]
  },
  {
    "objectID": "pages/inputs/parameters_script.html#a-slight-improvement-global-parameters",
    "href": "pages/inputs/parameters_script.html#a-slight-improvement-global-parameters",
    "title": "Parameters from script",
    "section": "",
    "text": "A better (but still limited) approach is to define all parameters at the top of your script:\nThey‚Äôre no longer hard coded. Can see what parameters were used, easier when changing.\n\n# Parameters for base case\nINTERARRIVAL_TIME = 5\nCONSULTATION_TIME = 20\nTRANSFER_PROB = 0.3\n\ndef model():\n    # Use the global parameters\n    ...\n\nFor scenarios, you would define the same global variables with alternative values:\n\n# Scenario 1\nINTERARRIVAL_TIME = 6\nCONSULTATION_TIME = 20\nTRANSFER_PROB = 0.3\n# Scenario 2\nINTERARRIVAL_TIME = 5\nCONSULTATION_TIME = 20\nTRANSFER_PROB = 0.4\n\nThe improvements are that parameters are:\n\n‚úÖ No longer hardcoded. Within the model, it refers to the variable name (e.g.¬†INTERARRIVAL_TIME) rather than a specific value (e.g.¬†5), which means we are able to now list the values in one outside the model logic.\n‚úÖ Centralised. All parameters are in one place, making them easier to find and change.\n\nHowever, there are still several disadvantages:\n\n‚ùå Still inflexible. In order to re-run the model with different scenarios, you would still need to do the approaches above - editing code directly or duplicating scripts for each scenario.\n‚ùå Not scalable. As the number of scenarios or parameters grows, managing all these global variables becomes messy.",
    "crumbs": [
      "Model inputs",
      "Parameters from script"
    ]
  },
  {
    "objectID": "pages/inputs/parameters_script.html#managing-parameters-in-des-two-critical-practices",
    "href": "pages/inputs/parameters_script.html#managing-parameters-in-des-two-critical-practices",
    "title": "Parameters from script",
    "section": "",
    "text": "To manage parameters effectively, you need to:\n\nGroup parameters into a dedicated object.\nPass these objects explicitly to your model.\n\nWhy?\n\n‚úÖ Clear parameter sets. Every scenario has its own object with all the parameters needed. This can be easily viewed, and comes in handy in logs (?@sec-logs) to easily print a copy of all parameters used for a scenario.\n‚úÖ No global variables. By explicitly passing our parameters, we avoid accidental parameter reuse between scenarios (which is a possibility with global variables!).\n‚úÖ Fewer inputs. If all parameters are in one object, then we can just pass that as a single input to our model function/class, reducing the number of arguments we need to pass.\n\n\nIt‚Äôs important to use both of these practices.\nIf you only do option 1 (group parameters, but use as globals), parameters might accidentally be modified elsewhere, or one scenario‚Äôs parameters might affect another‚Äôs.\n# BAD: Parameters are grouped but still global\nglobal_params = Parameters()\n\ndef simulate():\n    # Uses global_params.interarrival_time... üò¨\n    ...\nIf you only do option 2 (pass parameters, but don‚Äôt group them), you end up with messy, error-prone code that‚Äôs hard to maintain:\n# BAD: Parameters are passed but disorganized\ndef simulate(interarrival_time, consultation_time, transfer_prob, ...):\n    # 10+ parameters? Hard to track!\n    ...\n\nThere are three implementation options: dictionary, function or class.\n\n\n\n\nbase_params = {\n    \"interarrival_time\": 5,\n    \"consultation_time\": 20,\n    \"transfer_prob\": 0.3,\n}\n\n# Create a scenario by copying and tweaking only what's needed\nscenario1 = base_params.copy()\nscenario1[\"interarrival_time\"] = 6\nscenario2 = base_params.copy()\nscenario2[\"transfer_prob\"] = 0.4\n\n\n\n\n\n\ndef create_params(interarrival_time=5, consultation_time=20, transfer_prob=0.3):\n    return {\n        \"interarrival_time\": interarrival_time,\n        \"consultation_time\": consultation_time,\n        \"transfer_prob\": transfer_prob\n    }\n\nbase_params = create_params()\nscenario1 = create_params(interarrival_time=6)\nscenario2 = create_params(transfer_prob=0.4)\n\n\n\n\n\n\nclass Parameters:\n    def __init__(self, interarrival_time=5, consultation_time=20, transfer_prob=0.3):\n        self.interarrival_time = interarrival_time\n        self.consultation_time = consultation_time\n        self.transfer_prob = transfer_prob\n\n    def __repr__(self):\n        return (f\"Parameters(interarrival_time={self.interarrival_time}, \"\n                f\"consultation_time={self.consultation_time}, \"\n                f\"transfer_prob={self.transfer_prob})\")\n\n# Base case\nbase_params = Parameters()\n\n# Scenario 1: Only change interarrival_time\nscenario1 = Parameters(interarrival_time=6)\n\n# Scenario 2: Only change transfer_prob\nscenario2 = Parameters(transfer_prob=0.4)\n\n\n\n\n\nThe most robust approach is to use a function or class to manage your parameters.\n\n‚úÖ Functions and classes make it easy to create variations for different scenarios, since you simply change the inputs when you define a new scenario. For example, you can create a new scenario by only specifying the parameter you want to change, while all other parameters remain at their default values.\n‚ùå With a dictionary, you have to make a copy of the base dictionary and then manually change individual values for each scenario. This can become cumbersome as the number of parameters or scenarios grows - and is just a bit more clunky!\n\nYour choice may be further informed by options for parameter validation, where classes can be superior as you can incorporate validation within the class, as discussed on the page ‚Äú?@sec-param_validation‚Äù.",
    "crumbs": [
      "Model inputs",
      "Parameters from script"
    ]
  },
  {
    "objectID": "pages/inputs/parameters_script.html#handling-a-large-number-of-parameters",
    "href": "pages/inputs/parameters_script.html#handling-a-large-number-of-parameters",
    "title": "Parameters from script",
    "section": "",
    "text": "You may need to manage many parameters - for example, if you have several patient types and/or units each with their own arrival times, resource times, and so on.\nWe have suggested a few strategies you could use‚Ä¶\n\n\n\nThis can be convenient for smaller models, though can get unwieldly as the number of parameters grow, including potentially quick long parameter names!\nFunction example:\n\ndef create_params(\n    adult_interarrival=5, adult_consultation=20, adult_transfer=0.3,\n    child_interarrival=7, child_consultation=15, child_transfer=0.2,\n    elderly_interarrival=10, elderly_consultation=30, elderly_transfer=0.5\n):\n    return {\n        \"adult\": {\n            \"interarrival_time\": adult_interarrival,\n            \"consultation_time\": adult_consultation,\n            \"transfer_prob\": adult_transfer\n        },\n        \"child\": {\n            \"interarrival_time\": child_interarrival,\n            \"consultation_time\": child_consultation,\n            \"transfer_prob\": child_transfer\n        },\n        \"elderly\": {\n            \"interarrival_time\": elderly_interarrival,\n            \"consultation_time\": elderly_consultation,\n            \"transfer_prob\": elderly_transfer\n        }\n    }\n\nClass example:\n\nclass Parameters:\n    def __init__(\n        self,\n        adult_interarrival=5, adult_consultation=20, adult_transfer=0.3,\n        child_interarrival=7, child_consultation=15, child_transfer=0.2,\n        elderly_interarrival=10, elderly_consultation=30, elderly_transfer=0.5\n    ):\n        # Adult parameters\n        self.adult_interarrival = adult_interarrival\n        self.adult_consultation = adult_consultation\n        self.adult_transfer = adult_transfer\n        # Child parameters\n        self.child_interarrival = child_interarrival\n        self.child_consultation = child_consultation\n        self.child_transfer = child_transfer\n        # Elderly parameters\n        self.elderly_interarrival = elderly_interarrival\n        self.elderly_consultation = elderly_consultation\n        self.elderly_transfer = elderly_transfer\n\n\n\n\n\nAlternatively, you can split parameters into logical groups (e.g.¬†patient type, parameter type), each with it‚Äôs own function or class. These are then combined into single parameter set.\nWith a large number of parameters, this keeps each individual function/class simpler. seperate means its simpler with the inputs and stuff, just adult child elderly also, if doing validation, can do for each subclass\nFunction example:\n\ndef create_arrivals(adult=5, child=7, elderly=10):\n    return {\n        \"adult\": adult,\n        \"child\": child,\n        \"elderly\": elderly\n    }\n\ndef create_consultations(adult=20, child=15, elderly=30):\n    return {\n        \"adult\": adult,\n        \"child\": child,\n        \"elderly\": elderly\n    }\n\ndef create_transfers(adult=0.3, child=0.2, elderly=0.5):\n    return {\n        \"adult\": adult,\n        \"child\": child,\n        \"elderly\": elderly\n    }\n\ndef create_parameters(\n    arrivals=create_arrivals(),\n    consultations=create_consultations(),\n    transfers=create_transfers()\n):\n    return {\n        \"arrivals\": arrivals,\n        \"consultations\": consultations,\n        \"transfers\": transfers\n    }\n\nClass example:\n\n\n\nclass Arrivals:\n    def __init__(self, adult=5, child=7, elderly=10):\n        self.adult = adult\n        self.child = child\n        self.elderly = elderly\n\n\nclass Consultations:\n    def __init__(self, adult=20, child=15, elderly=30):\n        self.adult = adult\n        self.child = child\n        self.elderly = elderly\n\n\nclass Transfers:\n    def __init__(self, adult=0.3, child=0.2, elderly=0.5):\n        self.adult = adult\n        self.child = child\n        self.elderly = elderly\n\n\nclass Parameters():\n    def __init__(\n        self,\n        arrivals=Arrivals(),\n        consultations=Consultations(),\n        transfers=Transfers()\n    ):\n        self.arrivals = arrivals\n        self.consultations = consultations\n        self.transfers = transfers",
    "crumbs": [
      "Model inputs",
      "Parameters from script"
    ]
  },
  {
    "objectID": "pages/inputs/parameters_script.html#full-code",
    "href": "pages/inputs/parameters_script.html#full-code",
    "title": "Parameters from script",
    "section": "",
    "text": "for the ?@sec-examples‚Ä¶ parameter bits from those‚Ä¶ copy in full to have the parameters for models examples‚Ä¶\nnurse visit simulation does classes, with validation directly in class\nstroke pathway simulation has more parma so multiple classes combined, with validation from super class\n\n\n\nShow/Hide example 1: Nurse visit simulation\n\ntouch simulation/model.py\n# pylint: disable=too-many-instance-attributes,too-few-public-methods\nclass Param:\n    \"\"\"\n    Default parameters for simulation.\n\n    Attributes are described in initialisation docstring.\n    \"\"\"\n    # pylint: disable=too-many-arguments,too-many-positional-arguments\n    def __init__(\n        self,\n        patient_inter=4,\n        mean_n_consult_time=10,\n        number_of_nurses=5,\n        warm_up_period=1440*27,  # 27 days\n        data_collection_period=1440*30,  # 30 days\n        number_of_runs=31,\n        audit_interval=120,  # every 2 hours\n        scenario_name=0,\n        cores=-1,\n        logger=SimLogger(log_to_console=False, log_to_file=False)\n    ):\n        \"\"\"\n        Initialise instance of parameters class.\n\n        Arguments:\n            patient_inter (float):\n                Mean inter-arrival time between patients in minutes.\n            mean_n_consult_time (float):\n                Mean nurse consultation time in minutes.\n            number_of_nurses (float):\n                Number of available nurses.\n            warm_up_period (int):\n                Duration of the warm-up period in minutes - running simulation\n                but not yet collecting results.\n            data_collection_period (int):\n                Duration of data collection period in minutes (also known as\n                measurement interval) - which begins after any warm-up period.\n            number_of_runs (int):\n                The number of runs (i.e. replications), defining how many\n                times to re-run the simulation (with different random numbers).\n            audit_interval (int):\n                How frequently to audit resource utilisation, in minutes.\n            scenario_name (int|float|string):\n                Label for the scenario.\n            cores (int):\n                Number of CPU cores to use for parallel execution. Set to\n                desired number, or to -1 to use all available cores. For\n                sequential execution, set to 1 (default).\n            logger (logging.Logger):\n                The logging instance used for logging messages.\n        \"\"\"\n        # Disable restriction on attribute modification during initialisation\n        object.__setattr__(self, '_initialising', True)\n\n        self.patient_inter = patient_inter\n        self.mean_n_consult_time = mean_n_consult_time\n        self.number_of_nurses = number_of_nurses\n        self.warm_up_period = warm_up_period\n        self.data_collection_period = data_collection_period\n        self.number_of_runs = number_of_runs\n        self.audit_interval = audit_interval\n        self.scenario_name = scenario_name\n        self.cores = cores\n        self.logger = logger\n\n        # Re-enable attribute checks after initialisation\n        object.__setattr__(self, '_initialising', False)\n\n    def __setattr__(self, name, value):\n        \"\"\"\n        Prevent addition of new attributes.\n\n        This method overrides the default `__setattr__` behavior to restrict\n        the addition of new attributes to the instance. It allows modification\n        of existing attributes but raises an `AttributeError` if an attempt is\n        made to create a new attribute. This ensures that accidental typos in\n        attribute names do not silently create new attributes.\n\n        Arguments:\n            name (str):\n                The name of the attribute to set.\n            value (Any):\n                The value to assign to the attribute.\n\n        Raises:\n            AttributeError:\n                If `name` is not an existing attribute and an attempt is made\n                to add it to the instance.\n        \"\"\"\n        # Skip the check if the object is still initialising\n        # pylint: disable=maybe-no-member\n        if hasattr(self, '_initialising') and self._initialising:\n            super().__setattr__(name, value)\n        else:\n            # Check if attribute of that name is already present\n            if name in self.__dict__:\n                super().__setattr__(name, value)\n            else:\n                raise AttributeError(\n                    f'Cannot add new attribute \"{name}\" - only possible to ' +\n                    f'modify existing attributes: {self.__dict__.keys()}')\n\n\n\n\nShow/Hide example 2: stroke pathway simulation\n\nMake file.\ntouch simulation/parameters.py\nCopy in.\n\"\"\"\nStroke pathway simulation parameters.\n\nIt includes arrival rates, length of stay distributions, and routing\nprobabilities between different care settings.\n\"\"\"\n\nfrom simulation.logging import SimLogger\n\n\nclass RestrictAttributesMeta(type):\n    \"\"\"\n    Metaclass for attribute restriction.\n\n    A metaclass modifies class construction. It intercepts instance creation\n    via __call__, adding the _initialised flag after __init__ completes. This\n    is later used by RestrictAttributes to enforce attribute restrictions.\n    \"\"\"\n    def __call__(cls, *args, **kwargs):\n        # Create instance using the standard method\n        instance = super().__call__(*args, **kwargs)\n        # Set the \"_initialised\" flag to True, marking end of initialisation\n        instance.__dict__[\"_initialised\"] = True\n        return instance\n\n\nclass RestrictAttributes(metaclass=RestrictAttributesMeta):\n    \"\"\"\n    Base class that prevents the addition of new attributes after\n    initialisation.\n\n    This class uses RestrictAttributesMeta as its metaclass to implement\n    attribute restriction. It allows for safe initialisation of attributes\n    during the __init__ method, but prevents the addition of new attributes\n    afterwards.\n\n    The restriction is enforced through the custom __setattr__ method, which\n    checks if the attribute already exists before allowing assignment.\n    \"\"\"\n    def __setattr__(self, name, value):\n        \"\"\"\n        Prevent addition of new attributes.\n\n        Parameters\n        ----------\n        name: str\n            The name of the attribute to set.\n        value: any\n            The value to assign to the attribute.\n\n        Raises\n        ------\n        AttributeError\n            If `name` is not an existing attribute and an attempt is made\n            to add it to the class instance.\n        \"\"\"\n        # Check if the instance is initialised and the attribute doesn\"t exist\n        if hasattr(self, \"_initialised\") and not hasattr(self, name):\n            # Get a list of existing attributes for the error message\n            existing = \", \".join(self.__dict__.keys())\n            raise AttributeError(\n                f\"Cannot add new attribute '{name}' - only possible to \" +\n                f\"modify existing attributes: {existing}.\"\n            )\n        # If checks pass, set the attribute using the standard method\n        object.__setattr__(self, name, value)\n\n\nclass ASUArrivals(RestrictAttributes):\n    \"\"\"\n    Arrival rates for the acute stroke unit (ASU) by patient type.\n\n    These are the average time intervals (in days) between new admissions.\n    For example, a value of 1.2 means a new admission every 1.2 days.\n    \"\"\"\n    def __init__(self, stroke=1.2, tia=9.3, neuro=3.6, other=3.2):\n        \"\"\"\n        Parameters\n        ----------\n        stroke: float\n            Stroke patient.\n        tia: float\n            Transient ischaemic attack (TIA) patient.\n        neuro: float\n            Complex neurological patient.\n        other: float\n            Other patient types (including medical outliers).\n        \"\"\"\n        self.stroke = stroke\n        self.tia = tia\n        self.neuro = neuro\n        self.other = other\n\n\nclass RehabArrivals(RestrictAttributes):\n    \"\"\"\n    Arrival rates for the rehabiliation unit by patient type.\n\n    These are the average time intervals (in days) between new admissions.\n    For example, a value of 21.8 means a new admission every 21.8 days.\n    \"\"\"\n    def __init__(self, stroke=21.8, neuro=31.7, other=28.6):\n        \"\"\"\n        Parameters\n        ----------\n        stroke: float\n            Stroke patient.\n        neuro: float\n            Complex neurological patient.\n        other: float\n            Other patient types.\n        \"\"\"\n        self.stroke = stroke\n        self.neuro = neuro\n        self.other = other\n\n\nclass ASULOS(RestrictAttributes):\n    \"\"\"\n    Mean and standard deviation (SD) of length of stay (LOS) in days in the\n    acute stroke unit (ASU) by patient type.\n\n    Attributes\n    ----------\n    stroke_noesd: dict\n        Mean and SD of LOS for stroke patients without early support discharge.\n    stroke_esd: dict\n        Mean and SD of LOS for stroke patients with early support discharge.\n    tia: dict\n        Mean and SD of LOS for transient ischemic attack (TIA) patients.\n    neuro: dict\n        Mean and SD of LOS for complex neurological patients.\n    other: dict\n        Mean and SD of LOS for other patients.\n    \"\"\"\n    def __init__(\n        self,\n        stroke_no_esd_mean=7.4, stroke_no_esd_sd=8.61,\n        stroke_esd_mean=4.6, stroke_esd_sd=4.8,\n        tia_mean=1.8, tia_sd=2.3,\n        neuro_mean=4.0, neuro_sd=5.0,\n        other_mean=3.8, other_sd=5.2\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        stroke_no_esd_mean: float\n            Mean LOS for stroke patients without early support discharge (ESD)\n            services.\n        stroke_no_esd_sd: float\n            SD of LOS for stroke patients without ESD.\n        stroke_esd_mean: float\n            Mean LOS for stroke patients with ESD.\n        stroke_esd_sd: float\n            SD of LOS for stroke patients with ESD.\n        tia_mean: float\n            Mean LOS for TIA patients.\n        tia_sd: float\n            SD of LOS for TIA patients.\n        neuro_mean: float\n            Mean LOS for complex neurological patients.\n        neuro_sd: float\n            SD of LOS for complex neurological patients.\n        other_mean: float\n            Mean LOS for other patient types.\n        other_sd: float\n            SD of LOS for other patient types.\n        \"\"\"\n        self.stroke_noesd = {\n            \"mean\": stroke_no_esd_mean,\n            \"sd\": stroke_no_esd_sd\n        }\n        self.stroke_esd = {\n            \"mean\": stroke_esd_mean,\n            \"sd\": stroke_esd_sd\n        }\n        self.tia = {\n            \"mean\": tia_mean,\n            \"sd\": tia_sd\n        }\n        self.neuro = {\n            \"mean\": neuro_mean,\n            \"sd\": neuro_sd\n        }\n        self.other = {\n            \"mean\": other_mean,\n            \"sd\": other_sd\n        }\n\n\nclass RehabLOS(RestrictAttributes):\n    \"\"\"\n    Mean and standard deviation (SD) of length of stay (LOS) in days in the\n    rehabilitation unit by patient type.\n\n    Attributes\n    ----------\n    stroke_noesd: dict\n        Mean and SD of LOS for stroke patients without early support discharge.\n    stroke_esd: dict\n        Mean and SD of LOS for stroke patients with early support discharge.\n    tia: dict\n        Mean and SD of LOS for transient ischemic attack (TIA) patients.\n    neuro: dict\n        Mean and SD of LOS for complex neurological patients.\n    other: dict\n        Mean and SD of LOS for other patients.\n    \"\"\"\n    def __init__(\n        self,\n        stroke_no_esd_mean=28.4, stroke_no_esd_sd=27.2,\n        stroke_esd_mean=30.3, stroke_esd_sd=23.1,\n        tia_mean=18.7, tia_sd=23.5,\n        neuro_mean=27.6, neuro_sd=28.4,\n        other_mean=16.1, other_sd=14.1\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        stroke_no_esd_mean: float\n            Mean LOS for stroke patients without early support discharge (ESD)\n            services.\n        stroke_no_esd_sd: float\n            SD of LOS for stroke patients without ESD.\n        stroke_esd_mean: float\n            Mean LOS for stroke patients with ESD.\n        stroke_esd_sd: float\n            SD of LOS for stroke patients with ESD.\n        tia_mean: float\n            Mean LOS for TIA patients.\n        tia_sd: float\n            SD of LOS for TIA patients.\n        neuro_mean: float\n            Mean LOS for complex neurological patients.\n        neuro_sd: float\n            SD of LOS for complex neurological patients.\n        other_mean: float\n            Mean LOS for other patient types.\n        other_sd: float\n            SD of LOS for other patient types.\n        \"\"\"\n        self.stroke_noesd = {\n            \"mean\": stroke_no_esd_mean,\n            \"sd\": stroke_no_esd_sd\n        }\n        self.stroke_esd = {\n            \"mean\": stroke_esd_mean,\n            \"sd\": stroke_esd_sd\n        }\n        self.tia = {\n            \"mean\": tia_mean,\n            \"sd\": tia_sd\n        }\n        self.neuro = {\n            \"mean\": neuro_mean,\n            \"sd\": neuro_sd\n        }\n        self.other = {\n            \"mean\": other_mean,\n            \"sd\": other_sd\n        }\n\n\nclass ASURouting(RestrictAttributes):\n    \"\"\"\n    Probabilities of each patient type being transferred from the acute\n    stroke unit (ASU) to other destinations.\n\n    Attributes\n    ----------\n    stroke: dict\n        Routing probabilities for stroke patients.\n    tia: dict\n        Routing probabilities for transient ischemic attack (TIA) patients.\n    neuro: dict\n        Routing probabilities for complex neurological patients.\n    other: dict\n        Routing probabilities for other patients.\n    \"\"\"\n    def __init__(\n        self,\n        # Stroke patients\n        stroke_rehab=0.24, stroke_esd=0.13, stroke_other=0.63,\n        # TIA patients\n        tia_rehab=0.01, tia_esd=0.01, tia_other=0.98,\n        # Complex neurological patients\n        neuro_rehab=0.11, neuro_esd=0.05, neuro_other=0.84,\n        # Other patients\n        other_rehab=0.05, other_esd=0.10, other_other=0.85\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        stroke_rehab: float\n            Stroke patient to rehabilitation unit.\n        stroke_esd: float\n            Stroke patient to early support discharge (ESD) services.\n        stroke_other: float\n            Stroke patient to other destinations (e.g., own home, care\n            home, mortality).\n        tia_rehab: float\n            TIA patient to rehabilitation unit.\n        tia_esd: float\n            TIA patient to ESD.\n        tia_other: float\n            TIA patient to other destinations.\n        neuro_rehab: float\n            Complex neurological patient to rehabilitation unit.\n        neuro_esd: float\n            Complex neurological patient to ESD.\n        neuro_other: float\n            Complex neurological patient to other destinations.\n        other_rehab: float\n            Other patient type to rehabilitation unit.\n        other_esd: float\n            Other patient type to ESD.\n        other_other: float\n            Other patient type to other destinations.\n        \"\"\"\n        self.stroke = {\n            \"rehab\": stroke_rehab,\n            \"esd\": stroke_esd,\n            \"other\": stroke_other\n        }\n        self.tia = {\n            \"rehab\": tia_rehab,\n            \"esd\": tia_esd,\n            \"other\": tia_other\n        }\n        self.neuro = {\n            \"rehab\": neuro_rehab,\n            \"esd\": neuro_esd,\n            \"other\": neuro_other\n        }\n        self.other = {\n            \"rehab\": other_rehab,\n            \"esd\": other_esd,\n            \"other\": other_other\n        }\n\n\nclass RehabRouting(RestrictAttributes):\n    \"\"\"\n    Probabilities of each patient type being transferred from the rehabiliation\n    unit to other destinations.\n\n    Attributes\n    ----------\n    stroke: dict\n        Routing probabilities for stroke patients.\n    tia: dict\n        Routing probabilities for transient ischemic attack (TIA) patients.\n    neuro: dict\n        Routing probabilities for complex neurological patients.\n    other: dict\n        Routing probabilities for other patients.\n    \"\"\"\n    def __init__(\n        self,\n        # Stroke patients\n        stroke_esd=0.40, stroke_other=0.60,\n        # TIA patients\n        tia_esd=0, tia_other=1,\n        # Complex neurological patients\n        neuro_esd=0.09, neuro_other=0.91,\n        # Other patients\n        other_esd=0.13, other_other=0.88\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        stroke_esd: float\n            Stroke patient to early support discharge (ESD) services.\n        stroke_other: float\n            Stroke patient to other destinations (e.g., own home, care home,\n            mortality).\n        tia_esd: float\n            TIA patient to ESD.\n        tia_other: float\n            TIA patient to other destinations.\n        neuro_esd: float\n            Complex neurological patient to ESD.\n        neuro_other: float\n            Complex neurological patient to other destinations.\n        other_esd: float\n            Other patient type to ESD.\n        other_other: float\n            Other patient type to other destinations.\n        \"\"\"\n        self.stroke = {\n            \"esd\": stroke_esd,\n            \"other\": stroke_other\n        }\n        self.tia = {\n            \"esd\": tia_esd,\n            \"other\": tia_other\n        }\n        self.neuro = {\n            \"esd\": neuro_esd,\n            \"other\": neuro_other\n        }\n        self.other = {\n            \"esd\": other_esd,\n            \"other\": other_other\n        }\n\n\nclass Param(RestrictAttributes):\n    \"\"\"\n    Default parameters for simulation.\n    \"\"\"\n    def __init__(\n        self,\n        asu_arrivals=ASUArrivals(),\n        rehab_arrivals=RehabArrivals(),\n        asu_los=ASULOS(),\n        rehab_los=RehabLOS(),\n        asu_routing=ASURouting(),\n        rehab_routing=RehabRouting(),\n        warm_up_period=365*3,  # 3 years\n        data_collection_period=365*5,  # 5 years\n        number_of_runs=150,\n        audit_interval=1,\n        cores=1,\n        log_to_console=False,\n        log_to_file=False\n    ):\n        \"\"\"\n        Initialise a parameter set for the simulation.\n\n        Parameters\n        ----------\n        asu_arrivals: ASUArrivals\n            Arrival rates to the acute stroke unit (ASU) in days.\n        rehab_arrivals: RehabArrivals\n            Arrival rates to the rehabilitation unit in days.\n        asu_los: ASULOS\n            Length of stay (LOS) distributions for patients in the ASU in days.\n        rehab_los: RehabLOS\n            LOS distributions for patients in the rehabilitation unit in days.\n        asu_routing: ASURouting\n            Transfer probabilities from the ASU to other destinations.\n        rehab_routing: RehabRouting\n            Transfer probabilities from the rehabilitation unit to other\n            destinations.\n        warm_up_period: int\n            Length of the warm-up period.\n        data_collection_period: int\n            Length of the data collection period.\n        number_of_runs: int\n            The number of runs (i.e. replications), defining how many times to\n            re-run the simulation (with different random numbers).\n        audit_interval: float\n            Frequency of simulation audits in days.\n        cores: int\n            Number of CPU cores to use for parallel execution. Set to desired\n            number, or to -1 to use all available cores. For sequential\n            execution, set to 1.\n        log_to_console: boolean\n            Whether to print log messages to the console.\n        log_to_file: boolean\n            Whether to save log to a file.\n        \"\"\"\n        # Set parameters\n        self.asu_arrivals = asu_arrivals\n        self.rehab_arrivals = rehab_arrivals\n        self.asu_los = asu_los\n        self.rehab_los = rehab_los\n        self.asu_routing = asu_routing\n        self.rehab_routing = rehab_routing\n        self.warm_up_period = warm_up_period\n        self.data_collection_period = data_collection_period\n        self.number_of_runs = number_of_runs\n        self.audit_interval = audit_interval\n        self.cores = cores\n\n        # Set up logger\n        self.logger = SimLogger(log_to_console=log_to_console,\n                                log_to_file=log_to_file)\n\n    def check_param_validity(self):\n        \"\"\"\n        Check the validity of the provided parameters.\n\n        Validates all simulation parameters to ensure they meet requirements:\n        - Warm-up period and data collection period must be &gt;= 0\n        - Number of runs and audit interval must be &gt; 0\n        - Arrival rates must be &gt;= 0\n        - Length of stay parameters must be &gt;= 0\n        - Routing probabilities must sum to 1 and be between 0 and 1\n\n        Raises\n        ------\n        ValueError\n            If any parameter fails validation with a descriptive error message.\n        \"\"\"\n        # Validate parameters that must be &gt;= 0\n        for param in [\"warm_up_period\", \"data_collection_period\"]:\n            self.validate_param(\n                param, lambda x: x &gt;= 0,\n                \"must be greater than or equal to 0\")\n\n        # Validate parameters that must be &gt; 0\n        for param in [\"number_of_runs\", \"audit_interval\"]:\n            self.validate_param(\n                param, lambda x: x &gt; 0,\n                \"must be greater than 0\")\n\n        # Validate arrival parameters\n        for param in [\"asu_arrivals\", \"rehab_arrivals\"]:\n            self.validate_nested_param(\n                param, lambda x: x &gt;= 0,\n                \"must be greater than 0\")\n\n        # Validate length of stay parameters\n        for param in [\"asu_los\", \"rehab_los\"]:\n            self.validate_nested_param(\n                param, lambda x: x &gt;= 0,\n                \"must be greater than 0\", nested=True)\n\n        # Validate routing parameters\n        for param in [\"asu_routing\", \"rehab_routing\"]:\n            self.validate_routing(param)\n\n    def validate_param(self, param_name, condition, error_msg):\n        \"\"\"\n        Validate a single parameter against a condition.\n\n        Parameters\n        ----------\n        param_name: str\n            Name of the parameter being validated.\n        condition: callable\n            A function that returns True if the value is valid.\n        error_msg: str\n            Error message to display if validation fails.\n\n        Raises\n        ------\n        ValueError:\n            If the parameter fails the validation condition.\n        \"\"\"\n        value = getattr(self, param_name)\n        if not condition(value):\n            raise ValueError(\n                f\"Parameter '{param_name}' {error_msg}, but is: {value}\")\n\n    def validate_nested_param(\n        self, obj_name, condition, error_msg, nested=False\n    ):\n        \"\"\"\n        Validate parameters within a nested object structure.\n\n        Parameters\n        ----------\n        obj_name: str\n            Name of the object containing parameters.\n        condition: callable\n            A function that returns True if the value is valid.\n        error_msg: str\n            Error message to display if validation fails.\n        nested: bool, optional\n            If True, validates parameters in a doubly-nested structure. If\n            False, validates parameters in a singly-nested structure.\n\n        Raises\n        ------\n        ValueError:\n            If any nested parameter fails the validation condition.\n        \"\"\"\n        obj = getattr(self, obj_name)\n        for key, value in vars(obj).items():\n            if key == \"_initialised\":\n                continue\n            if nested:\n                for sub_key, sub_value in value.items():\n                    if not condition(sub_value):\n                        raise ValueError(\n                            f\"Parameter '{sub_key}' for '{key}' in \" +\n                            f\"'{obj_name}' {error_msg}, but is: {sub_value}\")\n            else:\n                if not condition(value):\n                    raise ValueError(\n                        f\"Parameter '{key}' from '{obj_name}' {error_msg}, \" +\n                        f\"but is: {value}\")\n\n    def validate_routing(self, obj_name):\n        \"\"\"\n        Validate routing probability parameters.\n\n        Performs two validations:\n        1. Checks that all probabilities for each routing option sum to 1.\n        2. Checks that individual probabilities are between 0 and 1 inclusive.\n\n        Parameters\n        ----------\n        obj_name: str\n            Name of the routing object.\n\n        Raises\n        ------\n        ValueError:\n            If the probabilities don't sum to 1, or if any probability is\n            outside [0,1].\n        \"\"\"\n        obj = getattr(self, obj_name)\n        for key, value in vars(obj).items():\n            if key == \"_initialised\":\n                continue\n\n            # Check that probabilities sum to 1\n            # Note: In the article, rehab other is 88% and 13%, so have\n            # allowed deviation of 1%\n            total_prob = sum(value.values())\n            if total_prob &lt; 0.99 or total_prob &gt; 1.01:\n                raise ValueError(\n                    f\"Routing probabilities for '{key}' in '{obj_name}' \" +\n                    f\"should sum to apx. 1 but sum to: {total_prob}\")\n\n            # Check that probabilities are between 0 and 1\n            for sub_key, sub_value in value.items():\n                if sub_value &lt; 0 or sub_value &gt; 1:\n                    raise ValueError(\n                        f\"Parameter '{sub_key}' for '{key}' in '{obj_name}'\" +\n                        f\"must be between 0 and 1, but is: {sub_value}\")",
    "crumbs": [
      "Model inputs",
      "Parameters from script"
    ]
  },
  {
    "objectID": "pages/inputs/parameters_script.html#summary",
    "href": "pages/inputs/parameters_script.html#summary",
    "title": "Parameters from script",
    "section": "",
    "text": "use function or class\nfeed that as input to model\nvalidation in the model code (for functions) or within class (either direct within or super class)",
    "crumbs": [
      "Model inputs",
      "Parameters from script"
    ]
  },
  {
    "objectID": "pages/output_analysis/warmup.html",
    "href": "pages/output_analysis/warmup.html",
    "title": "Initialisation bias",
    "section": "",
    "text": "Initialisation bias",
    "crumbs": [
      "Output analysis",
      "Initialisation bias"
    ]
  },
  {
    "objectID": "pages/output_analysis/parallel.html",
    "href": "pages/output_analysis/parallel.html",
    "title": "Parallel processing",
    "section": "",
    "text": "Parallel processing\n\nüîó Reproducibility guidelines:\n\nHeather et al.¬†2025: Optimise model run time.\n\n\n(if using replications to get accurate estimates, then can be handy to run them in parallel)",
    "crumbs": [
      "Output analysis",
      "Parallel processing"
    ]
  },
  {
    "objectID": "pages/output_analysis/outputs.html",
    "href": "pages/output_analysis/outputs.html",
    "title": "Performance measures",
    "section": "",
    "text": "Performance measures\n\nüîó Reproducibility guidelines:\n\nHeather et al.¬†2025 (‚≠ê): Include code to calculate all required model outputs\nHeather et al.¬†2025: Ensure clarity and consistency in the model results tables.\nNHS Levels of RAP (ü•à): Data is handled and output in a Tidy data format.",
    "crumbs": [
      "Output analysis",
      "Performance measures"
    ]
  },
  {
    "objectID": "pages/reports/full_run.html",
    "href": "pages/reports/full_run.html",
    "title": "Full run",
    "section": "",
    "text": "Full run\n\nüîó Reproducibility guidelines:\n\nHeather et al.¬†2025 (‚≠ê): Ensure model parameters are correct.\nNHS Levels of RAP (ü•à): Outputs are produced by code with minimal manual intervention.",
    "crumbs": [
      "Reports & manuscripts",
      "Full run"
    ]
  }
]