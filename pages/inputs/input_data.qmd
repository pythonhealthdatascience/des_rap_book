---
bibliography: input_data_resources/references.bib
format:
  html:
    css: input_data_resources/styles.css
---

{{< include ../../scripts/language-selector.html >}}

# Input data management {#sec-input_data}

To construct a discrete-event simulation (DES), you first need **data** that reflects the system you want to model. In healthcare, this might mean you need to access healthcare records with patient arrival, service and departure times, for example. The quality of your simulation depends directly on the quality of your data. Key considerations include:

* **Accuracy**. Inaccurate data leads to inaccurate simulation results.
* **Sample size**. Small samples can give misleading results if they capture unusual periods, lack variability, or are affected by outliers.
* **Level of detail**. The data must be granular enough for your needs. For example, daily totals may be insufficient if you want to model hourly arrivals (although may still be possible if know distribution - see @sec-input_data-fitting)
* **Representative**. The data should reflect the current system. For instance, data from the COVID-19 period may not represent typical operations.

From this raw data, you calculate **parameters** such as the mean patient inter-arrival time or average length of stay. These parameters are used in the model - but not as fixed intervals. For example the model wouldn't have a patient arriving exactly every fix minutes.

This is because DES models are **stochastic**, which means they incorporate random variation, to reflect the inherent variability of real-world systems. To achieve this, the data are fit to probability distributions (e.g. exponential, lognormal, gamma) and then the simulation samples from these distributions to generate events.


![](input_data_resources/samples.png)

<br>

## Ensuring a reproducible analytical pipeline

Your reproducible analytical pipeline (RAP) should start from the **earliest data you access** - either with raw data (if you estimate parameters yourself) or with pre-defined parameters (if those are supplied). This ensures that every step in your process is transparent and reproducible. For example, if you obtain updated raw data, you can re-estimate parameters, check that your chosen distributions are still appropriate, and re-run the simulation.

**External sharing of the full RAP may not always be possible**. In healthcare simulations, the raw data may be sensitive or identifiable, and so cannot be shared due to privacy or ethical concerns. However, mainintaing a complete RAP internally is still essential for your **team or organisation** to ensure that the process is fully reproducible.

There are a few key files to consider for your RAP:

* Raw data
* Parameter estimation scripts
* Distribution fitting scripts
* Parameters

![](input_data_resources/input_files.png)

<br>

### Raw data

This is data which reflects system you will be simulating. It is used to estimate parameters and fit distributions for your simulation model. For example:

::: {.grey-table}

| ARRIVAL_DATE | ARRIVAL_TIME | SERVICE_DATE | SERVICE_TIME | DEPARTURE_DATE | DEPARTURE_TIME |
|--------------|--------------|--------------|--------------|----------------|----------------|
| 2025-01-01   | 0001         | 2025-01-01   | 0007         | 2025-01-01     | 0012           |
| 2025-01-01   | 0002         | 2025-01-01   | 0004         | 2025-01-01     | 0007           |
| 2025-01-01   | 0003         | 2025-01-01   | 0010         | 2025-01-01     | 0030           |
| 2025-01-01   | 0007         | 2025-01-01   | 0014         | 2025-01-01     | 0022           |

:::

#### Internal management of raw data

You should either keep **copies of the raw data** or, if the data is in a secure database or very large, and cannot be exported, clearly **describe how to access it** (including the database location, access permissions, and navigation instructions).

For both options, you should also document **when** you obtained the data, and relevant **metadata** (e.g. time period the data covers, number of records, any known issues or missing data). For example:

> "Data sourced from the XYZ database. Copies are available in this repository, or, to access directly, log in to the XYZ database and navigate to [path/to/data].
> Data covers January 2012 to December 2017, with [number] records. Note: [details on missing data, known issues, etc.].
> A copy of the data dictionary is available in the repository or online at [URL]."

You should keep a copy of the **data dictionary**. A data dictionary describes each field, its format, units, and any coding schemes used. If one is not provided for your data, then you should create your own, to ensure the raw data used for your simulation is clear and understandable. For example:

::: {.grey-table}

| Field          | Field name                  | Format           | Description                                           |
|----------------|----------------------------|------------------|-------------------------------------------------------|
| ARRIVAL_DATE   | CLINIC ARRIVAL DATE        | Date(CCYY-MM-DD) | The date on which the patient arrived at the clinic   |
| ARRIVAL_TIME   | CLINIC ARRIVAL TIME        | Time(HH:MM)      | The time at which the patient arrived at the clinic   |
| DEPARTURE_DATE | CLINIC DEPARTURE DATE      | Date(CCYY-MM-DD) | The date on which the patient left the clinic         |
| DEPARTURE_TIME | CLINIC DEPARTURE TIME      | Time(HH:MM)      | The time at which the patient left the clinic         |
| SERVICE_DATE   | NURSE SERVICE START DATE   | Date(CCYY-MM-DD) | The date on which the nurse consultation began        |
| SERVICE_TIME   | NURSE SERVICE START TIME   | Time(HH:MM)      | The time at which the nurse consultation began        |

:::

#### Sharing raw data

**If you are able to share raw data externally**:

:::{.pale-blue}

* ☑️ **Make the data openly available**, following FAIR principles (Findable, Accessible, Interoperable, Reusable).

* ☑️ **Include a data dictionary**.

* ☑️ **Deposit the data** in a trusted public archive (e.g., Zenodo, Figshare) or a code/data repository (e.g., GitHub, GitLab).

* ☑️ **Include an open data licence** (e.g., CC0, CC-BY) to clarify usage rights and restrictions.

* ☑️ **Provide a clear citation or DOI** for others to reference.

:::

**If you cannot share raw data**:

:::{.pale-blue}

* ☑️ **Describe the dataset** in your documentation (e.g. "*Patient records from XYZ database, covering 2012–2017, with fields for arrival, service, and departure times.*").

* ☑️ **Share the data dictionary** (if possible) to demonstrate the structure and content of the dataset.

* ☑️ **Consider providing a synthetic dataset** that mimics the structure and format of the real data. This allows others to understand the data layout and run processing scripts without exposing sensitive information.

* ☑️ **Explain the access restrictions** for the dataset, and provide relevant contacts (e.g. "*Access to the dataset is restricted due to patient confidentiality. Researchers interested in accessing the data must submit a data access request to the XYZ Data Governance Committee. For more information, contact data.manager@xyz.org.*").

:::

<br>

### Parameter estimation scripts

<!--TODO:
If you are estimating parameters yourself from raw data, then use programming language to process
Have your scripts.
Share them with your code ideally.

EXAMPLE:

```
```

Internally, make sure to share them regradless.

Externtally, either
- share (may be possible even for sensitive data as nothing in script to give it away)
- describe processing (may be as simple as just "took mean of this and that" or may need more description for complex processing etc)
-->

<br>

### Distribution fitting scripts {#sec-input_data-fitting}

This section is written with credit to @Robinson2007 and @Monks2024.

When selecting appropriate statistical distributions for you model, one approach is to **base your choice on the known properties of the process being modelled**. This is useful when you have limited data - for example, only summary statistics like the mean. Some common distributions in healthcare simulation:

* **Arrivals**: Random independent arrivals are often modelled with the Poisson distribution, whilst their inter-arrival times are modelled using an exponential distribution (@Pishro-Nik2014).
* **Length of stay**: Length of stay is commonly right skewed (@Lee2003), and so will often be modelled with distributions like exponential, gamma, log-normal (for log-transformed length of stay) or Weibull.

However, it is possible that these are not accurate reflections, if the process deviates from your assumptions or has unique features not captured by the distribution. As such, if you have access to sufficient data, it is good to **fit distributions on your data**, when determining which to use. You can either do this manually or use provided tools.

#### Manually fitting distributions

When doing it manually, you first need to select some candidate distributions to fit to your data. You should both:

* Consider the known properties of the process being modelled (as above), and-
* Inspect the data by plotting a histogram.

<br>

This example uses **synthetic arrival data** from the nurse visit simulation (@sec-examples) - so, based on the known properties, we'd assume exponential could be a good choice - but we'll inspect the data too. First, we load the relevant data.

::: {.python-content}

```{python}
import pandas as pd
import plotly.express as px

# Import data
data = pd.read_csv("../../data/NHS_synthetic.csv", dtype={
    "ARRIVAL_TIME": str,
    "SERVICE_TIME": str,
    "DEPARTURE_TIME": str
})

# Preview data
data.head()
```

<br>

Many distributions assume data is **stationary** - i.e. no trends or sudden changes. Hence, we first plot the data as a time series to check if it is stationary - as, if not, certain periods may need to be excluded or modelled separately.

```{python}
# Define names
date_col = "Date"
arrivals_col = "Number of arrivals"

# Plot daily arrivals
daily_series = data.groupby(by=["ARRIVAL_DATE"]).size()
daily_df = daily_series.reset_index(name=arrivals_col).rename(
    columns={"ARRIVAL_DATE": date_col}
)

fig = px.line(daily_df, x=date_col, y=arrivals_col)
fig.update_layout(showlegend=False, width=700, height=400)
fig.show()
```

<br>

We can then plot a histogram of our data. <!--TODO: add distplot... https://plotly.com/python/distplot/... requires addition of scipy to environment-->

```{python}
# Plot histogram of daily arrivals
fig = px.histogram(daily_df, x=arrivals_col)
fig.update_layout(xaxis_title="Arrivals per day", showlegend=False)
fig.show()
```

This looks quite normally distributed.

```{python}
# Combine date/time and convert to datetime
data["arrival_datetime"] = pd.to_datetime(
    data["ARRIVAL_DATE"] + " " + data["ARRIVAL_TIME"].str.zfill(4),
    format="%Y-%m-%d %H%M"
)

# Sort by arrival time and calculate inter-arrival times
data_sorted = data.sort_values("arrival_datetime")
data_sorted["iat_mins"] = (
    data_sorted["arrival_datetime"].diff().dt.total_seconds() / 60
)

# Plot histogram of inter-arrival times
fig = px.histogram(data_sorted, x="iat_mins")
fig.update_layout(xaxis_title="Inter-arrival time (min)", showlegend=False)
fig.update_traces(
    hovertemplate="Inter-arrival time: %{x} min<br>Count: %{y}"
)
fig.show()
```

These are right skewed, so some candidates could be exponential, gamma, log-normal and Weibull.

<!--TODO:which you plot? how you know? etc?-->
<!--
:::

::: {.r-content}

```{.r}
library(readr)

data <- read_csv("../../data/NHS_synthetic.csv")

head(data)
```

:::

<br>

Next, calculate the parameters required for each of your chosen distributions.

Then... using graphs or statistical tests. with graphs, you compare data hist to hist of the distribution (which you make by sampling from distribution and placing on "same cell ranges"??? as the empirical data) (OR you make by finding freq from cum. dist functions for proposed distributions??? % obs in cell range) BUT these are quite influenced by cell range so important to do a few cell widths ALSO if <30 samples hist not smooth... so then there is p-p plot... and q-q plot...then for stats there is ch-square test... also kolmogorov-smirnov.., and assess goodness of fit.


#### Using a tool to test distributions

* fitter: <https://medium.com/the-researchers-guide/finding-the-best-distribution-that-fits-your-data-using-pythons-fitter-library-319a5a0972e9>
* distfit: <https://www.kdnuggets.com/2021/09/determine-best-fitting-data-distribution-python.html> amd <https://github.com/erdogant/distfit>
* manually/stats: <https://medium.com/@sigari.salman/discovering-the-best-fit-probability-distribution-for-your-data-a14c0e8d762>
* Mike's auto_fit: <https://github.com/health-data-science-OR/stochastic_systems/blob/master/labs/simulation/lab3/input_modelling/fitting.py> and <https://github.com/health-data-science-OR/stochastic_systems/blob/master/labs/simulation/lab3/sim_lab3_autofit_intro.ipynb>

::: {.python-content}

```{python}

```

:::
-->
<br>

#### Writing and sharing distribution fitting scripts

<!--TODO:
rmd/ipynb
located WHERE in repo
mkdir
touch notebook

sharing

need to fit to distributions. a few ways to do this.

PYTHON EXAMPLE:

R EXAMPLE:

Sharing: as with parameter estimation.
-->

<br>

### Parameters

<!--TODO:
If parameters were not calculated by you but from someone else, clearly state source and any relevant processing steps know about.

Sharing parameters is crucial, non optional. Must share some parameters, as without others cannot run model.

Can be stored in CSV files or defined directly in scripts.

could structure various ways... wide... long... etc.

EXAMPLE DATA
EXAMPLE DICTIONARY

If real parameters can't be shared, must provide synthetic (fake but structually similar) parameters to enable model execution.
could just have synthetic param or also pipeline with synthetic data at start etc.

when sharing:
* open, fair, github, archive, data dictionary
--->

<br>

## Maintaining a private and public version of your model

It is likely that you may have some data and/or code that you need to keep private, and cannot share along with the simulation model. It's important that **both the private and public components are version controlled** (@sec-version). One way of managing this is to have two separate repositories: a private repository and a public repository.

**If the public repository contains the real parameters** and results, it's quite simple: use the private repository for processing input data, then switch to the public repository for running the model.

**If the public repository only contains synthetic parameters**, you'll need to be able to run the simulation in the private repository with the real parameters and results, and also in the public repository with the synthetic parameters and results. To avoid duplicating the simulation code across both repositories, a good strategy is to develop your simulation code as a package (@sec-package). This package can be published on GitHub, PyPI, or simply installed locally. Your private repository can then import and use this package, allowing you to maintain a single version of the simulation code while keeping sensitive parameters and data private. <!--TODO: need to explain better about the import of package options-->

<br>

## Further information

* "[Simulation modelling for stochastic systems lab 3](https://github.com/health-data-science-OR/stochastic_systems/tree/master/labs/simulation/lab3)" from Tom Monks 2024.
* "Chapter 7: Data Collection and Analysis" from "Simulation: The Practice of Model Development and Use" by Stewart Robinson 2007.

<https://journals.sagepub.com/doi/full/10.1177/2515245920928007>

<https://help.osf.io/article/217-how-to-make-a-data-dictionary>

<https://open-science-training-handbook.github.io/Open-Science-Training-Handbook_EN/02OpenScienceBasics/02OpenResearchDataAndMaterials.html>

<https://caltechlibrary.github.io/RDMworkbook/>

<https://ddialliance.org/>

<br>

## References